---
title             : "How Does Dialect Exposure Affect Learning to Read and Spell? An Artificial Orthography Study"
shorttitle        : "Dialect Literacy"

author: 
  - name          : "Glenn P. Williams"
    affiliation   : "1, 2"
  - name          : "Nikolay Panayotov"
    affiliation   : "1"
  - name          : "Vera Kempe"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Division of Psychology. School of Applied Sciences. Abertay University. Dundee. DD1 1HG. Scotland, UK"
    email         : "v.kempe@abertay.ac.uk"
    
affiliation:
  - id            : "1"
    institution   : "Abertay University"
  - id            : "2"
    institution   : "University of Sunderland"

authornote: |
  Glenn P. Williams, Division of Psychology, Abertay University, and Department of Psychology, University of Sunderland; Nikolay Panayotov, Division of Psychology and Division of Games Technology and Mathematics, Abertay University; and Vera Kempe, Division of Psychology, Abertay University. 
  Preregistration, data, and code are available at https://osf.io/5mtdj/. 
  The authors gratefully acknowledge funding from The Leverhulme Trust (Grant RPG_2016-039) to Vera Kempe. We also would like to thank Richard Morey and E. J. Wagenmakers for helpful statistical advice but remain responsible for any potential flaws in the statistical analyses.

abstract: |
  Correlational studies have demonstrated detrimental effects of exposure to a mismatch between a nonstandard dialect at home and a mainstream variety at school on children’s literacy skills. However, dialect exposure often is confounded with reduced home literacy, negative teacher expectation, and more limited educational opportunities. To provide proof of concept for a possible causal relationship between variety mismatch and literacy skills, we taught adult learners to read and spell an artificial language with or without dialect variants using an artificial orthography. In 3 experiments, we confirmed earlier findings that reading is more error-prone for contrastive words; that is, words for which different variants exist in the input, especially when learners also acquire the joint meanings of these competing variants. Despite this contrastive deficit, no detriment from variety mismatch emerged for reading and spelling of untrained words, a task equivalent to nonword reading tests routinely administered to young schoolchildren. With longer training, we even found a benefit from variety mismatch on reading and spelling of untrained words. We suggest that such a dialect benefit in literacy learning can arise when competition between different variants leads learners to favor phonologically mediated decoding. Our findings should help to assuage educators’ concerns about detrimental effects of linguistic diversity.
  
keywords          : "literacy, dialect, artificial language learning"
wordcount         : "14,449"
note              : "Article accepted for publication in Journal of Experimental Psychology: General. The final article will be available, upon publication, via its DOI: 10.1037/xge0000778"
bibliography      : ["r-references.bib", "main-references.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "jou" # can be "man", "doc", or "jou"
output            : 
  papaja::apa6_pdf: # apa6_pdf
    keep_tex: FALSE
appendix          : 
  - "03_appendix_a.Rmd"
  - "03_appendix_b.Rmd"
  - "03_appendix_c.Rmd"
  - "03_appendix_d.Rmd"
  - "03_appendix_e.Rmd"
nocite            : | 
  @GIMP1995, @Forsythe2017, @Bowers2017, @Bowers2018, @Rastle2019, @Milde2011
header-includes   : 
  - \usepackage{tipa}
  - \usepackage{float}
  - \usepackage{hyperref}
---

```{r notes, include = FALSE}
# nocite used for keeping a reference that isn't cited in text.
# this is important as the appendix can't have citation codes
# otherwise additional references cited in the appendix only come after the appendix

# if it doesn't render properly, try changing the engine to:
#output:
#  papaja::apa6_pdf:
#    latex_engine: xelatex

# tipa package from LaTeX used for IPA: see https://www.tug.org/tugboat/tb17-2/tb51rei.pdf
# and https://github.com/crsh/papaja/issues/209 for notes on how to use this.

# longtable breaks 2 column pages ("jou"): https://github.com/jgm/pandoc/issues/1023
# this provides a fix for "jou" class:
# https://tex.stackexchange.com/questions/161431/how-to-solve-longtable-is-not-in-1-column-mode-error
# either don't use longtable (not needed for jou), or force to single col
# for tables

# Floats ----
# options for fig.pos = "H" etc. uses the float package from LaTeX
# kable latex_options: 
#   https://www.rdocumentation.org/packages/kableExtra/versions/1.0.1/topics/kable_stylingcan be #   "hold_position" and "HOLD_position" force printing in place for tables

# umlauts and pounds need special characters;
# &uml; = umlaut, &uuml; = umlaut over the u.
# \pounds = GBP sign
# \$ = $ (but escapes special character)

# fig positions use: htb (in place, top, or bottom of page); H for HOLD position
# see: https://www.overleaf.com/learn/latex/Errors/%60!h'%20float%20specifier%20changed%20to%20%60!ht'

# function using various general functions for tidying output in tables
# these are hardcoded so need improved before adding to general functions

# if you need to knit directly from LaTeX source, i.e. for journals
# remove use of here::here in figures, and replace with, e.g.
# knitr::include_graphics("../../04_figures/output/fig.png")
# then you need those folders in the LaTeX zip file for submission

# knitr::is_latex_output() can help with parsing latex or word tables
# with conditional formatting

# Note that the languages spoken function call negates subsets of languages
# so that e.g. Chinese and Chinese (Mandarin) aren't counted as two languages.
```

```{r packages-and-functions, include = FALSE, message = FALSE, warning = FALSE}
# load packages
packages <- c(
  "here",
  "papaja",
  "tidyverse",
  "lme4",
  "lmerTest",
  "emmeans",
  "irr",
  "BayesFactor",
  "brms",
  "broom.mixed",
  "knitr",
  "kableExtra",
  "english", # for parsing numbers as English words
  "cowplot", # for combining plots
  "magick" # required for cowplot
)

lapply(packages, library, character.only = TRUE)

# load user defined functions
source(here("05_paper", "00_functions", "paper_functions.R"))
source(here("05_paper", "00_functions", "model_names.R"))
```

```{r load-data, include = FALSE, message = FALSE, warning = FALSE}
# load experiment data
source(here("05_paper", "01_manuscript", "00_load-data.R"))

# load names for tables
source(here("05_paper", "01_manuscript", "01_names.R"))

# load gruffalo corpus data from GitHub
gruffalo_link <- 
  "https://raw.githubusercontent.com/gpwilliams/gruffalo_index/master/data/"

gruffalo_counts <- read_csv(paste0(
  gruffalo_link, "03_summary-data/counts.csv"
  ))
gruffalo_freq <- read_csv(paste0(
  gruffalo_link, "03_summary-data/frequencies.csv"
  ))
gruffalo_token <- read_csv(paste0(
  gruffalo_link, "02_processed-data/token_freq.csv"
))

# load data on demographic checks
demo_checks_path <- list.files(
  path = here("03_analysis", "04_demographic_checks"), 
  pattern = "csv$",
  full.names = TRUE
)
results$multiple_experiments$demographic_checks <- map( # read all csvs
  demo_checks_path, 
  read_csv,
  col_types = cols()
)
demo_checks_file_names <- list.files(
  path = here("03_analysis", "04_demographic_checks"), 
  pattern = "csv$",
  full.names = FALSE
) %>%
  stringr::str_sub(., start = 1, end = - 5)

names(results$multiple_experiments$demographic_checks) <- 
  demo_checks_file_names

# load data on participant exclusions
exclusions <- list()
exclusions$paths <- character(3)
exclusions$studies <- c(
  "01_study-zero", 
  "02_study-one-and-two", 
  "03_study-three"
)
exclusions$fileheadings <- c("ex_0", "ex_1_2", "ex_3")

# read files on exclusions
for(i in seq_along(exclusions$paths)) {
  exclusions$paths[i] <- list.files(
    path = here(
      "02_data", 
      exclusions$studies[i], 
      "05_data-checks"
    ),
    pattern = paste0(
      exclusions$fileheadings[i], 
      "_dropped_participants_and_reasons.csv"
    ),
    full.names = TRUE
  )
}
results$multiple_experiments$exclusions <- map(
  exclusions$paths, 
  read_csv,
  col_types = cols()
)
names(results$multiple_experiments$exclusions) <- exclusions$fileheadings

# filter to only those who were actually excluded (i.e. technical errors)
results$multiple_experiments$exclusions <- 
  results$multiple_experiments$exclusions %>% 
  bind_rows(.id = "id") %>% 
  select(-X1) %>% 
  filter(group == "technical_difficulty") %>% 
  mutate(id = case_when(
    id == "ex_1_2" & participant_number < 120 ~ "ex_1",
    id == "ex_1_2" & participant_number > 120 ~ "ex_2",
    TRUE ~ as.character(id)
  ))

# load in results from exploratory analysis of types of errors
error_types <- list()
error_types$paths <- list.files(
  path = here(
    "03_analysis", 
    "05_exploratory-analysis", 
    "output", 
    "summaries"
  ),
  pattern = "mean_proportions.csv",
  full.names = TRUE
)

# read files on exclusions
results$multiple_experiments$error_types <- map(
  error_types$paths , 
  read_csv,
  col_types = cols()
)
names(results$multiple_experiments$error_types) <- paste0(
  "ex_", 
  list.files(
    path = here(
      "03_analysis", 
      "05_exploratory-analysis", 
      "output", 
      "summaries"
    ),
    pattern = "mean_proportions.csv"
    ) %>% str_extract_all(., "[0-9]"))
```

```{r global-options, include = FALSE, warning = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r formatting-options, include = FALSE, warning = FALSE}
table_alignment <- c("l", rep("c", 8))

ex0_foot <- "Bayesian analyses report standardized parameter estimates (i.e. the intercept [grand mean] is centered at 0). Values of 0 with a sign indicate the direction of the estimate before rounding. nLED = length-normalized Levenshtein Edit Distance; Block (B) = 1 - 6; variety condition (VC) = variety match versus variety mismatch (VMa vs. VMi); picture condition (PC) = picture versus no picture (P vs. NP); word type (WT) = contrastive versusnoncontrastive. Lines in boldface indicate significant effects in frequentist analyses and credible intervals not straddling 0 in Bayesian analyses."

general_foot <- "Bayesian analyses report standardized parameter estimates (i.e. the intercept [grand mean] is centered at 0). Values of 0 with a sign indicate the direction of the estimate before rounding. nLED = length-normalized Levenshtein Edit Distance; Block (B) = 1 - 6; variety condition (VC) = variety match versus variety mismatch (VMa vs. VMi); picture condition (PC) = picture versus no picture (P vs. NP); word type (WT) = contrastive versusnoncontrastive. Lines in boldface indicate significant effects in frequentist analyses and credible intervals not straddling 0 in Bayesian analyses."

header_above <- c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
# multipage table to kable_coefs(): define whether tables can span multiple pages
# must be TRUE if using "man", as tables span multiple pages
# output format can be latex or pandoc (for Word)
```

\newpage

In 2013, the BBC reported that a Head Teacher in England had banned use of the local dialect in his Primary School [@BBC2013]. This decision appears to have been motivated by the notion that dialect exposure creates confusion when beginning readers encounter different variants associated with the same meaning and have to resolve the competition between them. However, direct empirical support for the notion that such competition slows the acquisition of literacy skills is lacking. The aim of the present study is to put this notion to a rigorously controlled test.

Although linguistic diversity is a ubiquitous feature of many languages, most research on how exposure to different varieties affects literacy acquisition has been conducted on minority dialects of English spoken in the United States. A considerable body of evidence has implicated exposure to these minority dialects, and especially the degree of “dialect density,” that is, the frequency of oral dialect use, as risk factors for reading difficulties [e.g., @charity2004familiarity; @terry2016dialect; @washington2018impact]. Although such a link has not been observed consistently, due to methodological flaws in the measurement of children’s dialect exposure and literacy outcomes in earlier studies [@Harber1977; @Steffensen1982], the persistent literacy achievement gap in U.S. minority children sustained interest in studying exposure to nonstandard varieties. A recent meta-analysis by @Gatlin2015 concluded that there was a moderate negative relationship between exposure to, and use of, nonmainstream American English and literacy outcomes in the absence of significant effects of socioeconomic status (SES). While SES has implications for a host of variables such as quality of input, home literacy, attitudes toward literacy, educational provision, and teacher expectation, the independent effect of these variables is difficult to control in correlational studies [@Artiles2010]. For example, one prominent U.S. minority dialect, African American English (AAE), has diverged from Mainstream American English (MAE) as a function of, among other things, social and cultural segregation leading to divergent attitudes toward literacy [@Labov1995]. It is therefore important to understand whether dialect exposure exerts a detrimental effect via those sociocultural and environmental variables or whether it plays a direct causal role in the impairment of emerging literacy.

According to the linguistic mismatch hypothesis [@Labov1995], dialect exposure increases the mismatch between orthographic and phonological forms thus rendering the discovery of phonologically mediated decoding principles more challenging. Specifically, dialect variants that deviate strongly from standard words so as to essentially constitute competing lexemes (e.g., Scots “bairn” vs. Standard English “child”) have to be acquired in addition to learning to read and spell. When learners attempt to establish links between orthographic and phonological representations as postulated in computational models of reading such as the Dual Route Cascaded model [@Coltheart2001], the Connectionist Dual Process model [@Perry2007; @Perry2010] or the Triangle model [@Harm2004; @Plaut1996], the activation of competing phonological representations of words with dialect variants (henceforth: contrastive words) might lead to interference, which should incur additional processing cost. On the other hand, dialect variants characterized by mainly phonological changes that deviate only slightly from words in the standard language (e.g., Scots “hoose” vs. Standard English “house” or AAE “aks” vs. MAE “ask”) add inconsistency to the mapping from print to sound. The resulting increased orthographic inconsistency is likely to make the acquisition of decoding skills via application of phoneme-grapheme conversion rules more difficult, particularly for phonologically less consistent orthographies, such as English, which are difficult to decode even without additional dialect variation (but see J. S. Bowers & Bowers, 2017, 2018; Rastle, 2019 for arguments in favour of benefits from morphological transparency of English spelling).

Alternatively, the linguistic awareness/flexibility hypothesis suggests that high dialect density is a manifestation of limited metalinguistic awareness of the social and contextual features that cue the appropriate use of one or the other variety [@terry2011phonological]. Limited metalinguistic awareness, especially in the phonological domain, has been linked to poorer decoding and comprehension skills [@ehri2001systematic]. Under this account it is not dialect exposure per se that impairs literacy acquisition. Rather, the effect is an indirect one: Children who persist with dialect use in settings which presuppose use of the mainstream variety betray a lack of metalinguistic awareness the manifestations of which in other domains like phonology hinder acquisition of decoding skills. As under the mismatch hypothesis, this deficit should be especially problematic for the decoding of contrastive words where metalinguistic awareness of contextual information can indicate which variant should be favored to resolve the competition. The mismatch and the awareness accounts need not be mutually exclusive as the direct effect of dialect exposure might be partially mediated by linguistic awareness [@terry2011phonological].

The hypothesis that contrastive words elicit reading difficulties was tested by Brown and colleagues [@Brown2015] with 8 to 13-year-old children exposed to AAE who were asked to read contrastive and noncontrastive words matched for frequency, length, and initial phonemes. The contrastive words typically had dialect variants with reduced consonant clusters. The results showed that the higher these children’s usage of AAE, assessed through number of AAE features in a sentence repetition task, the longer their reading latencies for contrastive words. This contrastive deficit was computationally simulated in a neural network which instantiated statistical learning of spelling-sound correspondences. The model was first exposed to repeated mappings of phonological to phonological representations within an attractor network (i.e., a task mimicking learning to speak) before being trained to map orthographic onto phonological representations via a layer of hidden units (i.e., a task mimicking learning to read) while still receiving interleaved blocks of phonological exposure to prevent “catastrophic interference” when switching from one type of input to another. Crucially, when the network was initially exposed to AAE variants for half of the words (i.e., variants comprising dialect-appropriate consonant cluster reductions, consonant drops, substitutions, exchanges, and devoicing) and then subsequently was trained to read MAE words (the mismatch condition), the cross-entropy error remained higher for contrastive compared to noncontrastive words. @Brown2015 interpreted this finding in analogy to the reading of heterophonic homographs--identical spellings of semantically unrelated words that are pronounced differently like “lead” or “wind,” which in the absence of contextual information are more difficult to read compared with nonhomographic control words [@Gottlob1999; @Jared2012]. As predicted by the linguistic awareness/flexibility hypothesis [@terry2011phonological], the contrastive deficit was greatly diminished in a second simulation that instantiated nodes coding explicitly for whether a word belonged to AAE versus MAE, a feature designed to simulate social cues for use of one or the other variety.

While the @Brown2015 simulation undoubtedly provided important insights into potential mechanisms that might be responsible for the difficulty with reading contrastive words, some crucial components of word representation and literacy learning were absent from the model. As a result, it is not entirely clear whether the contrastive deficit in the neural network arises for the same reasons as it did in beginning readers, even if extralinguistic factors are controlled. First, the network lacked a semantic layer precluding instantiation of semantic representations for individual words. Yet beginning readers tend to know the meanings for most, if not all, of the words presented in early literacy training, and start out by employing phonologically mediated decoding to gradually establish direct associations between the new orthographic code and the existing semantic representations [@Castles2018]. There was no mechanism in the @Brown2015 model by which different variants could be associated with the same meaning. Instead, in the mismatch condition, the network simply learned more words overall as literacy training added an additional set of MAE words which were phonologically similar to some of the already acquired AAE words. As a result, contrastive words shared many phonemes with other variants in the lexicon while noncontrastive words did not, rendering the contrastive deficit--as mentioned above--akin to inhibition from high-frequency heterophonic homographs. Extrapolating from existing models of interactive activation and competition that try to explain neighborhood effects we hypothesize that adding a semantic layer should retain or even exacerbate the contrastive deficit as bidirectional links between semantic and lexical representations may reinforce nonlinear inhibitory connections on the lexical layer [@Chen2012].

Second, neither human participants nor the connectionist model exhibited difficulties with reading noncontrastive words nor an overall reading deficit in the variety mismatch (AAE) condition. If potential detriments due to variety mismatch are mainly driven by processing difficulties with contrastive words then the overall amount of literacy problems associated with dialect exposure would mainly depend on the proportion of contrastive words in the input. Yet the linguistic mismatch hypothesis as formulated by @Labov1995 went beyond confining detrimental effects to contrastive words by suggesting that dialect exposure impairs orthographic decoding skills more generally. Similarly, the Linguistic Awareness/Flexibility Hypothesis [@terry2011phonological] also asserts that limited dialect awareness should impair beginning readers’ general phonological decoding skills. However, to directly confirm detrimental effects of dialect exposure beyond contrastive words one would have to test beginning bidialectal readers’ decoding skills independently of their word knowledge [@Castles2018] and show that their nonword reading skills are impaired compared with learners without dialect exposure. Such a test, which was absent from both the behavioral study and the computational simulation in @Brown2015, will be included in the present study.

Third, beginning readers never learn only to read but also to spell, as primary schools tend to incorporate writing instruction into their curricula from early on [@Cutler2008]. Spelling training strengthens the connections between individual phonemes and graphemes thereby promoting use of decoding skills. In children, phonological spelling ability has been shown to predict subsequent development not just of spelling but, crucially, reading skills [@caravolas2001foundations], confirming earlier proposals that in the early stages of literacy learning, phonological spelling ability drives the development of reading [@frith1985beneath]. For adults learning an artificial script, @Taylor2017 showed that including a spelling task in addition to other tasks emphasizing spelling-sound as opposed to spelling-meaning mappings into the training regimen encouraged a phonologically mediated reading acquisition strategy. While the child participants in @Brown2015 would certainly have engaged in spelling practice during their schooling, the neural network did not include bidirectional links that could have instantiated a “spelling path,” that is, a path from phonological to orthographic representations [see @Houghton2003], and we are not aware of any attempts to computationally model the contribution of spelling practice to emergent reading skills. Yet by promoting explicit reliance on links between graphemes and phonemes as the primary reading strategy [@ellis1990role] which minimizes reliance on direct word retrieval, and, hence, the possibility for lexical competition, spelling training might alleviate potentially detrimental effects of dialect exposure.

The evidence discussed so far was obtained in studies investigating the process of learning to read English, a deep orthography with a fair amount of inconsistent phoneme-grapheme mappings that still is often taught without placing sufficient emphasis on phonological mediation [@Castles2018]. For such inconsistent spelling systems, dialect exposure may be particularly detrimental as it can further hinder acquisition of already difficult-to-discover decoding rules. By contrast, learning to decode mappings from sound to spelling is easier in more consistent orthographies, and, consequently, dialect exposure may have less of a detrimental effect. It is even possible that more rapidly acquired decoding skills in consistent orthographies can render the decoding of words of the standard variety unperturbed by the existence of competing dialect variants. To our knowledge, the only more consistent orthography for which the role of dialect exposure in literacy learning has been investigated is German. @Buhler2018 examined early literacy skills in children exposed to Swiss German dialect and compared them with children exposed only to Standard German either in Switzerland or in Germany. The results showed that dialect exposure was associated with higher preschool literacy-related skills measured by the ability to identify, categorize, and synthesize onsets, rimes and individual phonemes, in the absence of differences in SES between the groups. Structural equation modeling revealed that only when preschool literacy-related skills were controlled was there a negative effect of dialect exposure on Grade 1 literacy skills, which was more pronounced in spelling due to the fact that German’s phoneme-grapheme mappings are less consistent than the grapheme-phoneme mappings. This finding exposes the multiple loci of effects that early dialect exposure might have: On the one hand, benefits from early dialect exposure on literacy-related skills might arise from enhanced sensitivity to phonological variation thereby increasing metalinguistic skills that benefit phonological awareness. On the other hand, residual negative effects of dialect exposure on subsequent literacy acquisition may reflect the consequences of decreased consistency in spelling-sound mappings as well as the difficulty associated with first having to learn a number of new lexemes in order to master literacy in the standard language. This suggests that in orthographies with greater feed-forward (reading) consistency, potentially detrimental effects of dialect exposure may be offset by its contribution to enhanced phonological awareness which, in turn, can aid phonological mediation of literacy learning.

To gain further clarity, our study asked whether there is a causal relationship between variety mismatch and difficulties with acquiring decoding skills when confounding extralinguistic variables that may impact the acquisition of these skills are controlled. By variety mismatch we mean a situation where another variety (e.g., a regional dialect) is used outside of the context of literacy acquisition. To achieve this control, we employed an artificial language learning paradigm combined with an invented script, a methodology that has successfully been used to explore various factors that affect the early stages of learning to read [e.g., @Taylor2017; @Taylor2011; for a review see @Vidal2017]. We attempted a conceptual replication of the contrastive deficit demonstrated in @Brown2015 to confirm whether variety mismatch is indeed the cause of deficits associated with dialect exposure. Crucially, we also asked whether variety mismatch affects general decoding skills as assessed via reading of untrained words. Here, we perform these tests with adult learners to provide a baseline for future comparison with children. We seek to provide proof of concept for how dialect exposure per se can affect literacy learning under optimal learning conditions associated with a mature cognitive system: Detrimental effects in adults would suggest that dialect exposure is bound to hinder literacy learning by virtue of increasing the amount of interference in the input, and detrimental effects in children may be inevitable. However, if no detrimental effects are observed in adults then detrimental effects in children may arise from how dialect exposure interacts with a less mature cognitive system or due to confounding factors that affect children who are exposed to dialects.

## The Present Study

We report three experiments designed to investigate effects of dialect exposure on the acquisition of decoding skills in inconsistent and consistent orthographies. For the present study, we defined dialect exposure following @Brown2015 as exposure to variants that entail phonological, but not lexical, changes (e.g., English “house” vs. Scots “hoose” or MAE “ask” vs. AAE “aks”). The focus on phonological variants was motivated by ecological validity based on a corpus analysis (the Gruffalo-corpus described below) of a range of Scots dialects [@Johnston2007]. Effects of exposure to lexical variants (e.g., English “children” vs. Scots “bairns”) are beyond the scope of the current study. Below we briefly preview the rationale behind the three experiments. 

We start by reporting a conceptual replication of Simulation 1 in @Brown2015 that examines effects of dialect exposure on learning to read an inconsistent artificial orthography[^1]. (Experiment 1). To explore the role of semantic information we compared performance for words presented with and without accompanying pictures. Availability of semantic information was crossed with dialect exposure: In the variety match conditions participants encountered the same words during initial exposure and during reading training. In the variety mismatch conditions half of the words underwent phonological changes between exposure and reading training to loosely resemble a situation in which learners initially are exposed to a dialect at home before being introduced to the standard variety at school. However, because Experiment 1 was conceived as a replication of @Brown2015, it did not include spelling training and did not vary orthographic consistency, two factors which need to be considered to be able to generalize cross-linguistically. To address these two limitations, we compared variety match and mismatch conditions on learning to read and spell a consistent (Experiment 2a) and an inconsistent (Experiment 2b) orthography. We found that learning to read and spell an entirely unfamiliar inconsistent artificial orthography proved to be a very difficult task that may require more extensive training. In Experiment 3 we therefore replicated Experiment 2b with a longer training phase, a larger sample size and semantic information throughout. All experiments received ethical approval from Abertay University’s Ethics Committee and were programmed to be compatible with all desktops and Android systems using most web browsers.

[^1]: Experiment 1 was conducted last but is reported first to maintain the logic of reporting first the replication attempt of the connectionist simulation reported in @Brown2015.

# Experiment 1: Effect of variety mismatch on learning to read an opaque orthography

## Method

### Participants

`r stringr::str_to_sentence(as.english(results$experiment_0$demographics$demographic_summary$n, UK = TRUE))` participants (aged `r results$experiment_0$demographics$demographic_summary$age_min`--`r results$experiment_0$demographics$demographic_summary$age_max`, *M* = `r results$experiment_0$demographics$demographic_summary$age_mean`, *SD* = `r results$experiment_0$demographics$demographic_summary$age_sd`, with `r results$experiment_0$demographics$gender_count %>% filter(gender == "f") %>% pull(n)` self-reported as female, `r results$experiment_0$demographics$gender_count %>% filter(gender == "m") %>% pull(n)` self-reported as male, and `r results$experiment_0$demographics$gender_count %>% filter(gender == "o") %>% pull(n)` self-reported as other)[^2] were recruited from the crowdsourcing website Prolific Academic. All participants reported English as their native language, and no known mild cognitive impairments or dementia, and were reimbursed \pounds4.20. Participants’ mean proficiency in English on a 1-5 Likert scale was `r results$experiment_0$demographics$english_proficiency$mean` (*SD* = = `r results$experiment_0$demographics$english_proficiency$SD`, range `r results$experiment_0$demographics$english_proficiency$min` [elementary] to `r results$experiment_0$demographics$english_proficiency$max` [native or native-like]). Despite declaring English as native language, `r results$multiple_experiments$demographic_checks$english_proficiency_counts %>% filter(experiment == 0) %>% filter(language_proficiency_rating != 5) %>% pull(n) %>% sum()` participants rated their English proficiency as below 5. `r results$multiple_experiments$demographic_checks$known_language_category %>% filter(experiment == 0) %>% filter(speaker_category == "monolingual") %>% pull(n) %>% english() %>% stringr::str_to_sentence()` participants reported knowing only English while `r results$multiple_experiments$demographic_checks$known_language_category %>% filter(experiment == 0) %>% filter(speaker_category == "multilingual") %>% pull(n) %>% english() %>% stringr::str_to_sentence()` participants also knew French (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 0, language_spoken == "French") %>% pull(n) %>% sum()` times), Spanish (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 0, language_spoken == "Spanish") %>% pull(n) %>% sum()` times), German (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 0, language_spoken == "German") %>% pull(n) %>% sum()` times), and `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 0, language_spoken %nin% c("French", "German", "Spanish", "English")) %>% filter(!str_detect(language_spoken, "\\(")) %>% summarise(n = length(unique(language_spoken))) %>% pull(n)` other languages (listed a total of `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 0, language_spoken %nin% c("French", "German", "Spanish", "English")) %>% pull(n) %>% sum()` times). Only `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 0, language_spoken %in% c("Chinese", "Chinese (Mandarin)", "Japanese")) %>% pull(n) %>% sum() %>% english()` participants were familiar with logographic scripts. An additional `r results$multiple_experiments$exclusions %>% filter(id == "ex_0") %>% summarise(n = length(unique(participant_number))) %>% pull(n) %>% english()` participants were tested but not included either because they gave the same response on all trials, their responses on most trials repeated the previous trial, their responses were in English rather than in the artificial language or were inaudible, or because a technical difficulty had occurred (e.g., losing trials due to poor Internet connection), and when recruitment inadvertently extended beyond our preregistered cut-offs for a given list.

[^2]: One participant self-reported an age of 14 which we assume is a typo as Prolific Academic enforces a minimum age of 18.

### Materials

#### Grapheme and Phoneme Inventory

We generated 13 graphemes (for a list of graphemes and criterio for inclusion see Appendix\ \@ref(appendix-a)) consisting of two to four curved or straight strokes as common to most alphabetic writing systems [@Changizi2005]. The phoneme inventory consisted of eight consonants [m], [n], [s], [k], [b], [d], [f] and [l] as well as the five cardinal vowels \textipa{[A]}, \textipa{[E]}, [i], \textipa{[O]}, [u]. Additionally, the dialect phonemic inventory included an additional phoneme, [x], which replaced [k] in certain contexts, as described below.

#### Words

Using this phoneme inventory, we constructed 42 artificial words distributed across six syllabic templates (3 monosyllabic, 3 bisyllabic) adhering to constraints of English phonotactics [@Harley2006;@Crystal2003]. To constrain phonological complexity and to avoid overly predictive clusters, words contained no more than one consonant cluster and no cluster with more than two consonants. Applying these rules to a string generation algorithm (accessible at [https://osf.io/5mtdj/](https://osf.io/5mtdj/)), we produced all possible phoneme permutations per syllable template, and selected seven strings from each template type, by removing strings with phoneme repetitions and ensuring a similar distribution of phonemes across items. To capture a range of English neighborhood densities, we selected a roughly equal number of words with high and low phonological neighborhood densities according to the Cross-Linguistic Easy-Access Resource for Phonological and Orthographic Neighborhood Densities [@Marian2012] database using the total neighbor metric (i.e. including substitutions, additions, and deletions) resulting in a mean neighborhood density of 2.88. To minimise confusability of words, our final list was filtered such that each word differed from each other word by a length-normalised Levenshtein Edit Distance (nLED)[^3] of at least 0.5, resulting in an average nLED of 0.86, ensuring sufficient variability across items. This restriction was applied as variability has been shown to support learning of grapheme-phoneme-correspondences [@Apfelbaum2013]. Thirty words were used during exposure and literacy training, while 12 words (two from each syllable template) were retained for testing only (henceforth: untrained words). All words are listed in Appendix\ \@ref(appendix-c)).

Words and isolated phonemes were recorded by a male and a female speaker in a soundproof booth with a Zoom H4n audio recorder, using normal prosody with stress on the first syllable in the bisyllabic items. Speaker voice was counterbalanced across participants. Sound files were normalized, with noise filtered using the Audacity audio suite [@Mazzoni2016] and extraneous silences trimmed using Praat [@Boersma2017]. In the Picture condition, words were randomly combined with images taken from the revised Snodgrass and Vanderwart image set of colourised images provided by @Rossion2004. Images were centred and resized to 280 by 280 pixels using GIMP version 2.10.6 [@GIMP1995]. For specifics of picture selection see Appendix\ \@ref(appendix-d)).

[^3]: A widely used normalized measure computed by dividing the number of insertions, deletions, and substitutions required to transform one string into another by the larger of the two string lengths [@levenshtein1966binary].

#### Orthography

To create orthographic inconsistencies, we introduced two conditional rules to supplement one-to-one mappings of graphemes to phonemes. First, the phoneme /l/ was rendered by its corresponding grapheme in all contexts except in five instances when it was preceded by /b/ or /s/, in which case it was spelled using the grapheme otherwise assigned to /n/ so that, for example, /bl\textipa{A}f/ was spelled as the artificial equivalent of *BNAF*. Second, the phoneme /s/ was rendered by its corresponding grapheme in all contexts except in five instances when it was preceded by /n/ in which case it was rendered by the grapheme otherwise assigned to /f/ so that, for example, /snid/ was spelled *FNID*. It is important to emphasize that these conditional rules introduced a roughly similar amount of inconsistency in both directions: In terms of feed-forward consistency (spelling-sound correspondences required for reading), the artificial grapheme signifying F was pronounced as /s/ 27% of times and as /f/ 73% of times. Similarly, the artificial grapheme signifying N was pronounced as /n/ 67% of times and as /l/ 33% of times. In terms of feed-back consistency (sound-spelling correspondences required for writing), the phoneme /s/ was spelled as (the artificial equivalent of) the letter S 74% of times and as F 26% of times. Similarly, the phoneme /l/ was spelled as L 75% of times and as N 25% of times. These conditional spelling rules were matched across word type resulting in five contrastive and five noncontrastive words with irregular spelling. For one contrastive and one noncontrastive word, both conditional spelling rules applied simultaneously so that /sl\textipa{O}ku/ and /slin\textipa{A}b/ were spelled as *FNOKU* and *FNINAB*, respectively.

#### Simulating dialect exposure based on the Gruffalo-corpus

```{r gruffalo-token-summary, include = FALSE, warning = FALSE}
gruffalo_token_summary <- gruffalo_token %>% 
  group_by(shift_category) %>% 
  summarise(
    n = length(nLED), 
    total = nrow(.), 
    proportion = length(nLED)/nrow(.),
    mean_nLED = mean(nLED)
  )
```

Because processing of phonological versus lexical variation might rely on different mechanisms as discussed above, we restricted this study to just one type of variation, namely that which is most frequent in prominent naturally occurring dialect varieties of English like Scots. This determination requires frequency estimates from transcribed corpora of dialect use which, to our knowledge, do not exist. We therefore consulted translations of the two popular children’s books “The Gruffalo” and “The Gruffalo’s Child” [@Donaldson1999; @Donaldson2005], written in Standard British English (SBE), into a number of varieties of Scots, including Dundonian, Glaswegian, and Doric, to obtain such a dialect corpus. The seven books comprising this corpus are listed in Appendix\ \@ref(appendix-b). This approach, in essence, amounts to treating the translators of the original version of "The Gruffalo" as native dialect informants. Using a corpus derived from children's verses gives us estimates for how dialects differ from standard varieties for linguistic content that is appropriate for the age group at which literacy is acquired. 

The Gruffalo corpus comprised `r gruffalo_counts %>% filter(label == "type") %>% pull(count)` translated word types. Each of the Scots words in each Gruffalo translation was aligned with its SBE equivalent and coded for whether it differed lexically resulting in a Scots word not existent in SBE (e.g. *big* -- *muckle*[^4]) or phonologically (e.g. *mouse* -- *moose*). To validate this categorization we computed nLEDs between the SBE and Scots variants for each category (lexcial variants: *M* = `r gruffalo_token_summary %>% filter(shift_category == "lexical") %>% pull(mean_nLED)`, phonological variants: *M* = `r gruffalo_token_summary %>% filter(shift_category == "phonological") %>% pull(mean_nLED)`). Phonological differences were further subcategorized as phoneme drops (e.g. *and* -- *an*), substitutions (e.g. *bright* -- *bricht*), or insertions (e.g. *it's* -- *hit's*), and whether diphthongization (e.g. *ahead* -- *ahaid*) or monophthongization (e.g. *mouse* -- *moose*) occurred[^5]. A total of `r english(gruffalo_token_summary %>% filter(shift_category == "unsure") %>% pull(n), UK = TRUE)` words involved a difference which could not be reliably categorised as lexical or phonological. Words that arose from paraphrasing the SBE phrases (e.g. *"...that no Gruffalo should ever set foot"* -- *"it wid come tae nae guid if..."*) were excluded from our analysis. 

Analyzing the distribution of variants revealed that `r gruffalo_freq %>% filter(label == "type_contrastive") %>% pull(frequency_proportion)*100`% of word types and `r gruffalo_freq %>% filter(label == "token_contrastive") %>% pull(frequency_proportion)*100`% of word tokens were contrastive, that is, had a dialect variant. Of these contrastive words, `r gruffalo_freq %>% filter(label == "type_constrastive_phon") %>% pull(frequency_proportion)*100`% of types and `r gruffalo_freq %>% filter(label == "token_constrastive_phon") %>% pull(frequency_proportion)*100`% of tokens had variants with phonological differences, confirming that phonological variation was indeed the most common variation. Of the phonological variants, the most frequent ones were phoneme subtitutions (`r gruffalo_freq %>% filter(label == "token_phoneme_subs") %>% pull(frequency_proportion)*100`% of all phonological variant tokens) and consonant drops (`r gruffalo_freq %>% filter(label == "token_consonant_drops") %>% pull(frequency_proportion)*100`% of all phonological variant tokens)[^6]. These estimates suggest that inclusion of 50% of words with dialect variants as in @Brown2015 provides an ecologically valid amount of dialect variation. We therefore implemented a range of variations that mimicked those found in the Gruffalo-corpus as listed below:

(a) *consonant substitution*: [k] was changed to [x] in all positions (e.g. /skub/ changed to /sxub/).

(b) *consonant drop*: [d] was dropped in final position (e.g. /snid/ changed to /sni/).

(c) *vowel change*: \textipa{[E]} and \textipa{[A]} were replaced with [i] and \textipa{[O]}, respectively (e.g. /n\textipa{E}f/ changed to /nif/ and /n\textipa{A}l/ changed to /n\textipa{O}l/) in all positions. In instances where multiple changes could apply all were implemented in the "dialect" so that, for example, /sk\textipa{E}fi/ becomes /sxifi/ and /fl\textipa{E}s\textipa{O}d/ becomes /flis\textipa{O}/.

[^4]: Described by the @DictionaryoftheScotsLanguage as an adjective meaning 'Of size or bulk: large, big, great'.

[^5]: In some cases, such as the Scots *ken* (*know*), this change is recorded as a lexical change as the phonology of the word changes dramatically, i.e. /k\textipa{E}n/ from /n\textipa{@}\textipa{U}/ despite maintaining the root of the word. Moreover, due to lack of standardization of Scots spelling, we only could include phonological changes that were orthographically rendered. For example, while the voiceless velar fricative was orthographically rendered in some cases, /x/ (e.g. *right* -- *richt*), in others it was not (e.g. *loch* -- *loch*), and these changes could not be counted in the corpus analysis.

[^6]: Note that these values sum to more than 100% as words could include both phoneme substitutions and consonant drops, amongst other changes.

### Procedure

Participants were instructed that they would learn to read a “made-up” language and that it was important to perform the task in a quiet environment and to not take any notes during participation. To ensure compliance the instructions misled participants into believing that detection of cheating on our part could jeopardize reward. After receiving instructions describing the experimental procedure and providing consent compatible with the General Data Protection Regulation, participants were asked to check the working order of their microphone and headphones/speakers. The experiment consisted of three components: exposure, training, and testing (see Table\ \@ref(tab:table-procedure-overview)). In the first exposure block, participants heard all 30 training words one by one in randomized order. They then viewed each grapheme one by one (cycling twice during the set), accompanied by the sound of the isolated phoneme. Crucially, phonemes were randomly assigned to graphemes for each participant to reduce the potential impact of any systematic differences in accessibility of grapheme-phoneme pairs. Following recommendations to include time limits preventing participants from taking notes in learning experiments [@Rodd2019], ach grapheme disappeared after 1,000 ms. This process was repeated once, exposing participants to each grapheme-phoneme combination for a total of four times.
 
Next, participants proceeded to the reading training, which was interleaved with more exposure. To this end, the set of 30 words was randomly split into three reading training blocks of 10 words each. For each item, participants saw a string of graphemes and had to read the target word out loud. To avoid recording long silences we timed participants’ responses by presenting a moving hand in a clock indicating the onset, duration, and offset of the 2,500 ms recording window. In the picture condition, orthographic representations were always accompanied by pictures to simulate availability of semantic context. Although script is typically not accompanied by pictures, we deemed such a procedure justified given that reading rarely is context-free and confined to single words and children’s early reading materials frequently contain illustrations. Upon completion of each recording, participants received auditory feedback by listening to the target sound form. Each 10-item training block was presented twice in a row to equate number of exposures per word with Experiment 2 to maintain comparability (for an overview of the task sequences in all three experiments see Table\ \@ref(tab:table-procedure-overview)). The first such block was followed by another exposure to all 30 words before proceeding to the second block of training, followed by another exposure. In total, participants were exposed to the set of 30 words three times--once at the beginning, once after the first, and once after the second reading training block. After completing the third reading training block, participants were tested on reading of the 30 trained and the 12 untrained words, all presented in random order without auditory feedback.

Crucially, in the variety mismatch condition, participants heard the dialect variants of contrastive words during all exposure blocks, but were presented with the standard variants during reading feedback. The source code for the experiment can be found at [https://osf.io/5mtdj/](https://osf.io/5mtdj/). The mean completion time was `r results$experiment_0$demographics$demographic_summary %>% pull(time_mean) %>% as.numeric("minutes")` min (*SD* = `r results$experiment_0$demographics$demographic_summary %>% pull(time_sd) %>% as.numeric("minutes")`). To ensure an equal number of participants per condition a randomized sequence of eight conditions comprising a crossing of variety condition, picture condition and speaker voice (female vs. male) was created and administered consecutively over the course of about two weeks thereby ensuring pseudorandom assignment to all conditions. Repeated sign-up of participants was blocked by the crowdsourcing website.

\renewcommand{\arraystretch}{0.8}
\renewcommand{\baselinestretch}{1}

```{r table-procedure-overview, results = "asis", warning=FALSE}
# above latex commands squishes row height (0.8) and spacing in footnotes (1)
procedure_overview <- read_csv(
  here("05_paper", "01_manuscript", "procedure-table.csv")
  ) %>% 
  mutate_all(replace_na, replace = "") %>% 
  mutate_at(
    vars(ex2_consistent, ex2b_inconsistent, ex3_inconsistent), 
    ~str_replace_all(., "1,2", "\\\\textsuperscript{a,b}") # superscript 1,2 order
    ) %>% 
  mutate_at(
    vars(ex2_consistent), 
    ~str_replace_all(., "2,1", "\\\\textsuperscript{b,a}")
  ) %>% mutate(block = str_to_title(block))

kable_coefs(
  data = procedure_overview,
  caption = "Task Sequence in All Experiments", 
  colnames = c("Sequence", "Inconsistent", "2a - Consistent", "2b - Inconsistent", "Inconsistent"),
  general_footnote = "Randomly presented word numbers are given in parentheses to indicate number of words per task.",
  alphabet_footnote = c(
    "\\\\textsuperscript{,}", # hacky way to get a and b in the same footnote
    "Counterbalanced order of reading and spelling training.", 
    "Prior to Block 4, words were rerandomized and repartitioned."),
  headers_above_cols = c(" ", "Experiment 1", "Experiment 2" = 2, "Experiment 3"),
  align = c("l", rep("c", 5)),
  long_table = FALSE,
  bold_rows = FALSE,
  footnote_as_chunk = TRUE,
  hold_position = "HOLD_position"
)
```

### Data analysis

```{r r-references-notes, include = FALSE, warning = FALSE}
# r_refs(here("05_paper", "01_manuscript", "r-references.bib")) # make .bib file
my_citations <- cite_r(
  file = here("05_paper", "01_manuscript", "r-references.bib"), 
  pkgs = c(
    "tidyverse", 
    "lme4", 
    "lmerTest", 
    "brms", 
    "emmeans",
    "irr",
    "here", 
    "knitr", 
    "kableExtra", 
    "papaja", 
    "broom.mixed", 
    "english",
    "cowplot"
  ), 
  withhold = FALSE
)
```

We used `r my_citations` for data preparation, analysis, and presentation. All data processing and analyses were preregistered and are hosted on the Open Science Framework [https://osf.io/5mtdj/](https://osf.io/5mtdj/). Any deviations from our preregistered analysis plan are outlined and justified in the preregistration deviations documents.

## Results

### Coding

Two coders (GPW and VK) transcribed all reading responses while blind to each participant’s condition. A coding convention was adopted for the 13 target phonemes in the artificial language using the CPSAMPA [@Marian2012] simplified notation of IPA characters such that [\textipa{A}\textipa{E}i\textipa{O}\textipa{U}] became a, E, i, O, u while the consonants were coded using the letters m, n, s, k, b, d, f, l and x. All extraneous, that is, non-target phonemes were rendered by single Latin characters that provided the closest match so as to be able to compute nLEDs, which constitute a more gradual and fine-grained performance measure than error rates, allowing us to distinguish near-matches from entirely erroneous productions akin to cross-entropy errors in the neural network simulation by @Brown2015. We computed intercoder reliability by obtaining intraclass correlations between the two coders' nLEDs, using the *irr* R-package [@R-irr]. We used a single-score, absolute agreement, two-way random effects model based on the summed nLEDs for each participant. Intercoder reliability was `r summarise_icc(results$experiment_0$irr_results)`. The 95% confidence interval around the parameter estimate indicates that the ICC falls above the bound of .90, which suggests excellent reliability across coders [@Koo2016]. However, in instances of discrepancy between the coders we based further analyses on the smaller of the two nLEDs thereby adopting a lenient coding criterion based on the assumption that a participant's response counts as acceptable if at least one of the coders can match it to the target as closely as possible.

### Model Fitting

We performed separate analyses for the training and testing phases. Our dependent variable, the leniently coded nLED, was arcsine square root transformed to adjust for the bounded nature of values between 0 and 1. We performed frequentist and Bayesian analyses. Bayesian analyses, although not fully adopted as standard in studies of this kind, provide a range of additional advantages [@Nicenboim2016; @Vasishth2018]: Maximal random effect structures [@Barr2013] can be fitted without convergence problems and data can be interrogated directly for null-effects. In our description and interpretation of the results we will focus on those effects that reached significance in the frequentist analysis and had credible intervals that did not include 0 in the Bayesian analysis (marked in boldface in all tables).

For the frequentist analyses, we modelled the data with linear mixed effects models fitted using the *lme4* R-package [@R-lme4]. Statistical significance of each term was evaluated with *p*-values approximated using the Satterthwaite method implemented in the *lmerTest* R-package [@R-lmerTest]. We used the maximal random effects structure that allowed for model convergence throughout [@Barr2013]. 

For the Bayesian analyses, we fitted linear mixed-effects models using the *brms* R-package [@R-brms_a; @R-brms_b] with the same fixed effects as in the frequentist models and a maximal random effects structure. To simplify the definition of priors for the estimated parameters, we scaled and centred the dependent variable on a mean of 0 with a standard deviation of 1. We used a regularizing, weakly informative prior, $Normal(0, 1)$, for the intercept term. Additionally, we used an informative prior for all fixed effects terms, defined as $Normal(0, 0.2)$, except for fixed effects involving time-terms. This prior places a larger probability on small effects for the parameter estimates. For any fixed effects including time terms (i.e., each time term and any interactions of other effects with time terms in the training phase only), we used very weakly informative priors, defined as $Normal(0, 10)$, which allows these effects to be dominated by the likelihood. We also used regularizing priors for the correlation parameters, $LKJ(2)$, which downweights perfect correlations [@Vasishth2018]. Additionally, the standard deviations of random effects and the residual error used the default priors in *brms* for these terms at the time of writing. Specifically, these priors are defined as half Student's-*t* priors (i.e. constrained to be non-negative) with 3 degrees of freedom and, minimally, a scale parameter of 10. Without a predefined region of practical equivalence [@Kruschke2018], we use the 95% credible interval around the posterior mean to summarize these models. As @Nicenboim2016 note, the 95% credible interval provides the range of values within which the true value of the parameter lies with 95% probability given the model and data. Thus, when a 95% credible interval includes zero, we conclude that we do not have sufficient evidence against a null result. However, when a 95% credible interval does not include zero, we conclude that we have evidence for a non-zero directional effect [see @Burkner2019 for use of similar criteria][^7].

[^7]: We initially attempted to evaluate evidence for and against the null hypothesis for each term in our model using Bayes factors calculated using the *generalTestBF* function from the *BayesFactor* R-package [@R-BayesFactor]. However, this resulted in Bayes factors with a large proportional error. Following this, we calculated Bayes factors using the *hypothesis* function from the *brms* R-package (using the Savage-Dickey density ratio). However, as @Nicenboim2016 discuss, with wide, weakly informative priors, the Bayes factor will always favour the null hypothesis as the alternative hypothesis is penalized for including large (and unlikely) values in the prior. We therefore rely on the 95% credible interval, rather than Bayes factors, to interpret non-significant results.

#### Training

Training data were modelled using growth-curve analyses [e.g., @Mirman2014] to establish change in performance over time, i.e. from block to block, across conditions. Time was modeled using fixed effects of orthogonal linear and quadratic polynomials to capture the potential nonlinear change in performance over the six half-blocks of 10 words as learning progressed. Because interactions of quadratic terms with other fixed effects do not lend themselves to meaningful interpretation, they will not be considered further. The model also included sum-coded fixed effects of picture condition (picture vs. no picture), variety (match vs. mismatch), and word type (contrastive vs. noncontrastive). We used nested fixed effects for these terms [see @Schad2018 for a discussion of this approach], with word type nested within the interaction between all other fixed effects. As a result of this parameterization, the intercept represents the average of condition means throughout the entire time window (and not at the first block), individual terms (except word type) represent main effects for the given term, and word type effects represent simple effects within each combination of the other factors (e.g., word type within each level of picture and variety conditions over the entire time). All other interactions aside from those involving word type are interpreted as usual. In the frequentist model, the random effect structure included zero-correlation random intercepts and slopes of picture condition, variety condition, and their interaction by items, and random intercepts, slopes (including correlations) for the linear and quadratic time terms, word type, and their interaction by participants. The results of the models including parameter estimates, confidence intervals (for the frequentist analysis), and credible intervals
T2 (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex0-train).

Our results show reduction of nLEDs across six half-blocks of training indicating that participants’ reading of the artificial script improved over time. The significant quadratic term suggests that in many instances more progress was made between Blocks 1 and 2 than between Blocks 2 and 3. Crucially, the contrastive deficit was significant in the variety mismatch condition without pictures and marginally significant in the variety mismatch condition with pictures, broadly confirming greater difficulties with F1 reading contrastive words (see Figure\ \@ref(fig:ex0-train-plots)). In addition, there was a significant three-way interaction between block, picture condition, and variety condition, which, however, is not of interest to the main questions of this study.

```{r ex0-train, results = "asis", warning = FALSE}
ex0_train_nhst <- results$experiment_0$main_models$lme_train %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex0_train_bayes <- 
  results$experiment_0$planned_comparisons_Bayes$brm_train$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex0_train <- bind_cols(ex0_train_nhst, ex0_train_bayes) %>% 
  rename_nested_terms()

ex0_train_caption <- "Parameter Estimates for the Models Fitted to nLEDs From the Training Phase in Experiment 1"

kable_coefs(
  ex0_train, 
  caption = ex0_train_caption,
  colnames = shared_colnames, 
  general_footnote = ex0_foot, 
  headers_above_cols = header_above,
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 4.8,
  output_format = "latex", 
  align = table_alignment,
  hold_position = "HOLD_position"
  )
```

```{r ex0-train-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for reading training of contrastive and noncontrastive words over three training blocks (coded as six half-blocks in the analyses but presented as three blocks for comparability with Experiment 2) in the variety match and mismatch conditions in Experiment 1. Error bars indicate $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp"}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-0_training_plot_reading.png"
))
```

#### Testing

For the analysis of the testing phase, we used the same fixed effect structure as for the analysis of the training phase with the exclusion of the linear and quadratic effects of block. The only difference was that here word type was modeled using Helmert contrasts, such that contrastive words were compared with noncontrastive words and untrained words were compared with the average of contrastive and noncontrastive words (i.e., the trained words). For the testing phase, the random effects structure included random intercepts and slopes of picture condition, variety condition, and their interaction by items, and random intercepts and slopes of word type by participants. Parameter estimates, confidence intervals (for the frequentist analysis), and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex0-test).

\vspace{6ex}
\newpage

```{r ex0-test, results = "asis", warning = FALSE}
ex0_test_nhst <- results$experiment_0$main_models$lme_test %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex0_test_bayes <- 
  results$experiment_0$planned_comparisons_Bayes$brm_test$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex0_test <- bind_cols(ex0_test_nhst, ex0_test_bayes) %>% 
  rename_nested_terms()

ex0_test_caption <- "Parameter Estimates for the Models Fitted to nLEDs From the Testing Phase in Experiment 1"

kable_coefs(
  ex0_test, 
  caption = ex0_test_caption,
  colnames = shared_colnames, 
  general_footnote = ex0_foot, 
  headers_above_cols = header_above,
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 5,
  output_format = "latex", 
  align = table_alignment,
  hold_position = "HOLD_position"
  )
```

We found that the contrastive deficit failed to reach significance in the variety mismatch condition. The effect of word familiarity was significant in the variety match condition with pictures and fell short of significance in the variety mismatch condition with pictures suggesting that participants were able to capitalize on knowledge of the phonological form of trained items either by using partial phonological decoding or direct access from the depicted meaning (see Figure\ \@ref(fig:ex0-test-plots)). 

```{r ex0-test-novel, include = FALSE, warning = FALSE}
ex0_novel_nhst <- results$experiment_0$planned_comparisons_NHST$test_l_n %>% 
  summarise_lme(., term, main_terms) %>%
  filter(term == "Variety Condition")

ex0_novel_bayes <- 
  results$experiment_0$planned_comparisons_Bayes$brm_test_novel$fixed %>% 
  summarise_bayes(., term, main_terms) %>%
  filter(term == "Variety Condition")
```

```{r ex0-test-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for reading testing of trained noncontrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions in Experiment 1. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-0_testing_plot_reading.png"
))
```

We performed a planned direct comparison of performance on untrained words only between the variety match and variety mismatch conditions. The model included fixed effects and interactions between the sum-coded picture conditions and variety conditions. We used the same criteria as in our main models for determining the random effects structure of the model. Here, this took the form of random zero-correlation intercepts and slopes of picture condition and variety condition and their interaction by items, and random intercepts by participants. This comparison showed no effect of variety mismatch (frequentist estimate: $\hat{\beta}$ = `r ex0_novel_nhst$estimate`, 95%CI = `r ex0_novel_nhst$"95% CI"`, *t* = `r ex0_novel_nhst$statistic`, *p* = `r ex0_novel_nhst$p.value`; Bayesian Estimate: $\hat{\beta}$ = `r ex0_novel_bayes$Estimate`, 95% CI = `r ex0_novel_bayes$"95% CI"`), thus failing to obtain conclusive evidence for a detrimental effect of dialect exposure on phonological decoding skills.

## Discussion

In this experiment, participants learned to read 30 words of an artificial language using an artificial script. In the variety match condition, words presented during reading training were identical to words presented during exposure while in the variety mismatch condition half of the words varied between exposure and literacy acquisition mimicking dialect exposure. Half of the participants in each variety condition saw pictures when hearing and reading the words enabling them to develop semantic representations while the other half did not. Reading performance improved significantly over the course of training in both variety conditions although the gains were steeper in the variety mismatch condition with pictures. We had predicted that performance would be worse for contrastive compared to noncontrastive words in the variety mismatch condition. While the results confirmed this trend, the contrastive deficit only reached significance during training in the no picture condition, thus replicating findings from the reading experiment and the connectionist simulation of AAE exposure by @Brown2015. Recall that in that simulation the contrastive deficit arose solely from similarity between the phonological representations of the AAE and MAE variants and not from competition between word forms associated with the same meaning. Our experiment was not able to unequivocally establish whether a contrastive deficit persists when meanings were provided by pictures as we only observed it in the no picture condition during training but not reliably during testing.

We only observed a word familiarity benefit in the variety match condition with pictures. In natural languages, faster reading of high-frequency, familiar words compared with low-frequency words or nonwords indicates the strength of the direct lexical route [@adelman2014individual; @caravolas2018growth]. This lexicality effect is either due to more efficient, larger-grained processing of more familiar orthographic forms or the result of tighter links to word meanings in familiar words. In contrast, unfamiliar words require serial decoding of graphemes. In this experiment, links to word meanings could only be established in picture conditions. In no picture conditions, benefits for trained words could arise either through greater acquired decoding efficiency or through partial decoding, for example, when seeing the artificial equivalent of *BLEKUS*, participants may first decode *B* as /b/ and then *L* as /l/, at which point the phonological form /bl\textipa{E}kus/ (or the contrastive variant /blixus/ in the variety mismatch condition) may be recognized. The fact that the word familiarity benefit occurred only in picture conditions (reliably in the variety match and marginally in the variety mismatch condition) indicates that lexicality benefits arose only when access to phonological forms could be mediated by meanings. The absence of word familiarity effects in the no picture conditions suggest that neither more efficient decoding strategies nor word recognition after partial decoding had a chance to emerge.

Our main question was whether exposure to competing dialect variants would affect learners’ emerging phonological decoding skills. To answer this question, we compared reading performance for untrained words between the variety match and mismatch conditions. If dialect exposure hinders reading skills in general, as suspected by the Head Teacher mentioned in our introductory paragraph, we would expect poorer performance with untrained words in the variety mismatch condition. Instead, we observed no difference to the variety match condition, although Bayesian estimates of the strength of evidence for the null hypothesis indicated that there was insufficient evidence for a null effect. We therefore can neither confirm nor exclude the possibility that dialect exposure impairs decoding skills.

As this experiment was a conceptual replication of the simulation of learning to read in @Brown2015, it is not clear how well the findings of no difference between the variety match and mismatch conditions generalize in the absence of spelling training. Moreover, in this experiment, learning grapheme-phoneme mappings was made difficult by the inconsistent orthography designed to mimic an orthography like English. Recall that we implemented two conditional rules according to which grapheme-phoneme and phoneme-grapheme mappings changed depending on context. These complex conditional rules likely further discouraged discovery and use of grapheme-phoneme conversion. To promote learning of such rules and to encourage phonologically mediated reading, we included spelling into Experiment 2. Participants were trained with an entirely consistent orthography in Experiment 2a and with the same inconsistent orthography in Experiment 2b, to examine whether effects of dialect exposure are similar for different levels of orthographic consistency.

# Experiment 2: Effect of variety mismatch on learning to read and spell

The aim of Experiment 2 was to provide more ecologically valid literacy training conditions by examining how exposure to variety mismatch affects learning to read and to spell a consistent (Experiment 2a) and an inconsistent (Experiment 2b) orthography.

# Experiment 2a: Consistent Orthography

## Method

### Participants

`r stringr::str_to_sentence(as.english(results$experiment_1$demographics$demographic_summary$n, UK = TRUE))` participants (aged `r results$experiment_1$demographics$demographic_summary$age_min`--`r results$experiment_1$demographics$demographic_summary$age_max`, *M* = `r results$experiment_1$demographics$demographic_summary$age_mean`, *SD* = `r results$experiment_1$demographics$demographic_summary$age_sd`, with `r results$experiment_1$demographics$gender_count %>% filter(gender == "f") %>% pull(n)` self-reported as female, `r results$experiment_1$demographics$gender_count %>% filter(gender == "m") %>% pull(n)` self-reported as male, and `r results$experiment_1$demographics$gender_count %>% filter(gender == "o") %>% pull(n)` self-reported as other) were recruited from Amazon's Mechanical Turk crowdsourcing platform and took part in the study for \$7.50. Participants' mean English proficiency on a 1-5 Likert scale was `r results$experiment_1$demographics$english_proficiency$mean` (*SD* = `r results$experiment_1$demographics$english_proficiency$SD`, range `r results$experiment_1$demographics$english_proficiency$min`--`r results$experiment_1$demographics$english_proficiency$max`). Only `r results$multiple_experiments$demographic_checks$english_proficiency_counts %>% filter(experiment == 1) %>% filter(language_proficiency_rating != 5) %>% pull(n) %>% sum() %>% english()` participants rated their English proficiency as below 5. `r results$multiple_experiments$demographic_checks$known_language_category %>% filter(experiment == 1) %>% filter(speaker_category == "monolingual") %>% pull(n) %>% english() %>% stringr::str_to_sentence()` participants reported knowing only English while `r results$multiple_experiments$demographic_checks$known_language_category %>% filter(experiment == 1) %>% filter(speaker_category == "multilingual") %>% pull(n)` participants also knew Spanish (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 1, language_spoken == "Spanish") %>% pull(n) %>% sum()` times), French (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 1, language_spoken == "French") %>% pull(n) %>% sum()` times), Hindi (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 1, language_spoken == "Hindi") %>% pull(n) %>% sum()` times), and `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 1, language_spoken %nin% c("Spanish", "French", "Hindi", "English")) %>% filter(!str_detect(language_spoken, "\\(")) %>% summarise(n = length(unique(language_spoken))) %>% pull(n) %>% english()` other languages (listed a total of `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 1, language_spoken %nin% c("Spanish", "French", "Hindi", "English", "None")) %>% pull(n) %>% sum()` times). Only `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 1, language_spoken %in% c("Chinese", "Chinese (Mandarin)", "Japanese")) %>% pull(n) %>% sum() %>% english()` participants were familiar with logographic scripts. Another `r results$multiple_experiments$exclusions %>% filter(id == "ex_1") %>% summarise(n = length(unique(participant_number))) %>% pull(n) %>% english()` participants were tested and excluded based on the criteria described for Experiment 1.

### Procedure

The procedure was identical to Experiment 1 aside from the following deviation: During training, each 10-word block was presented once for reading and once for spelling (see Table\ \@ref(tab:table-procedure-overview)). During spelling training participants heard a word and had to type it by clicking graphemes using an on-screen keyboard. Participants in the picture condition always saw the picture of the associated referent when hearing the word. Once participants had pressed the on-screen “enter” key the correct spelling appeared below their own spelling for purposes of feedback. The feedback screen was cleared after 1.5 to 3.0 s to prevent participants from taking notes or obtaining screenshots (the exact presentation time of the feedback was determined dynamically based on the word length, with a duration of 500 ms per letter so that, e.g., the correct spelling of a four-letter-word would be presented for 2 s). The overall amount of exposure to each item, combining presentations for reading and spelling, was identical to Experiment 1. In the testing phase, participants were presented with all 30 training words and an additional 12 untrained words in randomized order for reading and for spelling. Order of reading and spelling tasks was counterbalanced across participants but was kept constant across all phases within participants resulting in pseudorandom assignment of participants to 16 conditions comprising a crossing of variety condition, picture condition, speaker voice, and task order. The mean completion time was `r results$experiment_1$demographics$demographic_summary %>% pull(time_mean) %>% as.numeric("minutes")` min (*SD* = `r results$experiment_1$demographics$demographic_summary %>% pull(time_sd) %>% as.numeric("minutes")`).

## Results

### Coding

We used the same coding scheme for reading responses as in Experiment 1. The ICC between coders was `r summarise_icc(results$experiment_1$irr_results)`. The 95% confidence interval around the parameter estimate indicates that the ICC falls above the bound of .90, which suggests excellent reliability across coders [@Koo2016]. Spelling responses were analyzed by computing length-normalized Levenshtein Edit Distances between response and target sequences of graphemes.

### Model Fitting

Model fitting was similar to Experiment 1, with the exception of the inclusion of a sum-coded fixed effect of task (reading vs. spelling) and of random slopes of task. Additionally, because the training phase contained three training blocks per task, the training models were changed to include only an orthogonal linear (and not quadratic) time term as a fixed and random effect, to avoid overfitting change over time based on only three time points. Word type was nested within the combination of variety, picture, and task conditions. We used maximal random effect structure comprising random intercepts and slopes of all fixed effects by participants and items, with zero-correlation between intercepts and slopes where appropriate to avoid nonconvergence. The Bayesian mixed effects models used the same priors as in Experiment 1, with the addition of informative, $Normal(0, 0.2)$ priors on the fixed effect of task and any interactions of other terms with this factor.

#### Training

Parameter estimates, confidence intervals (for the frequentist analysis), and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex1-train). The results showed a main effect of T4 block, indicating an overall improvement of performance as training progressed, as well as a main effect of task demonstrating better performance for reading than for spelling. Crucially, as indicated by the effect of word type, we found that reading, but not spelling, of contrastive words was significantly impaired in the variety mismatch conditions with and without pictures. In the picture condition, the effect of word type in reading in the variety mismatch condition interacted with block reflecting the fact that impaired performance for contrastive words started to manifest itself gradually over the course of training (see Figures\ \@ref(fig:ex1-train-reading-plots) and \@ref(fig:ex1-train-spelling-plots)). 

```{r ex1-train, results = "asis", warning = FALSE}
ex1_train_nhst <- results$experiment_1$main_models$lme_train %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex1_train_bayes <- 
  results$experiment_1$planned_comparisons_Bayes$brm_train$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex1_train <- bind_cols(ex1_train_nhst, ex1_train_bayes) %>% 
  rename_nested_terms()

ex1_train_caption <- "Parameter Estimates for the Models Fitted to nLEDs From the Training Phase in Experiment 2a"

kable_coefs(
  ex1_train, 
  caption = ex1_train_caption,
  colnames = shared_colnames, 
  general_footnote = general_foot, 
  headers_above_cols = header_above,
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 5,
  output_format = "latex", 
  align = table_alignment,
  hold_position = "HOLD_position"
  )
```

```{r ex1-train-reading-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for reading of contrastive and noncontrastive words during three training blocks in the variety match and variety mismatch conditions in Experiment 2a. Error bars indicate $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-1_training_plot_reading.png"
))
```

```{r ex1-train-spelling-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for spelling of contrastive and noncontrastive words during three training blocks in the variety match and variety mismatch conditions in Experiment 2a. Error bars indicate $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-1_training_plot_spelling.png"
))
```

#### Testing

Parameter estimates, confidence intervals (for the frequentist analysis), and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex1-test). The results confirmed the main T5 effect of task observed during training which showed that performance was superior for reading compared to spelling. As during training, we found an effect of word type in the variety mismatch condition in reading but not in spelling, but only when pictures were present. The effect of word familiarity was significant in all conditions except for spelling in the variety mismatch condition with pictures, although Bayesian analyses failed to corroborate it for spelling in the variety match condition without pictures (see Figures\ \@ref(fig:ex1-test-reading-plots) and \@ref(fig:ex1-test-spelling-plots)).

```{r ex1-test, results = "asis", warning = FALSE}
ex1_test_nhst <- results$experiment_1$main_models$lme_test %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex1_test_bayes <- 
  results$experiment_1$planned_comparisons_Bayes$brm_test$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex1_test <- bind_cols(ex1_test_nhst, ex1_test_bayes) %>% 
  rename_nested_terms()

ex1_test_caption <- "Parameter Estimates for the Models Fitted to nLEDs From the Testing Phase in Experiment 2a"

kable_coefs(
  ex1_test, 
  caption = ex1_test_caption,
  colnames = shared_colnames, 
  general_footnote = general_foot, 
  headers_above_cols = header_above,
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 5,
  output_format = "latex", 
  align = table_alignment,
  hold_position = "HOLD_position"
  )
```

```{r ex1-test-reading-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for testing reading performance for trained noncontrastive, trained contrastive, and untrained words in the variety match and variety mismatch conditions in Experiment 2a. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-1_testing_plot_reading.png"
))
```

```{r ex1-test-spelling-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for testing spelling performance for trained noncontrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions in Experiment 2a. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-1_testing_plot_spelling.png"
))
```

```{r ex1-test-novel, include = FALSE, warning = FALSE}
ex1_novel_nhst <- results$experiment_1$planned_comparisons_NHST$test_l_n %>% 
  summarise_lme(., term, main_terms) %>%
  filter(term == "Variety Condition")

ex1_novel_bayes <- 
  results$experiment_1$planned_comparisons_Bayes$brm_test_novel$fixed %>% 
  summarise_bayes(., term, main_terms) %>%
  filter(term == "Variety Condition")
```

As in Experiment 1, we performed a planned direct comparison of performance on untrained words between all variety match and variety mismatch conditions. The model included fixed effects and interactions between the sum-coded levels of task condition, picture condition, and variety condition. We used the same criteria as in our main models for determining the random effects structure. Here, this took the form of zero-correlation random intercepts and slopes of task condition, picture condition, variety condition, and their interaction by items, as well as random intercepts by participants. As in Experiment 1, the effect of variety condition provided no evidence for a detrimental effect of a variety mismatch on reading and spelling of untrained words (frequentist estimate: $\hat{\beta}$ = `r ex1_novel_nhst$estimate`, 95% CI = `r ex1_novel_nhst$"95% CI"`, *t* = `r ex1_novel_nhst$statistic`, *p* = `r ex1_novel_nhst$p.value`; Bayesian Estimate: $\hat{\beta}$ = `r ex1_novel_bayes$Estimate`, 95% CI = `r ex1_novel_bayes$"95% CI"`).

## Discussion

When spelling was introduced to literacy training in a consistent artificial orthography, the contrastive deficit emerged in reading when participants encountered a variety mismatch, which persisted into the testing session. However, unlike the contrastive reading deficit found by @Brown2015 for children and neural networks exposed to both AAE and MAE here it was more persistent when meanings were provided by pictures. Notably, no contrastive deficit emerged for spelling. This is because no competing orthographic representations for words in the exposure variety (i.e., the “dialect”) existed and learners likely engaged in serial conversion of phonemes into graphemes. We had expected that introducing spelling would facilitate reliance on grapheme-phoneme decoding during reading, which should have attenuated the word familiarity effect. Yet the effect of word familiarity was significant in all reading conditions and even some of the spelling conditions. This is at odds with cross-linguistic findings of children learning to read [@caravolas2018growth] where lexicality effects were greater in the inconsistent orthography (English) compared to the consistent ones (Czech and Slovak). We suspect that the more consistent orthography may have encouraged more frequent partial decoding, that is, decoding of just enough graphemes to access the memorized word form, which benefitted trained but not untrained items. In spelling, the word familiarity effect is somewhat puzzling but may reflect emerging representations of the overall graphemic Gestalt or even the motor routines required to type a word. Most relevant to the main question of the study, as in Experiment 1, similar reading and spelling performance with untrained words in the variety match and mismatch conditions suggests that concurrent exposure to another variety did not seem to have any further detrimental effect on whatever decoding skills participants had acquired.

# Experiment 2b: Opaque Orthography

## Method

### Participants

`r stringr::str_to_sentence(as.english(results$experiment_2$demographics$demographic_summary$n, UK = TRUE))` participants (aged `r results$experiment_2$demographics$demographic_summary$age_min`--`r results$experiment_2$demographics$demographic_summary$age_max`, *M* = `r results$experiment_2$demographics$demographic_summary$age_mean`, *SD* = `r results$experiment_2$demographics$demographic_summary$age_sd`, with `r results$experiment_2$demographics$gender_count %>% filter(gender == "f") %>% pull(n)` self-reported as female, `r results$experiment_2$demographics$gender_count %>% filter(gender == "m") %>% pull(n)` and self-reported as male) were recruited from Amazon's Mechanical Turk crowdsourcing platform and took part in the study for \$7.50. Participants' mean English proficiency on a 1-5 Likert scale was `r results$experiment_2$demographics$english_proficiency$mean` (*SD* = `r results$experiment_2$demographics$english_proficiency$SD`, range `r results$experiment_2$demographics$english_proficiency$min`--`r results$experiment_2$demographics$english_proficiency$max`). Only `r results$multiple_experiments$demographic_checks$english_proficiency_counts %>% filter(experiment == 2) %>% filter(language_proficiency_rating != 5) %>% pull(n) %>% sum() %>% english()` participants rated their English proficiency as below 5. `r results$multiple_experiments$demographic_checks$known_language_category %>% filter(experiment == 2) %>% filter(speaker_category == "monolingual") %>% pull(n) %>% english() %>% stringr::str_to_sentence()` participants reported knowing only English while `r results$multiple_experiments$demographic_checks$known_language_category %>% filter(experiment == 2) %>% filter(speaker_category == "multilingual") %>% pull(n)` participants also knew Spanish (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 2, language_spoken == "Spanish") %>% pull(n) %>% sum()` times), Hindi (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 2, language_spoken == "Hindi") %>% pull(n) %>% sum()` times), Tamil (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 2, language_spoken == "Tamil") %>% pull(n) %>% sum()` times), and `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 2, language_spoken %nin% c("Spanish", "Hindi", "Tamil", "English")) %>% summarise(n = length(unique(language_spoken))) %>% pull(n) %>% english()` other languages (listed a total of `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 2, language_spoken %nin% c("Spanish", "Hindi", "Tamil", "English")) %>% pull(n) %>% sum()` times). Only `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 2, language_spoken %in% c("Chinese", "Chinese (Mandarin)", "Japanese")) %>% pull(n) %>% sum() %>% english()` participants were familiar with logographic scripts. Another `r results$multiple_experiments$exclusions %>% filter(id == "ex_2") %>% summarise(n = length(unique(participant_number))) %>% pull(n) %>% english()` participants were tested and excluded based on the criteria described for Experiment 1.

### Materials

Graphemes, words, and pictures were identical to the previous two experiments. We used the same inconsistent orthography as in Experiment 1.

### Procedure

The procedure was identical to Experiment 2a. The mean completion time was `r results$experiment_2$demographics$demographic_summary %>% pull(time_mean) %>% as.numeric("minutes")` min (*SD* = `r results$experiment_2$demographics$demographic_summary %>% pull(time_sd) %>% as.numeric("minutes")`).

## Results

```{r ex2-train-reading-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for reading of contrastive and noncontrastive words during three training blocks in the variety match and variety mismatch conditions in Experiment 2b. Error bars indicate $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
# placed here for better figure alignment in text
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-2_training_plot_reading.png"
))
```

```{r ex2-train-spelling-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for spelling of contrastive and noncontrastive words during three training blocks in the variety match and variety mismatch conditions in Experiment 2b. Error bars indicate $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-2_training_plot_spelling.png"
))
```

### Coding

We used the same coding scheme for reading responses as in the previous experiments. The ICC between coders was `r summarise_icc(results$experiment_2$irr_results)`. The 95% confidence interval around the parameter estimate indicates that the ICC falls above the bound of .90, which suggests excellent reliability across coders [@Koo2016]. Spelling responses were analyzed by computing length-normalized Levenshtein Edit Distances between response and target sequences of graphemes.

### Model Fitting

Frequentist and Bayesian analyses were conducted in the same way as for Experiment 2b. In the frequentist analyses, there were minor differences in the random effects structure compared to Experiment 2a due to differences in convergence: For the training phase, the maximal converging random effects structure included correlations between all by-participant terms. For the testing phase, correlations between all random effect terms had to be suppressed to avoid nonconvergence.

#### Training

```{r ex2-train-posthocs, include = FALSE, warning = FALSE}
ex2_train_posthocs_test <- 
  results$experiment_2$post_hoc_comparisons$train_pt_pair %>%
  pairs(by = NULL, adjust = "holm") %>%
  as_tibble() %>%
  filter(p.value < .05)

ex2_train_posthocs_confint <- 
  results$experiment_2$post_hoc_comparisons$train_pt_pair %>%
  pairs(by = NULL, adjust = "holm") %>%
  confint()

ex2_train_posthocs <- left_join(
  ex2_train_posthocs_test, 
  ex2_train_posthocs_confint,
  by = c("contrast", "estimate", "SE", "df")
) %>%
  merge_CI_limits(., "lower.CL", "upper.CL") %>%
  mutate(p.value = papa(p.value, asterisk = FALSE))
```

Parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex2-train). 

```{r ex2-train, results = "asis", warning = FALSE}
ex2_train_nhst <- results$experiment_2$main_models$lme_train %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex2_train_bayes <- 
  results$experiment_2$planned_comparisons_Bayes$brm_train$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex2_train <- bind_cols(ex2_train_nhst, ex2_train_bayes) %>% 
  rename_nested_terms()

ex2_train_caption <- "Parameter Estimates for the Models Fitted to nLEDs From the Training Phase in Experiment 2b"

kable_coefs(
  ex2_train, 
  caption = ex2_train_caption,
  colnames = shared_colnames, 
  general_footnote = general_foot, 
  headers_above_cols = header_above,
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 5,
  output_format = "latex", 
  align = table_alignment,
  hold_position = "HOLD_position"
  )
```

As in Experiment 2a, the main effect of block indicated improvement in performance over the course of training and the main effect of task confirmed that learning to spell was more difficult than learning to read. The only other significant effect was an interaction between task and picture condition. Pairwise contrasts based on the estimated marginal means of the training model were calculated using the *emmeans* R-package [@R-emmeans], using Holm's sequential Bonferroni correction. These contrasts indicate that reading performance was better than spelling performance in the picture condition only (picture, reading-spelling: $\Delta{M}$ = `r ex2_train_posthocs$estimate`, 95% CI =`r ex2_train_posthocs$"95% CI"`, *t* = `r ex2_train_posthocs$t.ratio`, *p* `r ex2_train_posthocs$p.value`). All other contrasts were non-significant (*p* > .05). Unlike Experiment 1, we did not find any evidence for a contrastive deficit (see Figures\ \@ref(fig:ex2-train-reading-plots) and \@ref(fig:ex2-train-spelling-plots)).

#### Testing

```{r ex2-test-posthocs, include = FALSE, warning = FALSE}
ex2_test_posthocs_test <- 
  results$experiment_2$post_hoc_comparisons$test_pt_pair %>%
  pairs(by = NULL, adjust = "holm") %>%
  as_tibble() %>%
  filter(p.value < .05)

ex2_test_posthocs_confint <- 
  results$experiment_2$post_hoc_comparisons$test_pt_pair %>%
  pairs(by = NULL, adjust = "holm") %>%
  confint()

ex2_test_posthocs <- left_join(
  ex2_test_posthocs_test, 
  ex2_test_posthocs_confint,
  by = c("contrast", "estimate", "SE", "df")
) %>%
  merge_CI_limits(., "lower.CL", "upper.CL") %>%
  mutate(p.value = papa(p.value, asterisk = FALSE))
```

Parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex2-test). The results confirmed the interaction between task and picture condition found already in the training data which suggests that reading performance was better than writing performance in the picture condition only (picture, reading-spelling: $\Delta{M}$ = `r ex2_test_posthocs$estimate``r ex2_test_posthocs$"95% CI"`, *t* = `r ex2_test_posthocs$t.ratio`, *p* = `r ex2_test_posthocs$p.value`). However, unlike Experiment 2a, there was no contrastive deficit and the effect of word familiarity appeared only in one condition, i.e. during reading in the variety match condition with pictures (see Figures\ \@ref(fig:ex2-test-reading-plots) and \@ref(fig:ex2-test-spelling-plots)).

```{r ex2-test, results = "asis", warning = FALSE}
ex2_test_nhst <- results$experiment_2$main_models$lme_test %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex2_test_bayes <- 
  results$experiment_2$planned_comparisons_Bayes$brm_test$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex2_test <- bind_cols(ex2_test_nhst, ex2_test_bayes) %>% 
  rename_nested_terms()

ex2_test_caption <- "Parameter Estimates for the Models Fitted to nLEDs From the Testing Phase in Experiment 2b"

kable_coefs(
  ex2_test, 
  caption = ex2_test_caption,
  colnames = shared_colnames, 
  general_footnote = general_foot, 
  headers_above_cols = header_above,
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 5,
  output_format = "latex", 
  align = table_alignment,
  hold_position = "HOLD_position"
  )
```

```{r ex2-test-reading-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for testing reading performance for trained noncontrastive, trained contrastive, and untrained words in the variety match and variety mismatch conditions in Experiment 2b. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-2_testing_plot_reading.png"
))
```

```{r ex2-test-spelling-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for testing spelling performance for trained noncontrastive, trained contrastive, and untrained words in the variety match and variety mismatch conditions in Experiment 2b. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-2_testing_plot_spelling.png"
))
```

```{r ex2-test-novel, include = FALSE, warning = FALSE}
ex2_novel_nhst <- results$experiment_2$planned_comparisons_NHST$test_l_n %>% 
  summarise_lme(., term, main_terms) %>%
  filter(term == "Variety Condition")

ex2_novel_bayes <- 
  results$experiment_2$planned_comparisons_Bayes$brm_test_novel$fixed %>% 
  summarise_bayes(., term, main_terms) %>%
  filter(term == "Variety Condition")
```

The planned comparison of performance on untrained words only between all variety match and variety mismatch conditions used the same model structure as for Experiment 2a. There was no effect of variety condition (frequentist estimate: $\hat{\beta}$ = `r ex2_novel_nhst$estimate`, 95% CI = `r ex2_novel_nhst$"95% CI"`, *t* = `r ex2_novel_nhst$statistic`, *p* = `r ex2_novel_nhst$p.value`; Bayesian Estimate: $\hat{\beta}$ = `r ex2_novel_bayes$Estimate`, 95% CI = `r ex2_novel_bayes$"95% CI"`), again suggesting that there was no evidence for a detrimental effect of exposure to a variety mismatch on reading and spelling of untrained words.

## Discussion

When attempting to learn to read and to spell an inconsistent artificial orthography, participants showed improvement over the course of training. However, unlike under reading-only conditions in Experiment 1, where the contrastive deficit was found in some of the variety mismatch conditions, we found no contrastive deficit in this experiment. It is possible that learning conditional rules in reading and spelling rendered literacy acquisition too difficult to allow for the establishment of phonological representations that could have been placed into competition with each other, even when pictures provided meanings. Such an explanation is certainly in line with cross-linguistic studies of literacy acquisition in children, which show that at the early stages learning is more difficult for inconsistent compared with more consistent orthographies [@KSeymour2003]. In our experiment, where the artificial words were also novel, this may have hindered word learning; without more or less stable representations competition cannot occur. Again, as in Experiment 2a, there was no effect of variety mismatch on reading and spelling of untrained words suggesting that whatever weak decoding skills had been acquired remained unaffected by the presence of dialect variant words in the input.

Although this was not the main aim of this study, combining the first three experiments gives us the opportunity to explore whether orthographic consistency or spelling training are more conducive to literacy acquisition. Figure\ \@ref(fig:ex1-to-2-test-plots) shows a direct comparison of reading performance for all trained and untrained items during testing in the three experiments. To obtain statistical evidence for
the comparison, we first fitted a linear mixed effect model with sum-coded fixed effects of word familiarity and treatment-coded fixed effects experiment (1, 2a, 2b), and with a maximal random effects structure of random intercepts and slopes of experiment by item and random intercepts and slopes of word familiarity by participants. Pairwise contrasts were then calculated for each experiment separately for trained versus untrained words based on the estimated marginal means from the model using the *emmeans* R-package [@R-emmeans]. The results of these contrasts are provided in Table\ \@ref(tab:ex1-to-2-test), where *p*-values are adjusted using Holm's sequential Bonferroni correction.

\clearpage

These contrasts show that for trained words, performance was better in Experiment 2a compared with Experiment 1 and to Experiment 2b. All other differences were nonsignificant, suggesting that while introduction of spelling had no effect on overall reading outcomes, learning a consistent orthography led to measurable benefits, albeit only for trained words, regardless of whether spelling training was provided or not. Recall that the contrastive deficit also emerged most reliably with the consistent orthography suggesting that in this paradigm, phonological skill contributed to word learning, and competition between variants emerges only once learning has progressed to a stage at which access to phonological representations, either via (partial) decoding of the orthographic form or via semantic representations, is possible. However, the considerable variability in performance, evident in all figures, compellingly shows that participants differ tremendously in terms of their success at the early stages of this process. To create conditions that would allow for more reliable establishment of phonological representations, Experiment 3 repeated Experiment 2b with a longer training phase and a larger sample of learners, expecting to see a more reliable emergence of the contrastive deficit.

```{r ex1-to-2-test, results = "asis", warning = FALSE}
ex1_to_2_test <- results$multiple_experiments$test_en_pair %>% 
  pairs(by = "test_words", adjust = "holm") %>%
  as_tibble()

ex1_to_2_ci <- results$multiple_experiments$test_en_pair %>% 
  pairs(by = "test_words", adjust = "holm") %>%
  confint()

ex1_to_2_results <- left_join(
  ex1_to_2_test, 
  ex1_to_2_ci,
  by = c("contrast", "test_words", "estimate", "SE", "df")
  ) %>%
  merge_CI_limits(., "lower.CL", "upper.CL") %>%
  mutate(p.value = papa(p.value, asterisk = FALSE)) %>%
  arrange(desc(test_words)) %>%
  select(contrast, estimate, SE, "95% CI", t.ratio, p.value) %>%
  rename_many(contrast, c("ex" = "Experiment ")) %>%
  rename_many(contrast, c("2" = "2b", "1" = "2a", "0" = "1")) # order matters

ex1_to_2_caption <- "Parameter Estimates for Pairwise Contrasts Between Experiments 1, 2a, and 2b as a Function of Word Familiarity in the Testing Phase"
ex1_to_2_foot <- "Lines in boldface indicate significant effects in frequentist analyses"

kable_coefs(
  ex1_to_2_results, 
  caption = ex1_to_2_caption,
  colnames = c(
    "Contrast", 
    "$\\Delta M$", 
    "$SE$", 
    "95\\% Conf. I", 
    "$t$", 
    "$p$"
  ), 
  general_footnote = ex1_to_2_foot, 
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 5,
  output_format = "latex", 
  align = c("l", rep("c", 5)),
  hold_position = "HOLD_position"
  ) %>% 
  kableExtra::group_rows("Trained Words", 1, 3, bold = FALSE) %>%
  kableExtra::group_rows("Untrained Words", 4, 6, bold = FALSE) %>%
  add_indent(c(1:6))
```

```{r ex1-to-2-test-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for reading testing performance in trained and untrained words in Experiments 1, 2a, and 2b. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "!htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiments-1-to-2_word-novelty.png"
))
```

# Experiment 3

## Method

### Participants

`r stringr::str_to_sentence(as.english(results$experiment_3$demographics$demographic_summary$n, UK = TRUE))` participants (aged `r results$experiment_3$demographics$demographic_summary$age_min`--`r results$experiment_3$demographics$demographic_summary$age_max`, *M* = `r results$experiment_3$demographics$demographic_summary$age_mean`, *SD* = `r results$experiment_3$demographics$demographic_summary$age_sd`, with `r results$experiment_3$demographics$gender_count %>% filter(gender == "f") %>% pull(n)` self-reported as female, `r results$experiment_3$demographics$gender_count %>% filter(gender == "m") %>% pull(n)` self-reported as male, and `r results$experiment_3$demographics$gender_count %>% filter(gender == "o") %>% pull(n)` self-reported as other) were recruited from the crowdsourcing platform Prolific Academic and took part in the study for \pounds9.00. All participants reported English as their native language, and had a self-rated mean English proficiency on a 1-5 Likert scale of `r results$experiment_3$demographics$english_proficiency$mean` (*SD* = `r results$experiment_3$demographics$english_proficiency$SD`, range `r results$experiment_3$demographics$english_proficiency$min`--`r results$experiment_3$demographics$english_proficiency$max`). Participants reported no known mild cognitive impairments or dementia. Despite declaring English as their native language, `r results$multiple_experiments$demographic_checks$english_proficiency_counts %>% filter(experiment == 3) %>% filter(language_proficiency_rating != 5) %>% pull(n) %>% sum()` participants rated their English proficiency as below 5. `r results$multiple_experiments$demographic_checks$known_language_category %>% filter(experiment == 3) %>% filter(speaker_category == "monolingual") %>% pull(n) %>% english() %>% stringr::str_to_sentence()` participants reported knowing only English while `r results$multiple_experiments$demographic_checks$known_language_category %>% filter(experiment == 3) %>% filter(speaker_category == "multilingual") %>% pull(n)` participants also knew Spanish (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 3, language_spoken == "French") %>% pull(n) %>% sum()` times), Spanish (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 3, language_spoken == "Spanish") %>% pull(n) %>% sum()` times), German (listed `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 3, language_spoken == "German") %>% pull(n) %>% sum()` times), and `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 3, language_spoken %nin% c("French", "Spanish", "German", "English", "Norwegin", "Rusian")) %>% summarise(n = length(unique(language_spoken))) %>% pull(n) %>% english()` other languages (listed a total of `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 3, language_spoken %nin% c("French", "Spanish", "German", "English")) %>% pull(n) %>% sum()` times). Only `r results$multiple_experiments$demographic_checks$all_language_counts %>% filter(experiment == 3, language_spoken %in% c("Chinese", "Chinese (Mandarin)", "Japanese")) %>% pull(n) %>% sum() %>% english()` participants were familiar with logographic scripts. An additional `r results$multiple_experiments$exclusions %>% filter(id == "ex_3") %>% summarise(n = length(unique(participant_number))) %>% pull(n) %>% english()` participants were tested and excluded based on the criteria described for Experiment 1.

### Materials

We used the same materials as in Experiments 1, 2a, and 2b, and the same inconsistent orthography as in Experiments 1 and 2b.

### Procedure

The procedure deviated from Experiment 2a and 2b in that the training phase was doubled in length by adding another three 10-word reading and spelling blocks (with order of tasks counterbalanced across participants) resulting in a total of six training blocks for reading and spelling. All words were first partitioned into sets of 10 for presentation in the first three reading and spelling blocks and then repartitioned for presentation in the final three reading and spelling blocks, ensuring that each block contained five contrastive and five noncontrastive words. To provide ecologically valid conditions, semantic information was presented by depicting a concrete object with all words during exposure and reading training. The mean completion time was `r results$experiment_3$demographics$demographic_summary %>% pull(time_mean) %>% as.numeric("minutes")` min (*SD* = `r results$experiment_3$demographics$demographic_summary %>% pull(time_sd) %>% as.numeric("minutes")`).

## Results

### Coding

We used the same coding scheme for reading responses as in the previous experiments. The ICC between coders was `r summarise_icc(results$experiment_3$irr_results)`. The 95% confidence interval around the parameter estimate indicates that the ICC falls above the bound of .90, which suggests excellent reliability across coders [@Koo2016]. Spelling responses were analyzed by computing length-normalized Levenshtein Edit Distances between response and target sequences of graphemes.

### Model Fitting

We used a similar model structure to Experiments 2a and b, with the exclusion of the picture condition factor. As in Experiments 1, 2a, and 2b, the fixed effects for training and testing were modeled by obtaining all main effects and interactions between all factors excluding word type, and nesting word type within each combination of factor levels of the task and variety conditions. Because this experiment, like Experiment 1, contained six blocks per task, we included the quadratic term for block in the analyses of the training data to improve model fit. For the training data, the maximal converging random effects structure comprised zero-correlation random intercepts and slopes of task, variety condition, and their interaction by items, and random intercepts and slopes for the linear and quadratic time terms, task, word type, and their interaction by participants, including all correlations between these terms. For the testing data, the random effects structure comprised random intercepts and slopes of task, variety condition, and their interaction by items, and random intercepts and slopes for task, word type, and their interaction by participants, including all correlations between these terms for both by-participants and by-items random effects.

As with Experiments 1, 2a, and 2b, we also modeled the data using Bayesian mixed effects models with a full maximal random effects structure (i.e., without suppressing correlations between the by-items random effects in the training phase). These models used the same priors as in Experiments 2a and 2b, with the inclusion of a regularizing, very weakly informative prior, $Normal(0, 10)$, on the orthogonal quadratic time term, and excluding priors for picture condition which was no longer in the model. We used these models to evaluate evidence in support of the null hypothesis for each parameter in the same way as in Experiments 1, 2a, and 2b.

#### Training

Parameter estimates, confidence intervals (for the frequentist analysis), and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex3-train). As in all previous experiments, we T9 found a main effect of block attesting performance improvement over the course of training. Similar to Experiment 1, the quadratic term also reached significance confirming nonlinearity of the learning trajectory. We also confirmed the main effect of task which indicates that reading performance exceeded spelling performance. The interaction between block and task suggests that while performance was similar across tasks at the outset, learning progressed more rapidly for reading than for spelling.

```{r ex3-train, results = "asis", warning = FALSE}
ex3_train_nhst <- results$experiment_3$main_models$lme_train %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex3_train_bayes <- 
  results$experiment_3$planned_comparisons_Bayes$brm_train$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex3_train <- bind_cols(ex3_train_nhst, ex3_train_bayes) %>% 
  rename_nested_terms()

ex3_train_caption <- "Parameter Estimates for the Models Fitted to nLEDs From the Training Phase in Experiment 3"

kable_coefs(
  ex3_train, 
  caption = ex3_train_caption,
  colnames = shared_colnames, 
  general_footnote = general_foot, 
  headers_above_cols = header_above,
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 5,
  output_format = "latex", 
  align = table_alignment,
  hold_position = "HOLD_position"
  )
```

```{r ex3-train-reading-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for reading of contrastive and noncontrastive words during three training blocks in the variety match and variety mismatch conditions in Experiment 3. Error bars indicate $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "!htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-3_training_plot_reading.png"
))
```

```{r ex3-train-spelling-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for spelling of contrastive and noncontrastive words during three training blocks in the variety match and variety mismatch conditions in Experiment 3. Error bars indicate $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "!htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-3_training_plot_spelling.png"
))
```

With respect to the main questions of interest--the contrastive deficit and the effect of variety mismatch--we found evidence for a contrastive deficit for reading evidenced by the effect of word type in the variety mismatch condition. In addition, we observed an interaction between the quadratic term of block and variety condition and a three-way interaction between block, task, and variety condition, which suggest that performance levelled off somewhat faster in the variety match condition, especially for spelling, while further learning gains were made in the variety condition (see Figures\ \@ref(fig:ex3-train-reading-plots) and \@ref(fig:ex3-train-spelling-plots)). 

#### Testing

Parameter estimates, confidence intervals (for the frequentist analysis), and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex3-test). As in the training data, there was
a main effect of task confirming superior performance for reading compared with spelling and an effect of word type, indicative of the contrastive deficit for reading in the variety mismatch condition. We also found that the effect of word familiarity was significant for reading in the variety match condition due to impaired performance for untrained compared to trained words in this condition. Crucially, the analysis yielded a main effect of variety which showed that overall performance at test was superior in the variety mismatch condition (see Figures\ \@ref(fig:ex3-test-reading-plots) and \@ref(fig:ex3-test-spelling-plots)).

```{r ex3-test, results = "asis", warning = FALSE}
ex3_test_nhst <- results$experiment_3$main_models$lme_test %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex3_test_bayes <- 
  results$experiment_3$planned_comparisons_Bayes$brm_test$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex3_test <- bind_cols(ex3_test_nhst, ex3_test_bayes) %>% 
  rename_nested_terms()

ex3_test_caption <- "Parameter Estimates for the Models Fitted to nLEDs From the Testing Phase in Experiment 3"

kable_coefs(
  ex3_test, 
  caption = ex3_test_caption,
  colnames = shared_colnames, 
  general_footnote = general_foot, 
  headers_above_cols = header_above,
  format_args = list(decimal.mark = ".", big.mark = ","), 
  digits = 2,
  font_size = 5,
  output_format = "latex", 
  align = table_alignment,
  hold_position = "HOLD_position"
  )
```

```{r ex3-test-reading-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for testing reading performance for trained noncontrastive, trained contrastive, and untrained words in the variety match and variety mismatch conditions in Experiment 3. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "!htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-3_testing_plot_reading.png"
))
```

```{r ex3-test-spelling-plots, fig.cap = "Length-normalized Levenshtein Edit Distance (nLEDs) for testing spelling performance for trained noncontrastive, trained contrastive, and untrained words in the variety match and variety mismatch conditions in Experiment 3. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", out.width = "63%", fig.env = "figure*", fig.align = "center", fig.pos = "!htbp", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-3_testing_plot_spelling.png"
))
```

```{r ex3-test-novel, include = FALSE, warning = FALSE}
ex3_novel_nhst <- results$experiment_3$planned_comparisons_NHST$test_l_n %>% 
  summarise_lme(., term, main_terms) %>%
  filter(term == "Variety Condition")

ex3_novel_bayes <- 
  results$experiment_3$planned_comparisons_Bayes$brm_test_novel$fixed %>% 
  summarise_bayes(., term, main_terms) %>%
  filter(term == "Variety Condition")
```

As in the previous experiments, we performed a planned comparison of performance between the variety match and variety mismatch conditions on untrained words only using the same model structure as in Experiments 2a and 2b. The frequentist model yielded a significant effect of variety condition ($\hat{\beta}$ = `r ex3_novel_nhst$estimate`, 95% CI = `r ex3_novel_nhst$"95% CI"`, *t* = `r ex3_novel_nhst$statistic`, *p* = `r ex3_novel_nhst$p.value`) with the Bayesian estimate suggesting sufficient evidence in favour of this effect ($\hat{\beta}$ = `r ex3_novel_bayes$Estimate`, 95% CI = `r ex3_novel_bayes$"95% CI"`). This effect indicates that reading and spelling performance were superior in the variety mismatch condition.

## Discussion

When a larger sample of participants was trained for longer in reading and spelling of an inconsistent artificial orthography with semantic information there was clear evidence for a contrastive deficit in reading, both in training as well as in testing. This indicates that when training is long enough for phonological representations to be established, exposure of competing variants that are associated with the same meaning impairs reading. A contrastive deficit could not have arisen had participants exclusively relied on a phonologically mediated reading strategy that involved serial conversion of all graphemes into the associated phonemes. In contrast, no contrastive deficit emerged for spelling because no dialect orthographic form had ever been presented, and because spelling could only be achieved through sequential conversion of individual phonemes into the associated graphemes. 

At the same time, the word familiarity effect observed for reading in the variety match condition suggests that when no competing variants were encountered during literacy training participants seemed to rely more on direct access to the phonological forms of words, likely mediated by a word’s meaning. In contrast, no phonological representations were available for untrained words, making serial conversion of graphemes into phonemes necessary--a process that is presumably more error-prone than direct lexical access. The lack of an effect of word familiarity for reading in the variety mismatch condition suggests that having encountered many competing variants in the input discouraged a lexical strategy but rather encouraged grapheme-phoneme conversion, which was equally successful for trained and untrained words. As a result of more systematic use of phonological decoding, participants in the variety mismatch condition exhibited an overall benefit in their literacy skills, especially for untrained words.

\clearpage

# General Discussion

In three experiments we investigated the effect of exposure to dialect variants of words on literacy learning. Employing an artificial language with an invented script allowed us to control for potential extralinguistic confounds that are often associated with dialect exposure such as differences in quality of input, home literacy environment, cultural attitudes to literacy, educational provision, or teacher expectation. Previous research [@Brown2015] had shown that encountering a variety mismatch impairs processing of contrastive words, that is, words with different variants across varieties (e.g., Scots "hoose" vs. English "house" or AAE "aks" vs. MAE "ask"). What remained unclear was whether impaired performance with these contrastive words is also associated with a general deficit in decoding skills as measured by reading and spelling of novel, untrained words.

Our results confirmed and extended the finding of a contrastive deficit, which we replicated for reading training without semantic information in the variety mismatch conditions in Experiments 1 and 2a, where participants might not have noticed that similar, yet distinct variants were associated with the same lexical item. These conditions corresponded to the @Brown2015 connectionist simulation where a contrastive deficit is akin to the processing of heterophonic homographs—words that are spelled the same but activate phonological competitors that are pronounced differently. However, when a consistent orthography (Experiment 2a) or longer training (Experiment 3) improved conditions for the establishment of phonological representations, the contrastive deficit appeared also when pictures enabled participants to access semantic representations, suggesting that a shared semantic representation further promotes competition between phonological variants, provided these are sufficiently stable. This finding is in line with interactive activation and competition models postulating inhibition from high-frequency competitors at the lexical layer, which can be reinforced via bidirectional connections between lexical and semantic representations [@Chen2012], and suggests that both phonological and lexical competition contribute to a contrastive deficit in situations of dialect exposure.

```{r error-types, include = FALSE, warning = FALSE}
correct_error_types <- results$multiple_experiments$error_types %>%
  bind_rows(., .id = "experiment") %>% 
  filter(
    task == "Reading",
    dialect_words == "Contrastive",
    lenient_coder_error_types %in% c("Correct", "Dialect Word Match"),
    language_variety %in% c("Variety Match", "Variety Mismatch")
  ) %>% group_by(language_variety, lenient_coder_error_types) %>%   
  summarise(min = min(mean_prop)*100, max = max(mean_prop)*100) %>% 
  mutate_if(is.numeric, signif, 2) # 2 significant figures
```

One question that has not been addressed so far is whether the competition associated with the contrastive deficit in reading manifests itself in confusion between the two variants of a word or in increased nonspecific errors when processing graphemic input. Our dependent variable, the Levenshtein Edit Distance, which provides the best comparison to the cross-entropy error computed for the neural network simulations of @Brown2015, is not informative with respect to specific error types. In order to gain further insight into errors, we used automatic string comparison to code productions in the testing phase for all experiments with respect to whether dialect variants were produced in response to their standard contrastive counterpart (e.g., target: *kuble*--response: *xuble*), whether dialect variants were produced in response to another standard contrastive word (e.g., target: *skefi*--response: *xuble*), whether standard words were produced in response to another standard word (e.g., target: *skefi*--response: *kuble*), or whether any other nonsubstitution error was made (for summary graphs see Appendix\ \@ref(appendix-e)). Inspection of these response patterns shows a clear trend across experiments: While the mean percentage of correct responses to contrastive words did not differ between variety match 

(ranging from `r correct_error_types %>% filter(language_variety == "Variety Match", lenient_coder_error_types == "Correct") %>% pull(min)`% to `r correct_error_types %>% filter(language_variety == "Variety Match", lenient_coder_error_types == "Correct") %>% pull(max)`%) and variety mismatch (ranging from `r correct_error_types %>% filter(language_variety == "Variety Mismatch", lenient_coder_error_types == "Correct") %>% pull(min)`% to `r correct_error_types %>% filter(language_variety == "Variety Mismatch", lenient_coder_error_types == "Correct") %>% pull(min)`%) conditions, roughly three times more dialect variants were substituted for a standard contrastive counterpart (e.g., *kuble*-*xuble*) in variety mismatch (ranging from `r correct_error_types %>% filter(language_variety == "Variety Mismatch", lenient_coder_error_types == "Dialect Word Match") %>% pull(min)`% to `r correct_error_types %>% filter(language_variety == "Variety Mismatch", lenient_coder_error_types == "Dialect Word Match") %>% pull(max)`%) than variety match (ranging from `r correct_error_types %>% filter(language_variety == "Variety Match", lenient_coder_error_types == "Dialect Word Match") %>% pull(min)`% to `r correct_error_types %>% filter(language_variety == "Variety Match", lenient_coder_error_types == "Dialect Word Match") %>% pull(max)`%) conditions[^8]. This trend suggests that the contrastive deficit, albeit small, is predominantly due to variant substitution rather than impaired overall reading skills.

[^8]: Dialect errors in the variety match condition, where no dialect variants were never encountered, simply reflect the frequency with which these variants may occur if learners substitute or omit phonemes.

We had hypothesized that introduction of spelling training should attenuate the contrastive deficit by facilitating phonologically mediated decoding [@Taylor2017]. Indeed, the fact that no contrastive deficit was found for spelling confirms that spelling itself did not rely on direct retrieval of orthographic forms but required conversion of phonemes into graphemes. In fact, variant substitution in response to contrastive words (e.g., *kuble*--*xuble*) did not occur at all for spelling even though spelling training did not prevent the emergence of such substitutions in reading, as described above. Moreover, as the joint analyses of Experiments 1 and 2 indicated, introducing spelling training did not lead to a significant improvement in overall literacy nor in phonologically mediated decoding, in contrast to studies demonstrating that invented, that is, non-normative spelling facilitates reading by boosting phonemic awareness and by promoting a more analytical stance toward letter–sound correspondences [@caravolas2001foundations; @Ehri2006; @Ouellette2008a; @Ouellette2008b; @Ouellette2017]. It is likely that adult learners, who already have mastered the alphabetic principle, do not experience an additional boost from spelling training as they prefer direct access to word forms during reading whenever possible--either after partial decoding of initial graphemes or via the depicted word meaning or both. If conversion of individual phonemes into graphemes and vice versa is perceived as effortful and error-prone, adult participants may follow this route only when there is no alternative, as in spelling, for which performance was indeed consistently inferior to reading.

In the picture conditions, we had included pictures not just during exposure but also during reading as a means of providing some semantic information to compensate for lack of sentential context or of accompanying pictures that often are found in children’s books. It could be argued that whenever pictures were present alongside a word’s orthographic form no decoding needed to take place at all as direct access of the phonological form via rote-memorization of meaning–sound associations was possible. Under such conditions, dialect exposure should have no detrimental effects on the ability to decode novel words simply because no decoding skills would have been acquired, and reading of untrained words--the artificial-language analogy to nonword reading tests--should have presented considerable difficulty. Indeed, for the inconsistent orthography the familiarity effect was statistically significant when pictures were present, suggesting that a combination of a difficult-to-learn orthography with the availability of semantic information may have reduced the pressure to decode individual graphemes. However, it is unlikely that picture presentation during reading would have precluded the acquisition of decoding skills entirely because the number of times participants encountered each word and its meaning (five times in Experiments 1 and 2, 10 times in Experiment 3) was probably insufficient to enable participants to reliably memorize sound–meaning associations for the entire set of 30 items, leaving them with having to decode, at least partially, those words they could not remember based on meaning. Reliance on partial decoding may then have been moderated by our experimental conditions: When pictures were provided, direct access from meaning to the sound form was possible, attenuating use of the decoding strategy. On the other hand, whenever a consistent orthography made decoding easier, as in Experiment 2a, word form access via meaning may have been discouraged, and partial decoding may have been encouraged so that a familiarity benefit appeared regardless of picture condition. Emergence of a word familiarity effect in some of the spelling conditions shows that a consistent orthography can facilitate access to orthographic representations, perhaps via implicit statistical learning of grapheme sequences, spatial locations of letters on the on-screen keyboard, or associated motor routines that underpinned the keyboard-based spelling.

The crucial question of the present study was whether exposure to different variants of some of the training words in the variety mismatch conditions impaired decoding skills. In Experiments 1 and 2, the analyses did not provide sufficient evidence to answer this question. For the inconsistent orthography, this may simply have been a consequence of the overall difficulty of the task. But even for the consistent orthography, where learning was more successful, there was no evidence for a detrimental effect of variety mismatch. Moreover, when we increased our sample size to gain greater statistical power and extended the training phase (Experiment 3), we found a clear performance benefit in the variety mismatch condition. This benefit was significant for overall performance as well as for the untrained words separately, and provides clear evidence that under conditions mimicking dialect exposure participants acquired superior decoding skills compared with conditions without dialect variation.

What might account for such a dialect benefit in artificial literacy acquisition? When discussing differential performance in reading and spelling we suggested that learners appear to select strategies based on perceived difficulty: We argue that greater linguistic variety may limit reliance on memory-based retrieval of phonological forms during reading and facilitate rule-based, phonologically mediated decoding, which, in turn, can lead to an overall improvement in decoding skills. This conclusion is also confirmed by the word familiarity effect in Experiment 3, which reached significance only in the variety match condition, suggesting that in the mismatch condition, dialect exposure may have encouraged more reliance on phonological decoding to resolve the conflict between contrastive variants. Thus, counter to expectations formulated in the literature so far, our results suggest that when extralinguistic confounds are controlled dialect exposure may in some situations even be beneficial for acquisition of phonological decoding skills.

Our experiments do not allow us to determine whether the observed dialect benefit requires explicit noticing of the competing variants for contrastive words or whether phonological decoding benefits simply from greater variability of word forms in the input. The idea that explicit noticing of dialect variants could benefit decoding skills is in agreement with the linguistic awareness/flexibility hypothesis of @terry2011phonological. While awareness of appropriate dialect usage can be seen as an indicator for general metalinguistic knowledge, which is known to be beneficial for acquisition of phonological decoding skills, there is also evidence that directly boosting learners’ dialect awareness can help literacy learning. For example, @johnson2017effects demonstrated that an intervention that involved explicit teaching of dialect awareness to primary school-children exposed to both NMAE and MAE resulted not only in more flexible and appropriate use of NMAE but also in better MAE literacy skills. Future research will have to investigate to what extent explicit awareness of contrastive words is required for a dialect benefit to occur during literacy learning.

Our finding of a dialect benefit in the artificial literacy paradigm comes with several caveats: First, learners in this study were adults who already had acquired literacy in one or more languages and were certainly familiar with the alphabetic principle. Their prior literacy competence may have endowed them with knowledge--implicit or explicit--of a variety of routes to access phonological and orthographic forms, and the ability to select strategically between them depending on input. Such choices may not be available to children who are just starting on the path to literacy using whatever principles are emphasized in their specific educational setting. Thus, caution is indicated when trying to generalize our findings to children until future research has examined whether dialect exposure has similar benefits in learners who are just beginning to acquire the different pathways to reading and spelling.

Second, the artificial conditions of our study differ from naturalistic literacy acquisition in several fundamental ways. For one, the goal of learning in the conditions in which no pictures were present was different from the typical goal of reading and spelling, which is to access and to convey meaning. Here, all that participants were asked to learn was the connection between print and sound, a limitation that was motivated by our attempt to replicate the findings from the @Brown2015 connectionist simulations. Still, it may have shifted the emphasis on access to phonological and orthographic representations more than is appropriate in naturalistic literacy learning thereby affecting the learners’ repertoire of mechanisms and strategies. We had tried to remedy this limitation by comparing these conditions with conditions in which pictorial information about the meaning was available at all times. However, unlike children, who typically know the meanings of the words they try to read and spell, in these conditions our participants learned the meaning of novel words at the same time as they learned to read and spell. This is more akin to acquisition of a second language in settings where learning is underpinned by print exposure, for example, when adult speakers of English learn Hebrew both from a teacher and a textbook--a more complex and potentially more effortful learning task than literacy learning in the native language. We had tried to mitigate against this additional burden by providing pictorial information about the meaning at all times, but it is still possible that this more difficult task may have proved taxing on attentional resources and thereby altered learning strategies. To be able to generalize from learning of artificial scripts to literacy acquisition in children future research may seek to study more ecologically valid conditions, for example the learning of novel scripts for familiar words or pretraining of word knowledge before literacy acquisition commences.

Third, our experiments provided no cues, social or otherwise, for dialect use. All that participants encountered in the variety mismatch conditions was greater variability in terms of variants, whether associated with the same meaning or not. Yet dialect use is typically associated with specific regional, social, and situational constraints. @Brown2015, in their second simulation, showed that when dialect variants were cued by context nodes that coded variety (AAE vs. MAE) the contrastive deficit was attenuated. This shows that additional differentiating contextual information, provided consistently alongside phonologically similar contrastive variants, reduces competition. Despite the lack of social context, the present artificial language learning experiments are still of relevance as some evidence suggests that, unlike in bilingual language acquisition, the sociolinguistic competence required to contextualize dialect variation takes considerable time to build, as indicated by the slow developmental trajectory for dialect recognition [@McCullough2019] and emergene of social attitutes toward dialects [@Kinzler2013]. One could construe the situation simulated in our experiments as one in which literacy acquisition precedes reliable acquisition of the sociolinguistic competence that governs dialect use. In future studies, we plan to provide contextual information alongside the different variants, which might reduce the difficulty with processing contrastive words. The intriguing question is in what ways such contextual information will affect the reliance on rote-memorization versus phonological decoding.

Finally, it should be noted that we observed considerable variability in performance in all experiments. A visual inspection of the figures indicates that the distributions of edit distances were bimodal in many conditions. Even though the lack of a normal distribution of this dependent variable does not preclude fitting the statistical models described above, as the residuals were normally distributed in all instances, it still points to the possibility qualitatively different mechanisms were employed by subgroups of our participants. This variability may in part reflect greater demographic diversity on crowdsourcing platforms compared with laboratory samples. We had refrained from selecting participants according to prespecified demographic variables like SES because proxies for such variables (e.g., annual income) may have different validity in different cultural and economic contexts, and because of evidence that on crowdsourcing platforms responses to eligibility questions may not be reliable and consistent [@chandler2017lie]. (Note in this context the curious discrepancy in some participants who were asked to self-select as native English speakers in Experiments 1 and 3 but rated their English proficiency as below-native or even elementary.) Variability in performance may also reflect different solutions to the trade-off between minimizing expended effort while maximizing monetary gain, which may depend on whether participants use crowdsourcing platforms repeatedly as a source of income [@ElMaarry2018]. In particular, the substantial duration of our experiments, in conjunction with the monetary reward, may have induced effort-minimizing strategies beyond what would be expected in more naturalistic literacy acquisition contexts and in potentially better supervised laboratory studies. Although we tried to mitigate against outright cheating (e.g., note taking) by placing time constraints on different tasks, we still have to accept that some participants may have expended too little effort for learning to occur. These shortcomings should at least in part be compensated for by our substantial sample sizes that exceed those typically used in laboratory experiments.

# Conclusions

In naturalistic contexts, it is difficult to disentangle dialect exposure from other confounding factors that may affect literacy learning. The results from this artificial literacy learning study showed that while words with dialect variants are more difficult to read, their presence in the input can facilitate acquisition of phonological decoding skills as a means of reducing the arising competition. Because a phonologically mediated route to literacy acquisition has been shown to be essential in the early stages of learning to read and spell [@Castles2018; @Taylor2017] our results--if confirmed in further studies with children--raise the intriguing possibility that dialect exposure may, in fact, yield tangible benefits for literacy acquisition.

# Context of the Research

This project has brought together two strands of experimental research that we have pursued in the past: the study of how cognitive representations of dialects in bidialectal speakers differ from representations of languages in bilinguals, and the study of how distributional features of the language input affect language learning. Inspired by the applied question of whether dialect bans in schools are justified from the point of view of the underlying learning mechanisms, we extended the artificial language learning paradigm to the investigation of how input variability induced by dialect exposure might affect literacy acquisition. A major challenge was to scale up artificial language learning to larger numbers of participants via the use of crowd-sourcing platforms. To our knowledge, this is the first study to analyze large scale artificial language production data obtained from online participants. The strict controls afforded by artificial language and artificial script learning enabled us to replicate with human learners what neural network simulations had demonstrated before for natural language: that there is a small cost for processing words for which dialect variants exist. Our finding that this local cost does not necessarily impair acquisition of general decoding skills, at least in adult learners, will hopefully be of interest to researchers working on artificial language learning, on models of bidialectal lexical representation, and on literacy acquisition as well as to educational practitioners. In the future, we will aim to extend this controlled approach to the study of how dialect exposure affects literacy acquisition in children.

# References

```{r create-r-references, include = FALSE}
# make a new .bib file for citing r-packages
# Note: We manually changed the "irr" package since it incorrectly includes an
#       email as an author name when created programatically.
# r_refs(file = here("05_paper", "01_manuscript", "r-references.bib"))
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
\onecolumn

