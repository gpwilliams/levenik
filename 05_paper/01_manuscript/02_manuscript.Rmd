---
title             : "How Does Dialect Exposure Affect Learning to Read and Spell? An Artificial Orthography Study"
shorttitle        : "Dialect Literacy"

author: 
  - name          : "Glenn P. Williams"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Division of Psychology. Abertay University. Dundee. DD1 1HG. Scotland, UK."
    email         : "g.williams@abertay.ac.uk"
  - name          : "Nikolay Panayotov"
    affiliation   : "1"
  - name          : "Vera Kempe"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Abertay University"

authornote: |
  Division of Psychology, School of Social and Health Sciences, Abertay University, Dundee, UK.
  
  Acknowledgements: The authors gratefully acknowledge funding from The Leverhulme Trust (Grant #RPG_2016-039) for this research. We also would like to thank Richard Morey & E. J.  Wagenmakers for helpful statistical advice but remain responsible for any potential flaws in the statistical analyses.

abstract: |
  Correlational studies have demonstrated detrimental effects of exposure to a mismatch between a non-standard dialect at home and a mainstream variety at school on children's literacy skills. However, dialect exposure often may be confounded with reduced home literacy, negative teacher expectation and more limited educational opportunities. To examine whether there is a causal relationship between variety mismatch and literacy skills, we taught adult learners to read and spell an artificial language using an artificial orthography. In three experiments, we confirm earlier findings that reading is more error-prone for contrastive words, i.e. words for which different variants were present in the input, especially when learners also acquire joint meanings of the competing variants. Despite this contrastive deficit, no detriment from variety mismatch emerged for reading and spelling of untrained words, a task equivalent to non-word reading tests routinely administered to young school children. When training was extended, we even found a benefit from variety mismatch on reading and spelling of untrained words. We suggest that a dialect benefit in literacy learning can arise when competition between different variants leads learners to direct their attention towards phonologically mediated decoding strategies. Our findings should encourage educators to harness the benefits that can arise from linguistic diversity.
  
keywords          : "literacy, dialect, artificial language learning"
wordcount         : "XXXX"

bibliography      : ["r-references.bib", "main-references.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
linkcolor         : "blue"
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "doc" # can be "man", "doc", or "jou"
output            : 
  papaja::apa6_pdf: # apa6_pdf or docx
    keep_tex: TRUE
appendix          : 
  - "03_appendix_a.Rmd"
  - "03_appendix_b.Rmd"
  - "03_appendix_c.Rmd"
  - "03_appendix_d.Rmd"
nocite            : | 
  @GIMP1995, @Forsythe2017
header-includes   : 
  - \usepackage{tipa}
---

```{r notes, include = FALSE}
# nocite used for keeping a reference that isn't cited in text.
# this is important as the appendix can't have citation codes
# otherwise additional references cited in the appendix only come after the appendix

# if it doesn't render properly, try changing the engine to:
#output:
#  papaja::apa6_pdf:
#    latex_engine: xelatex

# tipa package used for IPA: see https://www.tug.org/tugboat/tb17-2/tb51rei.pdf
# and https://github.com/crsh/papaja/issues/209 for notes on how to use this.

# longtable breaks 2 column pages ("jou"): https://github.com/jgm/pandoc/issues/1023
# this provides a fix for "jou" class:
# https://tex.stackexchange.com/questions/161431/how-to-solve-longtable-is-not-in-1-column-mode-error
# either don't use longtable (not needed for jou), or force to single col
# for tables

# Floats ----
# options for fig.pos = "!ht" etc.
# kable latex_options: 
#   https://www.rdocumentation.org/packages/kableExtra/versions/1.0.1/topics/kable_stylingcan be #   "hold_position" and "HOLD_position" force printing in place for tables

# umlauts and pounds need special characters;
# &uml; = umlaut u
# \pounds = Â£

# fig positions use: htb (in place, top, or bottom of page)
# see: https://www.overleaf.com/learn/latex/Errors/%60!h'%20float%20specifier%20changed%20to%20%60!ht'

# function using various general functions for tidying output in tables
# these are hardcoded so need improved before adding to general functions
```

```{r packages-and-functions, include = FALSE, message = FALSE, warning = FALSE}
# load packages
packages <- c(
  "here",
  "papaja",
  "tidyverse",
  "lme4",
  "lmerTest",
  "emmeans",
  "irr",
  "BayesFactor",
  "brms",
  "broom.mixed",
  "knitr",
  "kableExtra",
  "english" # for parsing numbers as English words
)

lapply(packages, library, character.only = TRUE)

# load user defined functions
source(here("05_paper", "00_functions", "paper_functions.R"))
source(here("05_paper", "00_functions", "model_names.R"))
```

```{r load-data, include = FALSE, message = FALSE, warning = FALSE}
# load experiment data
source(here("05_paper", "01_manuscript", "00_load-data.R"))

# load names for tables
source(here("05_paper", "01_manuscript", "01_names.R"))

# load gruffalo corpus data from GitHub
gruffalo_link <- 
  "https://raw.githubusercontent.com/gpwilliams/gruffalo_index/master/data/"

gruffalo_counts <- read_csv(paste0(
  gruffalo_link, "03_summary-data/counts.csv"
  ))
gruffalo_freq <- read_csv(paste0(
  gruffalo_link, "03_summary-data/frequencies.csv"
  ))
gruffalo_token <- read_csv(paste0(
  gruffalo_link, "02_processed-data/token_freq.csv"
))

# need to load data on exclusions for each experiment if possible
```

```{r global-options, include = FALSE, warning = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r formatting-options, include = FALSE, warning = FALSE}
table_font_size <- 8 # 8pt

table_format_args <- list(
  decimal.mark = ".", 
  big.mark = ","
)

# define whether tables can span multiple pages
# must be TRUE if using "man", as tables span multiple pages
multipage_table <- FALSE
```

# Introduction

In 2013, the BBC reported that a Head Teacher in England had banned use of the local dialect in his Primary School [@BBC2013]. This decision appears to have been motivated by the notion that dialect exposure creates confusion when beginning readers encounter different variants associated with the same meaning and have to resolve the competition between them. However, direct empirical support for the idea that such competition, if it arises, slows the acquisition of literacy skills is lacking. The aim of the present study is to put this notion to a rigorously controlled test.

Most research on how exposure to different varieties affects literacy acquisition has been conducted on minority dialects of English spoken in the United States, chiefly African-American English (AAE), a variety that tends to diverge from Mainstream American English (MAE) as a function of, among other things, social and cultural segregation [@Labov1995]. However, linguistic diversity is a ubiquitous feature of most languages. While exposure to a minority dialect has long been implicated as one of the risk factors for reading difficulties, early research in this domain failed to find a consistent association between exposure to non-mainstream American English (NMAE) and literacy skills [@Harber1977; @Steffensen1982]. These inconclusive results were attributed to methodological flaws in the measurement of dialect exposure and literacy outcomes in children. Moreover, correlational findings of this kind are inevitably confounded with variables other than the structure of language input such as lower socio-economic status (SES) and home literacy, and differences in attitudes, educational provision and teacher expectation, yet the relationship between these various non-linguistic variables and literacy outcomes is far from clear [@Artiles2010]. However, the persistent literacy achievement gap in US minority children prompted renewed interest in literacy skills of children exposed to a mismatch between different varieties, resulting in methodological improvements of the more recent studies. A meta-analysis which systematically reviewed this second wave of research concluded that there was a moderate negative relationship between exposure to, and use of, NMAE and literacy outcomes in the absence of significant effects of SES [@Gatlin2015]. This conclusion invites further attempts to explain a detrimental effect of variety mismatch in terms of the underlying processing mechanisms.

According to the Linguistic Mismatch Hypothesis [@Labov1995], dialect exposure increases the mismatch between orthographic and phonological forms thus rendering the discovery of phonologically mediated decoding principles more challenging. Specifically, dialect variants that deviate strongly from standard words as they essentially constitute competing lexemes (e.g. Scots "bairn" vs. Standard English "child") require the learner to acquire such new lexemes in parallel to learning to read and spell. Thus, when learners attempt to establish direct links between orthography and phonology as postulated in most computational reading models such as the DRC [@Coltheart2001], the CDP+ [@Perry2007; @Perry2010] or the triangle model [@Harm2004; @Plaut1996], orthographic representations of words for which a dialect variant exists (henceforth: contrastive words), might lead to interference between the competing variants, which should incur additional processing cost to the system. On the other hand, dialect variants characterised by mainly phonological changes that deviate only slightly from words in the standard language (e.g. Scots "hoose" vs. Standard English "house" or African-American English [AAE] "aks" vs. MAE "ask"), while not introducing entirely different lexemes, add inconsistency to the mapping from print to sound. The resulting increased orthographic depth is likely to make the acquisition of decoding skills via application of phoneme-grapheme conversion rules more difficult, particularly for phonologically opaque orthographies, such as English, that are already difficult to decode (but see @Rastle2019 for an argument in favour of benefits from morphological transparency of English spelling).

The hypothesis that contrastive words elicit reading difficulties was tested by Brown and colleagues [@Brown2015] with 8-13-year-old children exposed to AAE who were asked to read contrastive and non-contrastive words matched for frequency, length and initial phonemes. Contrastive words typically had dialect variants that displayed reduction of consonant clusters. The results showed that the higher these childrens' usage of AAE, assessed through number of AAE features in a sentence repetition task, the longer their reading latencies for contrastive words. This contrastive deficit was computationally simulated in a neural network exposed to repeated mapping of phonological to phonological representations (i.e. a task mimicking learning to speak) before being trained to map orthographic representations onto phonological representations (i.e. a task mimicking learning to read) while still receiving interleaved blocks of phonological exposure to prevent "catastrophic interference" due to simply switching from one type of input to another. Crucially, when the network was initially exposed to AAE variants for half of the words comprising consonant cluster reductions, consonant drops, substitutions and exchanges as well as devoicing, and then subsequently was trained to read MAE words (the mismatch condition), cross-entropy error remained higher for contrastive compared to non-contrastive words. Brown and colleagues interpreted this finding in analogy to the reading of heterophonic homographs - identical orthographic representations of semantically unrelated words that are pronounced differently like "lead" or "wind", which in the absence of contextual information are more difficult to read compared to non-homographic control words [@Gottlob1999; @Jared2012]. The contrastive deficit was greatly diminished in a second simulation that instantiated nodes coding for whether words belonged to AAE vs. MAE, a feature designed to simulate social cues that activate one or the other variety.

While the @Brown2015 simulation undoubtedly provides important insights into potential mechanisms that might be responsible for the difficulty with reading contrastive words, some crucial components of word representation and literacy learning were absent from the model. As a result, it is not entirely clear whether the contrastive deficit in the neural network arises for the same reasons as it did in beginning readers, even if extralinguistic factors are controlled. Firstly, the network lacked a semantic layer precluding instantiation of semantic representations for individual words. Yet beginning readers tend to know the meanings for most, if not all, of the words used in early literacy training, and start out by employing phonologically mediated decoding to gradually establish direct associations between the new orthographic code and the existing semantic representations [@Castles2018]. There was no mechanism in the Brown et al. model  by which different variants could be associated with the same meaning. Instead, in the mismatch condition, the network learned more words overall as the MAE variants presented during literacy training added an additional set of words some of which were phonologically similar to the already acquired AAE words. As a result, dialect variants of contrastive words shared many phonemes with other words in the lexicon while non-contrastive word did not. This renders the contrastive deficit akin to inhibition from high-frequency phonological neighbours in mono-dialectal monolinguals.  Models of interactive activation and competition of word reading and naming suggest that adding a semantic layer should retain or even exacerbate the contrastive deficit as non-linear inhibitory connections are implemented on the lexical layer [@Chen2012] yet we have no empirical evidence so far whether this is indeed the case. Conceivably adding shared semantic representations might renders bidialectal processing of contrastive words more akin to the reading of cognates in bilingual readers for which facilitation effects have been reported [e.g. @VanAssche2009], which are modulated by target word frequency and language context. It is therefore also possible that a detrimental effect for reading contrastive words might be attenuated when different variants of a word are associated with the same meaning.

Secondly, neither human participants nor the connectionist model exhibited difficulties with reading non-contrastive words nor an overall reading deficit in the variety mismatch (AAE) condition. If potential detriments due to variety mismatch are mainly driven by processing difficulties with contrastive words then the overall degree of difficulty associated with dialect exposure would depend on the proportion of contrastive words in the input. There is so far no indication that dialect exposure affects literacy skill acquisition beyond difficulties with contrastive words. Yet the Linguistic Mismatch Hypothesis as formulated by @Labov1995 went beyond a detrimental effect confined to contrastive words by suggesting that dialect exposure impairs orthographic decoding skills more generally. To confirm this assertion one would have to assess beginning readers' decoding skills independently of their word knowledge by testing their nonword reading skills [@Castles2018]. Since neither the behavioural study nor the computational simulation in @Brown2015 tested nonword reading, it is difficult to arrive at a conclusion regarding the impact of variety mismatch on beginning readers' general literacy skills. 

The evidence discussed so far was obtained in studies investigating the process of learning to read English, a deep orthography with fairly opaque phoneme-grapheme conversion rules that still is often taught without sufficient emphasis on phonological mediation [@Castles2018]. For opaque orthographies, dialect exposure may be particularly detrimental as it can further hinder the acquisition of already difficult-to-discover decoding rules. By contrast, learning to decode mappings from sound to spelling is easier in transparent orthographies, and, consequently, dialect exposure may have less of a detrimental effect. It is even possible that rapidly acquired decoding skills in transparent orthographies can render the decoding of standard words unperturbed by the existence of competing dialect variants. To our knowledge, the only transparent orthography for which the role of dialect exposure in literacy learning has been investigated is German. B&uml;hler and colleagues [@Buhler2018] examined early literacy skills in children exposed to Swiss German dialect and compared them with children exposed only to Standard German either in Switzerland or in Germany. The results showed that dialect exposure was associated with higher preschool literacy-related skills measured by the ability to identify, categorise and synthesise onsets, rimes and individual phonemes. Structural equation modelling revealed that only when preschool literacy-related skills were controlled was there a negative effect of dialect exposure on Grade 1 literacy skills, which was more pronounced in spelling owing to German's less transparent  phoneme-grapheme mappings. This finding exposes the multiple loci of the effects early dialect exposure might have: On the one hand, a beneficial effect of early dialect exposure on literacy-related skills might arise from enhanced sensitivity for phonological variation thereby increasing metalinguistic skills that benefit phonological awareness. On the other hand, a residual negative effect of dialect exposure on subsequent literacy acquisition may reflect the consequences of decreased consistency in the sound-spelling mappings as well as the difficulty associated with first having to learn a number of new lexemes in order to master literacy in the standard language. This suggests that in transparent orthographies, potentially detrimental effects of dialect exposure may be offset by dialect exposure contributing to enhanced phonological awareness which, in turn, can aid phonologically mediated reading and spelling.

To gain further clarity on the effects of dialect exposure and the potential underlying mechanisms, our study asked whether there is a causal relationship between variety mismatch and difficulties with acquiring general literacy skills when confounding extra-linguistic factors that may impact literacy acquisition are controlled. To achieve this control, we employed an artificial language learning paradigm combined with an invented script, a methodology that has successfully been used to explore different factors that may affect learning to read in the early stages [e.g. @Taylor2011; @Taylor2017; for a review see @Vidal2017]. We attempted a conceptual replication of the contrastive deficit demonstrated in @Brown2015 to confirm whether this is indeed the main locus of deficits arising from dialect exposure. Crucially, we also asked whether variety mismatch affects general decoding skills as assessed via reading of untrained words.

## The Present Study

We report a set of experiments designed to investigate effects of dialect exposure on the acquisition of literacy skills in opaque and transparent orthographies. For the present study, we defined dialect exposure following @Brown2015 as exposure to variants that entail phonological, but not lexical, changes (e.g. dialect cognates like "house" vs. "hoose"). This decision was also motivated by the attempt to maximise ecological validity based on a corpus analysis (the Gruffalo-corpus described below) which revealed that in Scottish English dialects phonological variants are more common than lexical variants (e.g. "children" vs. "bairns"). As a result, effects of encountering lexical variants are beyond the scope of the current study. Below we briefly preview the rationale behind the three experiments.

We first report an experiment which attempted a conceptual replication of Simulation 1 in @Brown2015 by examining effects of dialect exposure on learning to read an opaque orthography[^1]. It compared a *variety match condition*, where participants encountered the same words during initial exposure and when learning to read, with a *variety mismatch condition* where half of the words underwent phonological changes between exposure and reading training. The latter condition was intended to loosely resemble a situation in which learners first are exposed to a dialect at home before being introduced to the standard variety at school. In line with the experimental findings and computational simulations in @Brown2015 described above, participants in the *variety mismatch condition* showed a tendency to perform worse with contrastive than non-contrastive words but no evidence for impaired performance with untrained words compared to participants in the *variety match condition*. While this initial finding suggests that worries about detrimental effects of dialect exposure on general decoding skills may be unwarranted, the experimental design of the first experiment did not account for the fact that when children receive literacy instruction they do not just learn how to read but also how to spell. Moreover, the first experiment did not provide information on whether a contrastive deficit would also be obtained in a transparent orthography. The second experiment compared variety match and mismatch conditions on learning to read and spell a transparent (Experiment 2a) and an opaque (Experiment 2b) orthography. In the transparent orthography, we found the expected impaired performance for contrastive words in reading, but not in spelling. There was no effect of variety mismatch on reading and spelling of untrained words. In the opaque orthography (Experiment 2b), adding spelling rendered the overall task considerably more difficult so that a floor effect likely prevented the emergence of a contrastive deficit. To alleviate this difficulty, Experiment 3 examined reading and spelling using the same opaque orthography, but with a longer training phase and a larger sample size. Under these conditions we found a reading impairment for contrastive words. Most remarkably, despite the contrastive deficit, there was an overall benefit from being exposed to a variety mismatch, which was also evident when comparing reading performance between conditions for just the untrained words. We will discuss potential reasons for a such a dialect benefit.

[^1]: Experiment 1 was conducted last but is reported first because it constitutes a replication attempt of the connectionist simulation reported in @Brown2015.

# Experiment 1: Effect of variety mismatch on learning to read an opaque orthography

## Method

### Participants

`r stringr::str_to_sentence(as.english(results$experiment_0$demographics$demographic_summary$n, UK = TRUE))` participants (aged `r results$experiment_0$demographics$demographic_summary$age_min`--`r results$experiment_0$demographics$demographic_summary$age_max`, *M* = `r results$experiment_0$demographics$demographic_summary$age_mean`, *SD* = `r results$experiment_0$demographics$demographic_summary$age_sd`)[^2] who  reported English as their native language, and no known mild cognitive impairments/dementia were recruited from the crowdsourcing website Prolific Academic were reimbursed \pounds 4.20 for participation. An additional 7 participants were tested but not included either because they gave the same response on all trials, responses on most trials repeated the previous trial, responses were provided in English, responses were inaudible or other technical difficulties occurred (e.g. losing trials due to poor internet connection), and when recruitment inadvertently extended beyond our cutoffs for a given list.

[^2]: One participant self-reported an age of 14 which we assume is a typo as Prolific Academic enforces a minimum age of 18.

### Materials

#### Grapheme and Phoneme Inventory

We generated thirteen graphemes (see Appendix\ \@ref(appendix-a)) consisting of two to four curved or straight strokes as common to most alphabetic writing systems [@Changizi2005]. To prevent participants from memorising the novel graphemes based on resemblance to known graphemes we controlled for similarity to characters of extant writing systems by comparing each invented grapheme against the database of 11817 characters (excluding Chinese, Korean, and Japanese) on the Shapecatcher website [@Milde2011]. If visual inspection indicated a resemblance, we modified the grapheme to minimise that resemblance. The phoneme inventory consisted of eight consonants [m], [n], [s], [k], [b], [d], [f] and [l] as well as the five cardinal vowels [a], \textipa{[E]}, [i], \textipa{[O]}, [u] found in most English varieties. Additionally, the dialect phonemic inventory included an additional phoneme, [x], which replaced [k] in certain contexts, as described below.

#### Words

Using this phoneme inventory, we constructed 42 artificial words distributed across six syllabic templates (3 monosyllabic, 3 bisyllabic) adhering to constraints of English phonotactics [@Harley2006;@Crystal2003]. To constrain phonological complexity and to avoid overly predictive clusters, words contained no more than one consonant cluster and no cluster with more than two consonants.

Applying these rules to a string generation algorithm (accessible at [https://osf.io/5mtdj/](https://osf.io/5mtdj/), we produced all possible phoneme permutations per syllable template, and selected seven strings from each template type, by removing strings with phoneme repetitions and ensuring a roughly similar distribution of phonemes across items. To capture a range of English neighborhood densities, we selected a roughly equal number of words with high and low phonological neighbourhood densities according to the Cross-Linguistic Easy-Access Resource for Phonological and Orthographic Neighborhood Densities [@Marian2012] database using the total neighbour metric (i.e. including substitutions, additions, and deletions) resulting in a mean neighbourhood density of 2.88. To minimise confusability of words, our final list was filtered such that each word differed from each other word by a length-normalised Levenshtein Edit Distance (nLED)[^3] of at least 0.5, resulting in an average nLED of 0.86, indicating a large degree of variability across items. This restriction was applied as variability has been shown to support learning of grapheme-phoneme-correspondences [@Apfelbaum2013]. Thirty words were used during exposure and literacy training, while twelve words (two words from each syllable template) were retained for testing only (henceforth: untrained words). Crucially, we randomly assigned phonemes to graphemes for each participant to reduce the potential impact of any systematic differences in the accessibility for certain grapheme-phoneme pairs.

[^3]: A normalised measure computed by dividing the number of insertions, deletions, and substitutions required to transform one string into another by the larger of the two string lengths.

#### Orthography

To create an opaque orthography, we introduced two conditional rules to supplement one-to-one mappings of graphemes to phonemes. First, the phoneme /l/ was rendered by its corresponding grapheme in all contexts except in five instances when it was preceded by /b/ or /s/, in which case it was spelled using the grapheme otherwise assigned to /n/ so that, for example, /blaf/ was spelled as the artificial equivalent of *bnaf*. Second, the phoneme /s/ was rendered by its corresponding grapheme in all contexts except in five instances when it was preceded by an /n/ in which case it was rendered by the grapheme otherwise assigned to /f/ so that, for example, /snid/ was spelled *fnid*. These conditional spelling rules were matched across word type resulting in five contrastive and five non-contrastive words with irregular spelling. For one contrastive and one non-contrastive word, both conditional spelling rules applied simultaneously so that /sloku/ and /slinab/ were spelled as *fnoku* and *fninab*, respectively. 

#### Simulating Dialect Exposure based on the Gruffalo-corpus

```{r gruffalo-token-summary, include = FALSE, warning = FALSE}
gruffalo_token_summary <- gruffalo_token %>% 
  group_by(shift_category) %>% 
  summarise(
    n = length(nLED), 
    total = nrow(.), 
    proportion = length(nLED)/nrow(.),
    mean_nLED = mean(nLED)
  )
```

Because processing of phonological vs. lexical variation might rely on different mechanisms as discussed above, we decided to restrict this study to just one type of variation, namely, whichever is most frequent in prominent naturally occuring dialect varieties of English. This determination requires frequency estimates from transcribed corpora of dialect use which, to our knowledge, do not exist. We therefore used translations of the two popular children's books "The Gruffalo" and "The Gruffalo's Child" [@Donaldson1999; @Donaldson2005], written in Standard British English (SBE), into a number of varieties of Scots, including Dundonian, Glaswegian, and Doric, to obtain such a dialect corpus. The seven books comprising this corpus are listed in Appendix\ \@ref(appendix-b). This approach, in essence, amounts to treating the translators as native dialect informants. Using a corpus derived from children's verses gives us estimates for how dialects differ from standard varieties for language that is appropriate for the age group at which literacy is acquired. The Gruffalo corpus comprised `r gruffalo_counts %>% filter(label == "type") %>% pull(count)` translated word types.

Each of the Scots words in each Gruffalo translation was aligned with its SBE equivalent and coded for whether it differed lexically resulting in a Scots word not existent in SBE (e.g. *big* -- *muckle*[^4]) or phonologically (e.g. *mouse* -- *moose*). To validate this categorisation we computed nLEDS between the SBE and Scots variants for each category (*M* lexical = `r gruffalo_token_summary %>% filter(shift_category == "lexical") %>% pull(mean_nLED)`, *M* phonological = `r gruffalo_token_summary %>% filter(shift_category == "phonological") %>% pull(mean_nLED)`), which confirms that lexical variants in Scots deviate more strongly from SBE than phonological ones. Phonological differences were further sub-categorised as phoneme drops (e.g. *and* -- *an*), substitutions (e.g. *bright* -- *bricht*), or insertions (e.g. *it's* -- *hit's*), and whether diphthongisation (e.g. *ahead* -- *ahaid*) or monophthongisation (e.g. *mouse* -- *moose*) occurred[^5]. A total of `r english(gruffalo_token_summary %>% filter(shift_category == "unsure") %>% pull(n), UK = TRUE)` words involved a difference which could not be reliably categorised as lexical or phonological. Words that arose from paraphrasing the SBE phrases (e.g. *"...that no Gruffalo should ever set foot"* -- *"it wid come tae nae guid if..."*) were excluded from our analysis. 

Analysing the distribution of variants revealed that `r gruffalo_freq %>% filter(label == "token_contrastive") %>% pull(frequency_proportion)*100`% of word tokens and `r gruffalo_freq %>% filter(label == "type_contrastive") %>% pull(frequency_proportion)*100`% of word types were contrastive, i.e. had a dialect variant. Of these contrastive words, `r gruffalo_freq %>% filter(label == "token_constrastive_phon") %>% pull(frequency_proportion)*100`% (`r gruffalo_freq %>% filter(label == "type_constrastive_phon") %>% pull(frequency_proportion)*100`% by type) had variants with phonological differences confirming that phonological variation was most common in terms of token frequency. Of these, the most frequent phonological differences were phoneme substitutions (`r gruffalo_freq %>% filter(label == "token_phoneme_subs") %>% pull(frequency_proportion)*100`% of all phonological differences) and final consonant drops (`r gruffalo_freq %>% filter(label == "token_consonant_drops") %>% pull(frequency_proportion)*100`% of all phonological differences)[^6].

These estimates suggest that inclusion of 50% of words with dialect variants as in @Brown2015 provides an ecologically valid amount of dialect variation. It also suggests that phonological variants are the more frequent ones justifying a focus on phonological variation following @Brown2015. We therefore implemented a range of variations that mimicked those found in the Gruffalo-corpus as listed below:

(a) *consonant substitution*: [k] was changed to [x] in all positions (e.g. /skub/ changed to /sxub/).

(b) *consonant drop*: [d] was dropped in final position (e.g. /snid/ changed to /sni/).

(c) *vowel change*: \textipa{[E]} and [a] were replaced with [i] and \textipa{[O]}, respectively (e.g. /n\textipa{E}f/ changed to /nif/) and /nal/ changed to /n\textipa{O}l/) in all positions.

In instances where multiple changes could apply all were implemented in the "dialect" so that, for example, /sk\textipa{E}fi/ became /sxifi/ and /fl\textipa{E}s\textipa{O}d/ becomes /flis\textipa{O}/.

To examine the role of semantic representations, half of the participants encountered words paired with a picture taken from the revised Snodgrass and Vanderwart image set of colourised images provided by @Rossion2004. We selected seven objects from the six categories of (i) human body parts, (ii) furniture and kitchen utensils, (iii) household objects, tools, and instruments; (iv) food and clothing; (v) buildings, building features, and vehicles; and (vi) animals and plants, resulting in a total of 42 pictures. Amongst each category, we selected images with the highest familiarity scores based on subjective ratings from @Rossion2004, avoiding any items with unclear or incomplete features or those that were deemed to be too similar to another image (e.g. finger and toe) by removing the item with the lower familiarity score and replacing it with the next highest familiarity score in that category (e.g. replacing toe with ear, in this instance). Images were randomly assigned to words for each participant.

The final set of forty-two words and associated images can be found in Appendix \@ref(appendix-c) and \@ref(appendix-d). Each word as well as each isolated phoneme were recorded by a male and female speakers in a soundproof booth with a Zoom H4n Audio recorder, using normal prosody with stress on the first syllable in the bi-syllabic items. Sound files were normalised, with noise filtered using the Audacity audio suite [@Mazzoni2016] and extraneous silences were trimmed using Praat [@Boersma2017].

[^4]: Described by the @DictionaryoftheScotsLanguage as an adjective meaning 'Of size or bulk: large, big, great'.

[^5]: In some cases, such as the Scots *ken*, for *know*, this shift is recorded as a lexical change as the phonology of the word changes dramatically, i.e. [k\textipa{E}n] from [n\textipa{@}\textipa{U}] despite orthographically maintaining the root of the word. Moreover, due to lack of standardisation of Scots spelling, we only could include phonological changes that were orthographically rendered. For example, while the voiceless velar fricative was orthographically rendered in some cases, /x/ (e.g. *right* -- *richt*), in others it was not (e.g. *loch* -- *loch*), and these changes could not be counted in the corpus analysis.

[^6]: Note that these values sum to more than 100% as words could include both phoneme substitutions and consonant drops, amongst other changes.

### Procedure

The experiment was programmed to be compatible with all desktops and Android systems using most web browsers to be administered online on the crowdsourcing platform Prolific Academic. Participants were instructed that they would learn to read a "made-up" language and that it was important to perform the task in a quiet environment and to not take any notes during participation. To ensure compliance the instructions misled participants into believing that detection of cheating on our part could jeopardise reward. After receiving instructions describing the experimental procedure and providing GDPR-compatible consent, participants were asked to check working order of their microphone and headphones/speakers. The experiment consisted of three phases: exposure, training and testing. In the first exposure block, participants heard all thirty training words one by one in randomised order. For half of the participants randomly assigned to the Picture condition, words were accompanied by images depicting their meaning. Participants subsequently viewed each grapheme one by one (cycling twice during the set), accompanied by the sound of the isolated phoneme that was randomly assigned to this grapheme for each participant. Following recommendations to include time limits preventing participants from taking notes in learning experiments [@Rodd2019], graphemes disappeared after 1,000ms. This process was repeated once, such that participants saw each grapheme for a total of four times.

Next, participants proceeded to the reading training, which was interleaved with more exposure. To this end, the set of thirty words was randomly split into three reading training blocks of ten words. For each item in each  training block, participants saw a string of graphemes and had to read the target word out loud. To avoid recording long silences we timed participants' responses by presenting a moving hand in a clock indicating the onset, duration and offset of the 2500ms recording window. In the Picture condition, orthographic representations were accompanied by pictures to simulate availability of semantic context. Although script is typically not accompanied by pictures, such a procedure is justified given that children's early reading materials tend to contain frequent illustrations. Upon completion of each recording, participants received auditory feedback by listening to the target sound file. Each ten-item training block was presented twice in a row. The double presentation of each training block was necessary to equate number of exposures per word with Experiment 2 to maintain comparability. The first double-block was followed by another exposure block comprising all thirty words before proceeding to the second double-block of training, followed by another exposure block. In total, participants underwent three exposure blocks - one at the beginning, one after the first double-block of training, and one after the second double-block of training. After completing the third double-block of training, participants were tested on reading of the thirty trained and the twelve untrained words, all presented in random order without auditory feedback. Participants in the Picture condition saw pictures of the associated referents on all trials.

Crucially, in the Variety Mismatch condition, participants heard the dialect variants of contrastive words during all exposure blocks, but were presented with the standard variant during reading feedback. This condition mimics learning one variant in the home environment (e.g. Scots "hoose") while being required to read another variant in the school environment (e.g. the Standard English "house"). Additionally, the source code for the experiment can be found at [https://osf.io/5mtdj/](https://osf.io/5mtdj/). The mean completion time was `r results$experiment_0$demographics$demographic_summary %>% pull(time_mean) %>% as.numeric("minutes")` minutes (*SD* = `r results$experiment_0$demographics$demographic_summary %>% pull(time_sd) %>% as.numeric("minutes")`).

### Data analysis

```{r r-references-notes, include = FALSE, warning = FALSE}
# r_refs(here("05_paper", "01_manuscript", "r-references.bib")) # make .bib file
my_citations <- cite_r(
  file = here("05_paper", "01_manuscript", "r-references.bib"), 
  pkgs = c(
    "tidyverse", 
    "lme4", 
    "lmerTest", 
    "brms", 
    "emmeans",
    "irr",
    "here", 
    "knitr", 
    "kableExtra", 
    "papaja", 
    "broom.mixed", 
    "english"
  ), 
  withhold = FALSE
)
```

We used `r my_citations` for data preparation, analysis, and presentation.

## Results

### Coding

Two coders (GPW and VK) transcribed all reading responses while blind to each participant's condition. To allow for coding using a standard keyboard while ensuring uniformity across coders, a coding convention was adopted for the 13 target phonemes in the artificial language, which were rendered uniformly using the CPSAMPA [@Marian2012] simplified notation of IPA characters such that [\textipa{@}\textipa{E}i\textipa{O}\textipa{U}] became a, E, i, O, u while the consonants were coded using the letters m, n, s, k, b, d, f, l and x. All extraneous, i.e. non-target phonemes were rendered by single Latin characters that provided the closest match so as to be able to compute length-normalised Levenshtein Edit Distance (nLED), which constitutes a more gradual and fine-grained performance measure allowing us to distinguish near-matches from entirely erroneous productions akin to cross-entropy errors in the neural network simulation by @Brown2015. We computed inter-coder reliability by obtaining intra-class correlations between the two coders' nLEDs, using the *irr* R-package [@R-irr], which was  `r summarise_icc(results$experiment_0$irr_results$icc_results)`. The 95% confidence interval around the parameter estimate indicates that the ICC falls above the bound of .90, which suggests excellent reliability across raters [see @Koo2016].

### Model Fitting

All data processing and analyses, hosted on the Open Science Framework (URL) were conducted used the same programs and packages outlined above. Any deviations from our pre-registered analysis plan are outlined and justified in our pre-registration deviations document, found at [https://osf.io/5mtdj/](https://osf.io/5mtdj/).

We performed separate analyses for the training and testing phases of the experiment. Our dependent variable, the leniently coded nLED, was arcsine square root transformed as variance was not homogenous across the entire range of possible outcomes, with larger variance towards the endpoints of 0 or 1 [@Mirman2014]. All frequentist analyses are supplemented by Bayesian analyses which, although not fully adopted as standard in studies of this kind, provide a range of additional advantages [@Nicenboim2016; @Vasishth2018]: Maximal random effect structures [@Barr2013] can be fitted without convergence problems and data can be interrogated directly for null-effects which is crucial for this study as it allows us to test directly the possibility that dialect exposure might not have any detrimental effects on acquisition of literacy skills. For the frequentist analyses, we modelled the data with linear mixed effects models fitted using the *lme4* R-package [@R-lme4]. Statistical significance of each term was evaluated using *p*-values approximated using the Satterthwaite method implemented in the *lmerTest* R-package [@R-lmerTest]. We used the maximal random effects structure that allowed for model convergence throughout [@Barr2013]. 

For the Bayesian analyses, we fitted linear mixed-effects models using the *brms* R-package [@R-brms_a; @R-brms_b] with the same fixed effects as in the frequentist models and a maximal random effects structure. To simplify the definition of priors for the estimated parameters, we scaled and centred the dependent variable on a mean of 0 with a standard deviation of 1. We used a regularising, weakly informative prior, $Normal(0, 1)$, for the intercept term. Additionally, we used an informative prior for all fixed effects terms, defined as $Normal(0, 0.2)$, except for fixed effects involving time-terms. This prior places a larger probability on small effects for any of the parameter estimates. For any fixed effects including time terms (i.e. each time term and any interactions of other effects with time terms; included in the training phase only), we used very weakly informative priors, defined as $Normal(0, 10)$, which allows these effects to be dominated by the likelihood. We also used regularising priors for the correlation parameters, $LKJ(2)$, which downweights perfect correlations [@Vasishth2018]. Additionally, the standard deviations of random effects and the residual error used the default priors used in *brms*[@R-brms_a; @R-brms_b] for these terms at the time of writing. Specifically, these priors are defined as half student's-*t* priors (i.e. constrained to be non-negative) with 3 degrees of freedom and, minimally, a scale parameter of 10. Without a predefined region of practical equivalence [@Kruschke2018], we use the 95% credible interval around the posterior mean to summarise these models. As @Nicenboim2016 note, the 95% credible interval provides the range of values within which the true value of the parameter lies with 95% probability given the model and data. Thus, when a 95% credible interval includes zero, we conclude that we do not have sufficient evidence against a null result. However, when a 95% credible interval does not include zero, we conclude that we have evidence for a non-zero directional effect [see @Burkner2019 for use of similar criteria][^7].

[^7]: Note: We initially attempted to evaluate evidence against for and against the null hypothesis for each term in our model using Bayes factors calculated using the generalTestBF function from the *BayesFactor* R-package [@R-BayesFactor]. However, this resulted in Bayes factors with a large proportional error. Following this, we calculated Bayes factors using the hypothesis function from the *brms* R-package [@R-brms_a; @R-brms_b] (using the Savage-Dickey density ratio). However, as @Nicenboim2016 discuss, with wide, weakly informative priors, the Bayes factor will always favour the null hypothesis as the alternative hypothesis is penalised for including large (and unlikely) values in the prior. As such, we rely on interpreting the 95% credible interval, rather than Bayes factors, to interpret non-significant results.

#### Training

Training data were modelled using growth-curve analyses [e.g. @Mirman2014] to establish change in performance over time, i.e. from block to block, across conditions. Time was modelled using fixed effects of orthogonal linear and quadratic polynomials to capture the (potential) non-linear change in performance as learning progresses. Note that quadratic terms are included to improve model fit but their interactions with other fixed effects do not lend themselves to meaningful interpretation and will therefore not be considered. The model also included sum-coded fixed effects of Picture Condition (picture vs. no picture), Variety (match vs. mismatch) and Word Type (contrastive vs. non-contrastive). We used nested fixed effects for these terms [see @Schad2018 for a discussion of this approach], with Word Type nested within the interaction between all other fixed effects. As a result of this parameterisation, the intercept represents the average of condition means throughout the entire time window (and not at the first block), individual terms (except Word Type) represent main effects for the given term, and Word Type effects represent simple effects within each combination of the other factors (e.g. Word Type within each level of Picture Condition and Variety Condition over the entire time). All other interactions aside from those involving Word Type are interpreted as usual. In the frequentist model, the random effect structure included  zero-correlation random intercepts and slopes of Picture Condition, Variety Condition, and their interaction by items, and random intercepts, slopes (including correlations) for the linear and quadratic Time terms, Word Type, and their interaction by subjects. The results of the models including parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table 1. In our description and interpretation of the results we will focus on those effects that reached significance in the frequentist analysis and had credible intervals that did not include 0 in the Bayesian analysis (marked in boldface in Table\ \@ref(tab:ex0-train)).

```{r ex0-train, results = "asis", warning = FALSE}
ex0_train_nhst <- results$experiment_0$main_models$lme_train %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex0_train_bayes <- 
  results$experiment_0$planned_comparisons_Bayes$brm_train$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex0_train <- bind_cols(ex0_train_nhst, ex0_train_bayes) %>% 
  rename_nested_terms()

# define rows with significant effect (and no 95% cred I that cross 0)
# doing manually for instances with, e.g. -0.00 etc.
ex0_train_sig_rows <- c(2, 3, 11, 13, 24)

kable(
  ex0_train,
  format = "latex",
  booktabs = TRUE, 
  caption = "Experiment 1. Parameter estimates for the models fitted to nLEDs from the training phase. Bayesian analyses report standardised parameter estimates (i.e. the intercept [grand mean] is centred at 0). Values of 0 with a sign indicate the direction of the estimate before rounding.", 
  format.args = table_format_args,
  digits = 2,
  col.names = shared_colnames,
  longtable = multipage_table,
  escape = FALSE,
  linesep = "" # avoids spacing every 5 rows
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"),
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  footnote(
    general =
      c("Block (B) = 1-6, Variety Condition (VC) = variety match vs. variety mismatch (VMa vs. VMi),",
        "Picture Condition (PC) = picture vs. no picture (P vs. NP),",
        "Word Type (WT) = contrastive vs. non-contrastive"), 
    general_title = "", 
    footnote_as_chunk = FALSE, 
    escape = FALSE,
    threeparttable = multipage_table # unused; needs conditionals
    ) %>%
  add_header_above(
    c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
  ) %>%
  row_spec(ex0_train_sig_rows, bold = TRUE)
```

Our results show reduction of nLEDs across the three blocks showing that participants' reading of the artificial script improved over time. The significant quadratic term suggests that in many instances more progress was made between Blocks 1 and 2 than between Blocks 2 and 3. Crucially, the contrastive deficit was significant in the Variety Mismatch condition without pictures and marginally significant in the Variety Mismatch condition with pictures, broadly confirming greater difficulties with reading contrastive words (see Figure\ \@ref(fig:ex0-train-plots)). In addition,  there was a significant 3-way interaction between Block, Picture condition and Variety condition, which, however, is not of interest to the main questions of this study. 

```{r ex0-train-plots, fig.cap = "Experiment 1. Length-normalised Levenshtein Edit Distance for reading training of contrastive and non-contrastive words over 3 double-blocks (coded as 6 blocks in the analyses but presented as 3 double-blocks for comparability with Experiment 2) in the variety match and mismatch conditions. Error bars indicate $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-0_training_plot_reading.png"
))
```

#### Testing

For the analysis of the testing phase, we used the same fixed effect structure as for the analysis of the training phase with the exclusion of the linear and quadratic effects of Block. The only difference was that here Word Type was modelled using Helmert contrasts, such that contrastive words were compared to non-contrastive words and untrained words were compared to the average of contrastive and non-contrastive words (i.e. the trained words). For the testing phase, the random effects structure included random intercepts and slopes of Picture Condition, Language Variety, and their interaction by items, and random intercepts and slopes of Word Type by subjects.

```{r ex0-test, results = "asis", warning = FALSE}
ex0_test_nhst <- results$experiment_0$main_models$lme_test %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex0_test_bayes <- 
  results$experiment_0$planned_comparisons_Bayes$brm_test$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex0_test <- bind_cols(ex0_test_nhst, ex0_test_bayes) %>% 
  rename_nested_terms()

# define rows with significant effect (and no 95% cred I that cross 0)
ex0_test_sig_rows <- 12

kable(
  ex0_test,
  format = "latex",
  booktabs = TRUE, 
  caption = "Experiment 1. Parameter estimates for the models fitted to nLEDs from the testing phase. Bayesian analyses report standardised parameter estimates (i.e. the intercept [grand mean] is centred at 0). Values of 0 with a sign indicate the direction of the estimate before rounding.", 
  format.args = table_format_args,
  digits = 2,
  col.names = shared_colnames,
  longtable = multipage_table,
  escape = FALSE,
  linesep = "" # avoids spacing every 5 rows
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"), 
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  footnote(
    general =
      c("Variety Condition (VC) = variety match vs. variety mismatch (VMa vs. VMi),",
        "Picture Condition (PC) = picture vs. no picture (P vs. NP),",
        "Word Type (WT) = contrastive vs. non-contrastive, Word Familiarity (WF) = familiar vs. unfamiliar (novel)"),
    general_title = "", 
    footnote_as_chunk = FALSE, 
    escape = FALSE,
    threeparttable = multipage_table
    ) %>%
  add_header_above(
    c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
  ) %>%
  row_spec(ex0_test_sig_rows, bold = TRUE)
```

We found that the contrastive deficit failed to reach significance in the Variety Mismatch condition. The effect of Word Familiarity was significant in the Variety Match condition with pictures and fell short of significance in the Variety Mismatch condition with pictures suggesting that when pictures were present allowing for establishment of links between word form and meaning phonological decoding skills were applied insufficiently as participants were more likely to access the phonological form via a word's meaning rather than applying grapheme-phoneme conversion rules. 

We performed a planned direct comparison of performance on untrained words only between the Variety Match and Variety Mismatch conditions. The model included fixed effects and interactions between the sum-coded Picture Condition and Language Variety. We used the same criteria as in our main models for determining the random effects structure of the model. Here, this took the form of random zero-correlation intercepts and slopes of Picture Condition and Language Variety and their interaction by items, and random intercepts by subjects. 

```{r ex0-test-novel, include = FALSE, warning = FALSE}
ex0_novel_nhst <- results$experiment_0$planned_comparisons_NHST$test_l_n %>% 
  summarise_lme(., term, main_terms) %>%
  filter(term == "Variety Condition")

ex0_novel_bayes <- 
  results$experiment_0$planned_comparisons_Bayes$brm_test_novel$fixed %>% 
  summarise_bayes(., term, main_terms) %>%
  filter(term == "Variety Condition")
```

This comparison showed no effect of variety mismatch (frequentist estimate: $\hat{\beta}$ = `r ex0_novel_nhst$estimate``r ex0_novel_nhst$"95% CI"`, *t* = `r ex0_novel_nhst$statistic`, *p* = `r ex0_novel_nhst$p.value`; Bayesian Estimate: $\hat{\beta}$ = `r ex0_novel_bayes$Estimate``r ex0_novel_bayes$"95% CI"`), thus failing to obtain conclusive evidence for a detrimental effect of dialect exposure on phonological decoding skills.

```{r ex0-test-plots, fig.cap = "Experiment 1. Length-normalised Levenshtein Edit Distance for reading testing of trained non-contrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-0_testing_plot_reading.png"
))
```

## Discussion

In this experiment, participants learned to read 30 words of an artificial language using an artificial script. One group learned to read the same variants as they had encountered during exposure while for the other group half of the words varied between exposure and literacy acquisition mimicking a situation of dialect exposure. Orthogonal to the manipulation of Variety Mismatch, half of the participants saw pictures when hearing the words enabling them to develop semantic representations while the other half did not. Reading performance improved significantly over the course of training in both the variety match and mismatch conditions although the gains were steeper in the Variety Mismatch condition with pictures. We had predicted that performance would be worse for contrastive compared to non-contrastive words in the variety mismatch condition. While the results confirmed this trend, the contrastive deficit only reached significance during training in the No Picture condition, in line with findings from the reading experiment and the connectionist simulation of exposure to AAE by @Brown2015. However, in that simulation the contrastive deficit arose solely from similarity between the phonological representations of the AAE and MAE variants and not from competition between word forms associated with the same concept. Our experiment was not able to unequivocally establish the role of semantic representations on the magnitude of the contrastive deficit as we only observed it in the No Picture condition during training but not reliably during testing. It should be noted, however, that participants displayed extraordinary variability due to the considerable difficulty of the task. Thus, our results remain equivocal with respect to whether contrastive deficits persist when semantic representations are being established. 

To the extent that beginning readers rely on direct associations between print and meaning the encounter of untrained words should disrupt performance as phonologically mediated decoding skills need to be employed. The Word Familiarity effect is therefore a measure for how reliably such grapheme-phoneme conversion rules have been acquired. In this experiment, reduced performance with untrained, and hence unfamiliar, words was only observed in the variety match condition with pictures. This suggests that when no competing variants were present in the input and semantic information was available learners may have attempted to memorise the direct associations between the orthographic and the phonological form of a word. It should be noted that there was also a marginal effect of word familiarity when pictures were provided in the variety mismatch condition confirming that availability of semantic information in general encouraged a strategy of establishing direct associations between meanings and phonological forms rather than trying to retrieve the sound based decoding of the orthographical form. There was no word familiarity effect in the No Picture conditions because attempting to forge direct links between graphemic and phonemic representations that bypass decoding in the absence of semantic cues is extremely difficult.  

The crucial question was whether exposure to competing variants would affect learners emerging phonological decoding skills. To answer this question, we compared reading performance for untrained words between the Variety Match and Mismatch conditions. If dialect exposure hinders literacy acquisition as suspected by the Head Teacher mentioned in our introductory paragraph we would expect poorer performance with untrained words in the variety mismatch condition, yet the comparison of reading performance of untrained words showed no difference to the variety match condition. However, Bayesian analyses designed to estimate the strength of evidence for the null hypothesis indicated that there was insufficient evidence for lack of an effect. We therefore can neither confirm nor exclude the possibility that dialect exposure impairs decoding skills. 

One possible reason for this result is that the absence of spelling training may have prevented participants from attempting to convert graphemes into phonemes. However, beginning readers never learn to read only but also learn to spell  as primary schools tend to incorporate writing instruction into their curricula from early on [@Cutler2008]. Spelling training strengthens the connections between individual phonemes and graphemes thereby promoting use of decoding skills. For example, @Taylor2017 showed that included a spelling task into the training regimen for an artificial script encouraged a phonologically-mediated reading acquisition strategy. While the child participants in the @Brown2015 study would certainly have engaged in spelling practice during their schooling, the corresponding neural network did not include bi-directional links that could have instantiated a "spelling path", i.e. a path from phonological to graphemic representations [see @Houghton2003], and we are not aware of any attempts to computationally model to contribution of spelling practice to emergent reading skills. Thus, it is not clear whether under conditions that encourage decoding skills like spelling practice detrimental effects of dialect exposure should be expected.

In this experiment, using grapheme-phoneme conversion rules had also been made difficult by the opaque spelling system that was designed to mimic the acquisition of a deep orthography like English. Recall that we implemented two conditional rules where grapheme-phoneme associations changed depending on orthographic context. This complex conditional rule system is likely to have further discouraged discovery and use of grapheme-phoneme conversion rules. To promote learning of these rules and to encourage phonologically-mediated reading, we presented a transparent orthography in Experiment 2a and the opaque orthography from Experiment 1 in Experiment 2b. The comparison will be instructive for trying to understand the potential role of dialect exposure in languages with a more shallow orthography, such as, for example, the effect of exposure to Swiss German on acquiring literacy in Standard German. 

# Experiment 2: Effect of variety mismatch on learning to read and spell

The aim of Experiment 2 was to provide more ecologically valid training conditions by examining the effect of exposure to variety mismatch when participants learned to read and to spell a transparent (Experiment 2a) and an opaque (Experiment 2b) orthography.

# Experiment 2a: Transparent Orthography

## Method

### Participants

`r stringr::str_to_sentence(as.english(results$experiment_1$demographics$demographic_summary$n, UK = TRUE))` participants (aged `r results$experiment_1$demographics$demographic_summary$age_min`--`r results$experiment_1$demographics$demographic_summary$age_max`, aged *M* = `r results$experiment_1$demographics$demographic_summary$age_mean`, *SD* = `r results$experiment_1$demographics$demographic_summary$age_sd`) were recruited from Amazon's Mechanical Turk and took part in the study for \$7.50. All participants reported proficiency in English (1-7 Likert scale: *M* = `r results$experiment_1$demographics$english_proficiency$mean`, *SD* = `r results$experiment_1$demographics$english_proficiency$SD`). Another 2 participants were tested and excluded based on the exclusion criteria described for Experiment 1.

### Materials

We used the same set of graphemes, phonemes and words as in Experiment 1. In contrast to Experiment 1, we adopted only one-to-one mappings between graphemes and phonemes resulting in an entirely transparent, shallow orthography.

### Procedure

The procedure was identical to Experiment 1 aside from the following deviation: In the training phase that followed exposure to the grapheme-phoneme mappings, each ten-word block was presented twice, once for reading training and once for spelling training. During spelling training participants heard a word and had to type it by clicking graphemes using an on-screen keyboard. Participants in the Picture condition always saw the picture of the associated referent when hearing the word. Once participants had pressed the on-screen "Enter" key the correct spelling appeared below their own spelling for purposes of feedback. The screen was cleared after 1.5 to 3.0 sec to prevent participants to take notes or obtain screenshots (the exact time was determined dynamically based on the word length, where for each letter the duration would last for an additional 500ms). The overall amount of exposure to each item, combining presentations for reading and spelling, was identical to Experiment 1. In the testing phase, participants were presented with all thirty training words and an additional twelve untrained words in randomised order. Just as the training phase, the testing phase also contained both a reading and a spelling stage. Order of reading and spelling task was counterbalanced across participants but was kept constant across all phases within participants. The mean completion time was `r results$experiment_1$demographics$demographic_summary %>% pull(time_mean) %>% as.numeric("minutes")` minutes (*SD* = `r results$experiment_1$demographics$demographic_summary %>% pull(time_sd) %>% as.numeric("minutes")`). An example of the experimental procedure can be found at [https://language.abertay.ac.uk/allp-demo/](https://language.abertay.ac.uk/allp-demo/).

## Results

### Coding

We used the same coding scheme for reading responses as in Experiment 1. The ICC between coders was `r summarise_icc(results$experiment_1$irr_results$icc_results)`. The 95% confidence interval around the parameter estimate indicates that the ICC falls above the bound of .90, which suggests excellent reliability across raters [see @Koo2016].

### Model Fitting

Model fitting was similar to Experiment 1, with the exception of the inclusion of a sum-coded fixed effect of Task (reading vs. spelling) and inclusion of Task in the random effects for the analyses of the training and testing data. Additionally, since the training phase contained 3 blocks of training for each task, the training models were changed to include only an orthogonal linear (and not quadratic) time term as a fixed and random effect. This change was made to avoid overfitting for change over time with only 3 time points. As in Experiment 1, the fixed effects for training and testing data were modelled with a nested structure, with Word Type nested within all other factors (i.e. Task, Variety Condition, Picture Condition in both phases, and with the addition of the orthogonal linear effect of time in the training phase). Thus, we obtained all main effects and interactions between all factors excluding Word Type, and simple effects of Word Type within each combination of the levels for the other factors. 

For the training phase, the maximal converging random effects structure took the form of zero-correlation random intercepts and slopes of Task, Picture Condition, Variety Condition, and their interaction by items, and zero-correlation random intercepts and slopes for the linear time term, Task, Word Type, and their interaction by subjects. For the testing phase, the random effects structure took the form of random intercepts and slopes of Task, Picture Condition, Variety Condition, and their interaction by items, and random intercepts and slopes for Task, Word Type, and their interaction by subjects (including correlations between all terms for both by-items and by-subjects random effects).

As with Experiment 1, we also modelled the data using Bayesian mixed effects models with a maximal random effects structure. These models used the same priors as in Experiment 1, with the addition of informative, $Normal(0, 0.2)$ priors on the fixed effect of Task and any interactions of other terms with this factor. We used these models to evaluate evidence in support of the null hypothesis for each parameter using in the same way as in Experiment 1.

#### Training

Parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex1-train) and depicted in Figures\ \@ref(fig:ex1-train-reading-plots) and \@ref(fig:ex1-train-spelling-plots). 

```{r ex1-train, results = "asis", warning = FALSE}
ex1_train_nhst <- results$experiment_1$main_models$lme_train %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex1_train_bayes <- 
  results$experiment_1$planned_comparisons_Bayes$brm_train$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex1_train <- bind_cols(ex1_train_nhst, ex1_train_bayes) %>% 
  rename_nested_terms()

# define rows with significant effect (and no 95% cred I that cross 0)
ex1_train_sig_rows <- c(2, 3, 17, 19, 27)

kable(
  ex1_train,
  format = "latex",
  booktabs = TRUE, 
  caption = "Experiment 2a. Parameter estimates for the models fitted to nLEDs from the training phase. Bayesian analyses report standardised parameter estimates (i.e. the intercept [grand mean] is centred at 0). Values of 0 with a sign indicate the direction of the estimate before rounding.", 
  format.args = table_format_args,
  digits = 2,
  col.names = shared_colnames,
  longtable = multipage_table,
  escape = FALSE,
  linesep = "" # avoids spacing every 5 rows
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"), 
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  footnote(
    general =
      c("Block (B) = 1-3, Variety Condition (VC) = variety match vs. variety mismatch (VMa vs. VMi),",
        "Picture Condition (PC) = picture vs. no picture (P vs. NP), Task Condition (TC) = reading vs. spelling (R vs. S),",
        "Word Type (WT) = contrastive vs. non-contrastive"),
    general_title = "", 
    footnote_as_chunk = FALSE, 
    escape = FALSE,
    threeparttable = multipage_table
    ) %>%
  add_header_above(
    c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
  ) %>%
  row_spec(ex1_train_sig_rows, bold = TRUE)
```

The results showed a main effect of Block, indicating an overall improvement of performance as training progressed, as well as a main effect of Task demonstrating better performance for reading than for spelling. Crucially, we found that reading, but not spelling, of contrastive words, as indicated by the effect of Word Type, was significantly impaired in the Variety Mismatch condition regardless of Picture condition. In the Picture condition, the effect of Word Type in reading in the Variety Mismatch condition interacted with Block reflecting the fact that impaired performance for contrastive words started to manifest itself gradually over the course of training.

```{r ex1-train-reading-plots, fig.cap = "Experiment 2a. Length-normalised Levenshtein Edit Distance for reading of contrastive and non-contrastive words during 3 training blocks in the variety match and variety mismatch conditions. Error bars indicate $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-1_training_plot_reading.png"
))
```

```{r ex1-train-spelling-plots, fig.cap = "Experiment 2a. Length-normalised Levenshtein Edit Distance for spelling of contrastive and non-contrastive words during 3 training blocks in the variety match and variety mismatch conditions. Error bars indicate $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-1_training_plot_spelling.png"
))
```

#### Testing

Parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex1-test) and depicted in Figures\ \@ref(fig:ex1-test-reading-plots) and \@ref(fig:ex1-test-spelling-plots). The results confirmed the main effect of task observed during training which showed that performance was superior for reading compared to spelling. As during training, we found an effect of Word Type, which is indicative of impaired performance with contrastive words, in the Variety Mismatch condition but only when pictures were present. The effect of Word Familiarity was significant in all conditions except for spelling in the Variety Mismatch condition with pictures, although Bayesian analyses failed to corroborate it for spelling in the Variety Match condition without pictures.

```{r ex1-test, results = "asis", warning = FALSE}
ex1_test_nhst <- results$experiment_1$main_models$lme_test %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex1_test_bayes <- 
  results$experiment_1$planned_comparisons_Bayes$brm_test$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex1_test <- bind_cols(ex1_test_nhst, ex1_test_bayes) %>% 
  rename_nested_terms()

# define rows with significant effect (and no 95% cred I that cross 0)
ex1_test_sig_rows <- c(2, 11, 17:19, 21, 23:24)

kable(
  ex1_test,
  format = "latex",
  booktabs = TRUE, 
  caption = "Experiment 2a. Parameter estimates for the models fitted to nLEDs from the testing phase. Bayesian analyses report standardised parameter estimates (i.e. the intercept [grand mean] is centred at 0). Values of 0 with a sign indicate the direction of the estimate before rounding.", 
  format.args = table_format_args,
  digits = 2,
  col.names = shared_colnames,
  longtable = multipage_table,
  escape = FALSE,
  linesep = "" # avoids spacing every 5 rows
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"), 
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  footnote(
    general =
      c("Variety Condition (VC) = variety match vs. variety mismatch (VMa vs. VMi),",
        "Picture Condition (PC) = picture vs. no picture (P vs. NP), Task Condition (TC) = reading vs. spelling (R vs. S),",
        "Word Type (WT) = contrastive vs. non-contrastive, Word Familiarity (WF) = familiar vs. unfamiliar (novel)"),
    general_title = "", 
    footnote_as_chunk = FALSE, 
    escape = FALSE,
    threeparttable = multipage_table
    ) %>%
  add_header_above(
    c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
  ) %>%
  row_spec(ex1_test_sig_rows, bold = TRUE)
```

```{r ex1-test-reading-plots, fig.cap = "Experiment 2a. Length-normalised Levenshtein Edit Distance for testing reading performance for trained non-contrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-1_testing_plot_reading.png"
))
```

```{r ex1-test-spelling-plots, fig.cap = "Experiment 2a. Length-normalised Levenshtein Edit Distance for testing spelling performance for trained non-contrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-1_testing_plot_spelling.png"
))
```

```{r ex1-test-novel, include = FALSE, warning = FALSE}
ex1_novel_nhst <- results$experiment_1$planned_comparisons_NHST$test_l_n %>% 
  summarise_lme(., term, main_terms) %>%
  filter(term == "Variety Condition")

ex1_novel_bayes <- 
  results$experiment_1$planned_comparisons_Bayes$brm_test_novel$fixed %>% 
  summarise_bayes(., term, main_terms) %>%
  filter(term == "Variety Condition")
```

As in Experiment 1, we performed a planned direct comparison of performance on untrained words between all Variety Match and Variety Mismatch conditions. The model included fixed effects and interactions between the sum-coded Task Condition, Picture Condition and Language Variety. We used the same criteria as in our main models for determining the random effects structure of the model. Here, this took the form of zero-correlation random intercepts and slopes of Task Condition, Picture Condition and Language Variety and their interaction by items, and random intercepts by subjects. As in Experiment 1, the effect of Variety provided no evidence for a detrimental effect of a variety mismatch on reading and spelling of untrained words (frequentist estimate: $\hat{\beta}$ = `r ex1_novel_nhst$estimate``r ex1_novel_nhst$"95% CI"`, *t* = `r ex1_novel_nhst$statistic`, *p* = `r ex1_novel_nhst$p.value`; Bayesian Estimate: $\hat{\beta}$ = `r ex1_novel_bayes$Estimate``r ex1_novel_bayes$"95% CI"`).

## Discussion

When spelling was introduced to the literacy training in a transparent artificial orthography, we observed that a contrastive deficit emerged in reading when participants were exposed to variety mismatch, which persisted into the testing session when pictures provided an opportunity to establish semantic representations. This contrastive deficit in reading replicates the contrastive deficit found by @Brown2015 for both children and a neural network exposed to both AAE and MAE but unlike in that study, it was more persistent when participants were able to establish semantic representations. It is noteworthy that no contrastive deficit emerged for spelling. This is because no competing orthographic representations for words in the exposure variety (i.e. the "dialect") exist and learners are likely to adopt a strategy of sequentially converting phonemes into graphemes.

We had expected that introducing spelling would also facilitate reliance on sequential decoding using grapheme-phoneme conversion during reading. Had participants adopted such a strategy exclusively, we should not have observed any effect of word familiary manifesting itself in impaired performance for untrained words. Yet the effect of word familiarity was significant in all reading conditions and even some of the spelling conditions suggesting that despite the transparent orthography, which encourages systematic mapping of graphemes onto phoneme and vice versa, participant still to some extent attempted to access phonological (in the case of reading) and even orthographic (in the case of spelling) representations more directly. Specifically, their emerging word knowledge may have resulted in a reading strategy that involved decoding just enough graphemes to be able to access the memorised word form so that knowledge of word forms benefitted trained but not untrained items. The familiarity effect in spelling is a bit more puzzling but may reflect emergence of representations of the overall graphemic Gestalt or even the motor routine required to type a word which may have facilitated spelling. Most importantly for the main question of the study, the lack of evidence for a difference between the variety match and mismatch conditions in reading and spelling of untrained words suggests that concurrent exposure to another variety did not appear to have any further detrimental effect on whatever limited amount of decoding skills participants had acquired.

# Experiment 2b: Opaque Orthography

## Method

### Participants

`r stringr::str_to_sentence(as.english(results$experiment_2$demographics$demographic_summary$n, UK = TRUE))` participants (aged aged `r results$experiment_2$demographics$demographic_summary$age_min`--`r results$experiment_2$demographics$demographic_summary$age_max`, *M* = `r results$experiment_2$demographics$demographic_summary$age_mean`, *SD* = `r results$experiment_2$demographics$demographic_summary$age_sd`) were recruited from Amazon's Mechanical Turk and took part in the study for \$7.50. All participants reported proficiency in English (1-7 Likert scale: *M* = `r results$experiment_2$demographics$english_proficiency$mean`, *SD* = `r results$experiment_2$demographics$english_proficiency$SD`). Another 3 participants were tested and excluded based on the exclusion criteria described for Experiment 1.

### Materials

Graphemes, words and pictures were identical to the previous two experiments. We used the same opaque orthography as in Experiment 1.

### Procedure

The procedure was identical to Experiment 2a. The mean completion time was `r results$experiment_2$demographics$demographic_summary %>% pull(time_mean) %>% as.numeric("minutes")` minutes (*SD* = `r results$experiment_2$demographics$demographic_summary %>% pull(time_sd) %>% as.numeric("minutes")`).

## Results

### Coding

We used the same coding scheme for reading responses as in the previous experiments. The ICC between coders was `r summarise_icc(results$experiment_2$irr_results$icc_results)`. The 95% confidence interval around the parameter estimate indicates that the ICC falls above the bound of .90, which suggests excellent reliability across raters [see @Koo2016].

### Model Fitting

Frequentist and Bayesian analyses were conducted in the same way as for Experiment 2b. However, due to differences in convergence for the frequentist analyses across the two experiments, Experiment 2b had some minor differences in the random effects structure of frequentist models when compared to Experiment 2a. For the training phase, the maximal converging random effects structure included correlations between all by-participant terms. For the testing phase, correlations were suppressed between all random effects terms.

#### Training

Parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex2-train) and depicted in Figures\ \@ref(fig:ex2-train-reading-plots) and \@ref(fig:ex2-train-spelling-plots). 

```{r ex2-train, results = "asis", warning = FALSE}
ex2_train_nhst <- results$experiment_2$main_models$lme_train %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex2_train_bayes <- 
  results$experiment_2$planned_comparisons_Bayes$brm_train$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex2_train <- bind_cols(ex2_train_nhst, ex2_train_bayes) %>% 
  rename_nested_terms()

# define rows with significant effect (and no 95% cred I that cross 0)
ex2_train_sig_rows <- c(2, 3, 8)

kable(
  ex2_train,
  format = "latex",
  booktabs = TRUE, 
  caption = "Experiment 2b. Parameter estimates for the models fitted to nLEDs from the training phase. Bayesian analyses report standardised parameter estimates (i.e. the intercept [grand mean] is centred at 0). Values of 0 with a sign indicate the direction of the estimate before rounding.", 
  format.args = table_format_args,
  digits = 2,
  col.names = shared_colnames,
  longtable = multipage_table,
  escape = FALSE,
  linesep = "" # avoids spacing every 5 rows
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"), 
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  footnote(
    general =
      c("Block (B) = 1-3, Variety Condition (VC) = variety match vs. variety mismatch (VMa vs. VMi),",
        "Picture Condition (PC) = picture vs. no picture (P vs. NP), Task Condition (TC) = reading vs. spelling (R vs. S),",
        "Word Type (WT) = contrastive vs. non-contrastive"),
    general_title = "", 
    footnote_as_chunk = FALSE, 
    escape = FALSE,
    threeparttable = multipage_table
    ) %>%
  add_header_above(
    c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
  ) %>%
  row_spec(ex2_train_sig_rows, bold = TRUE)
```

```{r ex2-train-posthocs, include = FALSE, warning = FALSE}
ex2_train_posthocs_test <- 
  results$experiment_2$post_hoc_comparisons$train_pt_pair %>%
  pairs(by = NULL, adjust = "holm") %>%
  as_tibble() %>%
  filter(p.value < .05)

ex2_train_posthocs_confint <- 
  results$experiment_2$post_hoc_comparisons$train_pt_pair %>%
  pairs(by = NULL, adjust = "holm") %>%
  confint()

ex2_train_posthocs <- left_join(
  ex2_train_posthocs_test, 
  ex2_train_posthocs_confint,
  by = c("contrast", "estimate", "SE", "df")
) %>%
  merge_CI_limits(., "lower.CL", "upper.CL") %>%
  mutate(p.value = papa(p.value, asterisk = FALSE))
```

As in Experiment 2a, the main effect of Block indicated improvement in performance over the course of training and the main effect of task confirmed that learning to spell was more difficult than learning to read. The only other significant effect was an interaction between task and picture condition. Pairwise contrasts based on the estimated marginal means of the training model were calculated using the *emmeans* R-package [@R-emmeans], using Holm's sequential Bonferroni correction. These contrasts indicate that reading performance was better than writing performance in the picture condition only (Picture, Reading - Writing: $\Delta{M}$ = `r ex2_train_posthocs$estimate``r ex2_train_posthocs$"95% CI"`, *t* = `r ex2_train_posthocs$t.ratio`, *p* = `r ex2_train_posthocs$p.value`). All other contrasts were non-significant (*p* > .05). Unlike Experiment 1, we did not find any evidence for a contrastive deficit.

```{r ex2-train-reading-plots, fig.cap = "Experiment 2b. Length-normalised Levenshtein Edit Distance for reading of contrastive and non-contrastive words during 3 training blocks in the variety match and variety mismatch conditions. Error bars indicate $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-2_training_plot_reading.png"
))
```

```{r ex2-train-spelling-plots, fig.cap = "Experiment 2b. Length-normalised Levenshtein Edit Distance for spelling of contrastive and non-contrastive words during 3 training blocks in the variety match and variety mismatch conditions. Error bars indicate $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-2_training_plot_spelling.png"
))
```

#### Testing

Parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex2-test) and depicted in Figures\ \@ref(fig:ex2-test-reading-plots) and \@ref(fig:ex2-test-spelling-plots). 

```{r ex2-test, results = "asis", warning = FALSE}
ex2_test_nhst <- results$experiment_2$main_models$lme_test %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex2_test_bayes <- 
  results$experiment_2$planned_comparisons_Bayes$brm_test$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex2_test <- bind_cols(ex2_test_nhst, ex2_test_bayes) %>% 
  rename_nested_terms()

# define rows with significant effect (and no 95% cred I that cross 0)
ex2_test_sig_rows <- c(2, 5, 23)

kable(
  ex2_test,
  format = "latex",
  booktabs = TRUE, 
  caption = "Experiment 2b. Parameter estimates for the models fitted to nLEDs from the testing phase. Bayesian analyses report standardised parameter estimates (i.e. the intercept [grand mean] is centred at 0). Values of 0 with a sign indicate the direction of the estimate before rounding.", 
  format.args = table_format_args,
  digits = 2,
  col.names = shared_colnames,
  longtable = multipage_table,
  escape = FALSE,
  linesep = "" # avoids spacing every 5 rows
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"), 
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  footnote(
    general = c(
      "Variety Condition (VC) = variety match vs. variety mismatch (VMa vs. VMi),",
        "Picture Condition (PC) = picture vs. no picture (P vs. NP), Task Condition (TC) = reading vs. spelling (R vs. S),",
        "Word Type (WT) = contrastive vs. non-contrastive, Word Familiarity (WF) = familiar vs. unfamiliar (novel)"),
    general_title = "", 
    footnote_as_chunk = FALSE, 
    escape = FALSE,
    threeparttable = multipage_table
    ) %>%
  add_header_above(
    c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
  ) %>%
  row_spec(ex2_test_sig_rows, bold = TRUE)
```

```{r ex2-test-posthocs, include = FALSE, warning = FALSE}
ex2_test_posthocs_test <- 
  results$experiment_2$post_hoc_comparisons$test_pt_pair %>%
  pairs(by = NULL, adjust = "holm") %>%
  as_tibble() %>%
  filter(p.value < .05)

ex2_test_posthocs_confint <- 
  results$experiment_2$post_hoc_comparisons$test_pt_pair %>%
  pairs(by = NULL, adjust = "holm") %>%
  confint()

ex2_test_posthocs <- left_join(
  ex2_test_posthocs_test, 
  ex2_test_posthocs_confint,
  by = c("contrast", "estimate", "SE", "df")
) %>%
  merge_CI_limits(., "lower.CL", "upper.CL") %>%
  mutate(p.value = papa(p.value, asterisk = FALSE))
```

The results confirmed the interaction between Task and Picture Condition found already in the training data which suggests that reading performance was better than writing performance in the picture condition only (Picture, Reading - Writing: $\Delta{M}$ = `r ex2_test_posthocs$estimate``r ex2_test_posthocs$"95% CI"`, *t* = `r ex2_test_posthocs$t.ratio`, *p* = `r ex2_test_posthocs$p.value`). However, unlike Experiment 2b, there was no contrastive deficit and the effect of word familiarity appeared only in one condition, i.e. during reading in the Variety Match condition with pictures.

```{r ex2-test-reading-plots, fig.cap = "Experiment 2b. Length-normalised Levenshtein Edit Distance for testing reading performance for trained non-contrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-2_testing_plot_reading.png"
))
```

```{r ex2-test-spelling-plots, fig.cap = "Experiment 2b. Length-normalised Levenshtein Edit Distance for testing spelling performance for trained non-contrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-2_testing_plot_spelling.png"
))
```

```{r ex2-test-novel, include = FALSE, warning = FALSE}
ex2_novel_nhst <- results$experiment_2$planned_comparisons_NHST$test_l_n %>% 
  summarise_lme(., term, main_terms) %>%
  filter(term == "Variety Condition")

ex2_novel_bayes <- 
  results$experiment_2$planned_comparisons_Bayes$brm_test_novel$fixed %>% 
  summarise_bayes(., term, main_terms) %>%
  filter(term == "Variety Condition")
```

The planned comparison of performance on untrained words only between all Variety Match and Variety Mismatch conditions used the same model structure as for Experiment 2a. There was no effect of Variety (frequentist estimate: $\hat{\beta}$ = `r ex2_novel_nhst$estimate``r ex2_novel_nhst$"95% CI"`, *t* = `r ex2_novel_nhst$statistic`, *p* = `r ex2_novel_nhst$p.value`; Bayesian Estimate: $\hat{\beta}$ = `r ex2_novel_bayes$Estimate``r ex2_novel_bayes$"95% CI"`), again suggesting that there was no evidence for a deterimental effect of exposure to a variety mismatch on reading and spelling of untrained words.

## Discussion

When attempting to learn to read and to spell an opaque artificial orthography participants showed improvement over the course of training. However, unlike  in Experiment 1, where the contrastive deficit was found in some of the Variety Mismatch conditions, there was no effect of Word Type in this experiment. This suggests that learning conditional spelling rules has rendered literacy acquisition in an opaque orthography too difficult for phonological representations that could have been placed into competition with each other to emerge. This confirms observations from cross-linguistic studies of literacy acquisition in naturalistic settings which show that literacy acquisition at the early stages is more difficult for opaque compared to transparent orthographies [@KSeymour2003]. It would seem, then, that under these conditions participants did not acquire any decoding skills and -- consequently -- no effect of variety mismatch on reading and spelling of untrained words was observed.

Although this was not the main aim of this study, the first three experiments combined give us the opportunity to explore which conditions are most conducive to literacy acquisition. Figure\ \@ref(fig:ex1-to-2-test-plots) shows a direct comparison of reading performance for all trained and untrained items during testing in the three experiments. 

```{r ex1-to-2-test-plots, fig.cap = "\\textit{Experiments 1 \\& 2. Length-normalised Levenshtein Edit Distance for testing performance across both tasks for trained and untrained words. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.}", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiments-1-to-2_word-novelty.png"
))
```

We first fitted a linear mixed effect model with sum-coded fixed effects of Word Familiarity and treatment-coded fixed effects Experiment (1, 2a, 2b), and with a maximal random effects structure of random intercepts and slopes of Experiment by item and random intercepts and slopes of Word Familiarity by subjects. Pairwise contrasts were then calculated for each experiment split by Word Familiarity based on the estimated marginal means from the model using the *emmeans* R-package[@R-emmeans]. Here, *p*-values are adjusted using Holm's sequential Bonferroni correction.

```{r ex1-to-2-test, results = "asis", warning = FALSE}
ex1_to_2_test <- results$multiple_experiments$test_en_pair %>% 
  pairs(by = "test_words", adjust = "holm") %>%
  as_tibble()

ex1_to_2_ci <- results$multiple_experiments$test_en_pair %>% 
  pairs(by = "test_words", adjust = "holm") %>%
  confint()

ex1_to_2_results <- left_join(
  ex1_to_2_test, 
  ex1_to_2_ci,
  by = c("contrast", "test_words", "estimate", "SE", "df")
  ) %>%
  merge_CI_limits(., "lower.CL", "upper.CL") %>%
  mutate(p.value = papa(p.value, asterisk = FALSE)) %>%
  arrange(desc(test_words)) %>%
  select(contrast, estimate, SE, "95% CI", t.ratio, p.value) %>%
  rename_many(contrast, c("ex" = "Experiment "))

kable(
  ex1_to_2_results,
  format = "latex",
  booktabs = TRUE,
  caption = "Experiments 1 \\& 2. Parameter estimates for pairwise contrasts between Experiments 1, 2a, and 2b split by Word Familiarity in the testing phase.",
  format.args = table_format_args,
  digits = 2,
  col.names = c("Contrast", "$\\Delta M$", "$SE$", "95\\% Conf. I", "$t$", "$p$"),
  escape = FALSE,
  linesep = ""
) %>% kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"), 
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  row_spec(c(1, 3), bold = TRUE) %>%
  kableExtra::group_rows("Trained Words", 1, 3, bold = FALSE) %>%
  kableExtra::group_rows("Untrained Words", 4, 6, bold = FALSE) %>%
  add_indent(c(1:6))
```

These contrasts show that for trained words, performance was better in Experiment 1 than in Experiment 0. Additionally, performance was also better in Experiment 1 than in Experiment 2. All other contrasts were non-significant. This suggests that while introduction of spelling had no effect on overall learning outcomes, learning a transparent orthography lead to measurable benefits compared to learning an opaque orthography, regardless of spelling, but only for trained and not for untrained words. Recall that it was the transparent orthography for which the contrastive deficit emerged most consistently. This suggests that competition between variants will appear only once learning has progressed to a stage at which access to phonological representations, either via decoding of the  the graphemic form or via semantic representations, have been established. The considerable variability in performance, illustrated in all figures, compellingly shows that participants differ tremendously in terms of their success in the early stages of this process. Thus, to create conditions that would allow for more reliable emergence of phonological representations, Experiment 3 repeated Experiment 2b with a longer training phase and a larger sample of learners.

# Experiment 3

## Method

### Participants

`r stringr::str_to_sentence(as.english(results$experiment_3$demographics$demographic_summary$n, UK = TRUE))` participants (aged aged `r results$experiment_3$demographics$demographic_summary$age_min`--`r results$experiment_3$demographics$demographic_summary$age_max`, *M* = `r results$experiment_3$demographics$demographic_summary$age_mean`, *SD* = `r results$experiment_3$demographics$demographic_summary$age_sd`) were recruited online through Prolific Academic and took part in the study for \pounds 9. All participants reported proficiency in English (1-7 Likert scale: *M* = `r results$experiment_3$demographics$english_proficiency$mean`, *SD* = `r results$experiment_3$demographics$english_proficiency$SD`). Another 6 participants were tested and excluded based on the exclusion criteria described for Experiment 1.

### Materials

We used the same materials as in Experiments 1, 2a, and 2b, and the same opaque orthography as in Experiments 1 and 2b.

### Procedure

The procedure deviated from Experiment 2a and 2b in that the training phase was doubled in length by adding another three ten-word double-blocks (i.e. blocks comprising the same ten words in the reading and the spelling task with order of tasks counterbalanced across participants) resulting in a total of six double-blocks. All words were first partitioned for presentation in the first three double-blocks and then re-partitioned for presentation in the final three double-blocks, ensuring that each double-block contained five contrastive and five non-contrastive words.

As Experiment 2a had suggested that contrastive deficits are likely to persist longer in the Picture condition, presumably because participants associated the two variants of each contrastive words with the same concept, all words were presented along with an associated picture of a concrete object during exposure and reading training. The mean completion time was `r results$experiment_3$demographics$demographic_summary %>% pull(time_mean) %>% as.numeric("minutes")` minutes (*SD* = `r results$experiment_3$demographics$demographic_summary %>% pull(time_sd) %>% as.numeric("minutes")`).

## Results

### Coding

We used the same coding scheme for reading responses as in the previous experiments. The ICC between coders was `r summarise_icc(results$experiment_3$irr_results$icc_results)`. The 95% confidence interval around the parameter estimate indicates that the ICC falls above the bound of .90, which suggests excellent reliability across raters [see @Koo2016].

### Model Fitting

We used the a similar model structure to Experiments 2a and b, with the exclusion of the Picture condition factor. As in Experiments 1, 2a, and 2b, the fixed effects for training and testing were modelled by obtaining all main effects and interactions between all factors excluding Word Type and nesting Word Type within each combination of factor levels of Task and Variety Condition. Because this experiment, like Experiment 1, contained six blocks per task, we included the quadratic term for Block in the analyses of the training data to improve model fit.

The maximal converging random effects structure took the form of zero-correlation random intercepts and slopes of Task, Variety Condition, and their interaction by items, and random intercepts and slopes for the linear and quadratic time terms, Task, Word Type, and their interaction by subjects, including all correlations between these terms. For the testing data, the random effects structure took the form of random intercepts and slopes of Task, Variety Condition, and their interaction by items, and random intercepts and slopes for Task, Word Type, and their interaction by subjects, including all correlations between these terms for both by-subjects and by-items random effects. 

As with Experiments 1, 2a, and 2b, we also modelled the data using Bayesian mixed effects models with a full "maximal" random effects structure (i.e. without suppressing correlations between the by-items random effects in the training phase). These models used the same priors as in Experiments 2a and 2b, with the inclusion of a regularising, very weakly informative prior, $Normal(0, 10)$, on the orthogonal quadratic time term, excluding priors for Picture Condition which was no longer in the model. We used these models to evaluate evidence in support of the null hypothesis for each parameter in the same way as in Experiments 1, 2a, and 2b.

#### Training

Parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex3-train) and depicted in Figures\ \@ref(fig:ex3-train-reading-plots) and \@ref(fig:ex3-train-spelling-plots).

```{r ex3-train, results = "asis", warning = FALSE}
ex3_train_nhst <- results$experiment_3$main_models$lme_train %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex3_train_bayes <- 
  results$experiment_3$planned_comparisons_Bayes$brm_train$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex3_train <- bind_cols(ex3_train_nhst, ex3_train_bayes) %>% 
  rename_nested_terms()

# define rows with significant effect (and no 95% cred I that cross 0)
ex3_train_sig_rows <- c(2:4, 6, 9, 11, 13)

kable(
  ex3_train,
  format = "latex",
  booktabs = TRUE, 
  caption = "Experiment 3. Parameter estimates for the models fitted to nLEDs from the training phase. Bayesian analyses report standardised parameter estimates (i.e. the intercept [grand mean] is centred at 0). Values of 0 with a sign indicate the direction of the estimate before rounding.", 
  format.args = table_format_args,
  digits = 2,
  col.names = shared_colnames,
  longtable = multipage_table,
  escape = FALSE,
  linesep = "" # avoids spacing every 5 rows
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"), 
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  footnote(
    general =
      c("Block (B) = 1-6, Variety Condition (VC) = variety match vs. variety mismatch (VMa vs. VMi),",
        "Task Condition (TC) = reading vs. spelling (R vs. S),",
        "Word Type (WT) = contrastive vs. non-contrastive"),
    general_title = "", 
    footnote_as_chunk = FALSE, 
    escape = FALSE,
    threeparttable = multipage_table
    ) %>%
  add_header_above(
    c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
  ) %>%
  row_spec(ex3_train_sig_rows, bold = TRUE)
```

As in all previous experiments, we found a main effect of Block attesting performance improvement over the course of training. Similar to Experiment 1, the quadratic term also reached significance confirming non-linearity of the learning trajectory. We also confirmed the main effect of Task which indicates that reading performance exceeded spelling performance. The interaction between Block and Task suggests that while performance was similar across tasks at the outset, learning progressed more rapidly for reading than for spelling.

With respect to the main questions of interest -- the contrastive deficit and the effect of Variety -- we found evidence for a contrastive deficit for reading indicated by the effect of Word Type in the Variety Mismatch condition. In addition, we observed an interaction between the quadratic term of Block and Variety and a three-way interaction between Block, Task and Variety, which suggest that performance levelled off somewhat faster in the Variety Match condition, especially for spelling. 

```{r ex3-train-reading-plots, fig.cap = "Experiment 3. Length-normalised Levenshtein Edit Distance for reading of contrastive and non-contrastive words during 3 training blocks in the variety match and variety mismatch conditions. Error bars indicate $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-3_training_plot_reading.png"
))
```

```{r ex3-train-spelling-plots, fig.cap = "Experiment 3. Length-normalised Levenshtein Edit Distance for spelling of contrastive and non-contrastive words during 3 training blocks in the variety match and variety mismatch conditions. Error bars indicate $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-3_training_plot_spelling.png"
))
```

#### Testing

Parameter estimates, confidence intervals (for the frequentist analysis) and credible intervals (for the Bayesian analysis) are presented in Table\ \@ref(tab:ex3-test) and depicted in Figures\ \@ref(fig:ex3-test-reading-plots) and \@ref(fig:ex3-test-spelling-plots). 

```{r ex3-test, results = "asis", warning = FALSE}
ex3_test_nhst <- results$experiment_3$main_models$lme_test %>% 
  summarise_lme(., term, main_terms) %>% 
  select(-df)
  
ex3_test_bayes <- 
  results$experiment_3$planned_comparisons_Bayes$brm_test$fixed %>% 
  summarise_bayes(., term, main_terms) %>% 
  select(-term) # no need for repeated term names

ex3_test <- bind_cols(ex3_test_nhst, ex3_test_bayes) %>% 
  rename_nested_terms()

# define rows with significant effect (and no 95% cred I that cross 0)
ex3_test_sig_rows <- c(2, 3, 5, 11)

kable(
  ex3_test,
  format = "latex",
  booktabs = TRUE, 
  caption = "Experiment 3. Parameter estimates for the models fitted to nLEDs from the testing phase. Bayesian analyses report standardised parameter estimates (i.e. the intercept [grand mean] is centred at 0). Values of 0 with a sign indicate the direction of the estimate before rounding.", 
  format.args = table_format_args,
  digits = 2,
  col.names = shared_colnames,
  longtable = multipage_table,
  escape = FALSE,
  linesep = "" # avoids spacing every 5 rows
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header", "scale_down"), 
    full_width = FALSE,
    font_size = table_font_size
  ) %>%
  footnote(
    general =
      c("Variety Condition (VC) = variety match vs. variety mismatch (VMa vs. VMi),",
        "Task Condition (TC) = reading vs. spelling (R vs. S),",
        "Word Type (WT) = contrastive vs. non-contrastive, Word Familiarity (WF) = familiar vs. unfamiliar (novel)"),
    general_title = "", 
    footnote_as_chunk = FALSE, 
    escape = FALSE,
    threeparttable = multipage_table
    ) %>%
  add_header_above(
    c(" ", "Frequentist Estimates" = 5, "Bayesian Estimates" = 3)
  ) %>%
  row_spec(ex3_test_sig_rows, bold = TRUE)
```

As in the training data, there was a main effect of Task confirming superior performance for reading compared to spelling and an effect of Word Type, indicative of the contrastive deficit, for reading in the Variety Mismatch condition. We also found that the effect of Word Familiarity was significant for reading in the Variety Match condition due to impaired performance for untrained compared to trained words in this condition. Crucially, the analysis yielded a main effect of Variety which showed that overall performance at test was superior in the Variety Mismatch condition.

```{r ex3-test-reading-plots, fig.cap = "Experiment 3. Length-normalised Levenshtein Edit Distance for testing reading performance for trained non-contrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-3_testing_plot_reading.png"
))
```

```{r ex3-test-spelling-plots, fig.cap = "Experiment 3. Length-normalised Levenshtein Edit Distance for testing spelling performance for trained non-contrastive, trained contrastive and untrained words in the variety match and variety mismatch conditions. Large dots and whiskers indicate means and $\\pm$ 1 $SE$ of the mean.", fig.align = "center", out.width = "100%", fig.pos = "htb", warning = FALSE}
knitr::include_graphics(here(
  "04_figures", 
  "output", 
  "experiment-3_testing_plot_spelling.png"
))
```

```{r ex3-test-novel, include = FALSE, warning = FALSE}
ex3_novel_nhst <- results$experiment_3$planned_comparisons_NHST$test_l_n %>% 
  summarise_lme(., term, main_terms) %>%
  filter(term == "Variety Condition")

ex3_novel_bayes <- 
  results$experiment_3$planned_comparisons_Bayes$brm_test_novel$fixed %>% 
  summarise_bayes(., term, main_terms) %>%
  filter(term == "Variety Condition")
```

As in the previous experiments, we performed a planned comparison of performance on untrained words only between the Variety Match and Variety Mismatch conditions using the same model structure as in Experiments 2a and 2b. The frequentist model yielded a significant effect of Variety ($\hat{\beta}$ = `r ex3_novel_nhst$estimate``r ex3_novel_nhst$"95% CI"`, *t* = `r ex3_novel_nhst$statistic`, *p* = `r ex3_novel_nhst$p.value`) with the Bayesian estimate suggesting sufficient evidence in favour of this effect ($\hat{\beta}$ = `r ex3_novel_bayes$Estimate``r ex3_novel_bayes$"95% CI"`). This effect indicates that reading and spelling performance was superior in the variety mismatch condition.

## Discussion

When a larger sample of participants was trained for twice as long as in the previous experiments in reading and spelling of an opaque artificial orthography of words for which semantic information was available there was clear evidence for a contrastive deficit in reading in the Variety Mismatch condition, both in training as well as in testing. This indicates that when training is long enough for lexical and phonological representations to be established exposure of competing variants that are associated with the same meaning impairs reading.  This contrastive deficit could not have arisen had participants exclusively relied on a phonics-based reading strategy that involved piecemeal conversion of individual graphemes into the associated phonemes. In contrast, no contrastive deficit emerged for spelling because no "dialect" orthographic form had ever been presented and because spelling could only be achieved through conversion of individual phonemes into the associated graphemes.

At the same time, the Word Familiarity effect observed for reading in the Variety Match condition suggests that when no competing variants were encountered during literacy training participants seemed to rely much more on trying to directly access the phonological forms of words, either by converting the initial graphemes and accessing the ford form based on initial segments or by accessing the word form based on the meaning. Either processes would inevitably lead to impaired performance with untrained words for which no phonological representations were available, which is what we observed in this condition. The fact that no effect of Word Familiarity was found for reading in the Variety Mismatch condition suggests that when encountering competing variants participants were less likely to access the word forms directly but rather tried to assemble the words through sequential conversion of graphemes into phonemes, i.e. by pursuing a phonics-based reading strategy. As a result, participants in the Variety Mismatch condition exhibited an overall benefit in their literacy skills, especially for untrained words.

# General Discussion

Our experiments compared a condition that simulated beginning readers' encounter of different linguistic variants of words prior to literacy acquisition with a condition in which no new variants are introduced, to ascertain how exposure to more than one linguistic variety of the same language might affect learning to read and to spell. Employing an artificial language with an invented script allowed us to control for potential confounds that often are associated with dialect exposure such as differences in home literacy environment, cultural attitudes to literacy or educational provision. Previous research [@Brown2015] had shown that encountering a variety mismatch impairs processing of contrastive words, i.e. words with different variants across varieties such as Scots /hoose/ vs. English /house/. What remained unclear was whether impaired performance with these contrastive words is also associated with a general deficit in literacy acquisition, especially in learners' decoding abilities as measured by reading and spelling of untrained words. 

The results confirmed and extended the finding of a contrastive deficit in children exposed to AAE reported by @Brown2015. We replicated this deficit and showed that it appeared most reliably when pictures enabled participants to establish semantic representations (Experiment 2a), when a transparent orthography made learning easier (Experiment 2a) and There were no contrastive deficits in spelling. Thus, when there was sufficient training to associate two variants with the same meaning competition between these variants impairs reading but not spelling. Even though the contrastive deficit in reading also emerged in some instances when no pictures were present, it appeared more consistently in conditions with pictures suggesting that shared semantic representations seem to exacerbate the competition between variants. This is in line with interactive activation and competition models which place the locus of inhibition from high-frequency competitors at the lexical layer. The existence of a semantic representation will have reinforced this lexical competition via bidirectional connections between lexical and semantic representations [@Chen2012]. Thus, without a semantic layer, the connectionist simulation reported by @Brown2015 may even have somewhat underestimated the degree of competition between dialect variants of the same word. We predict that the addition of a semantic layer to a connectionist model of this type should result in an even more pronounced contrastive deficit.

The contrastive deficit differs from cognate facilitation effects in bilingual word processing where the existence of a similar variant in another language facilitates L1 and L2 reading due to language-non-specific automatic co-activation of the phonologically similar translation equivalent [@VanAssche2009]. It is interesting to ponder in what ways bilingual and bi-dialectal readers might differ in this regard. For example, it is conceivable that something akin to cognate facilitation can emerge for contrastive words at later stages of literacy acquisition under conditions of dialect exposure when clear socially conditioned language-specific production constraints, i.e. a clear understanding as to where and when to use each variety, are acquired. It is also possible that cognate facilitation in bilingual readers emerges only if bilinguals are trained to read in both languages, so that facilitation arises from partially or fully shared graphemic representations. Future research will have to show whether bi-dialectal individuals will start exhibiting facilitation for contrastive words once they become proficient in selectively using each of their varieties in appropriate contexts and situations, whether facilitation only occurs when individuals learn to read in both varieties or whether representations of languages vs. dialects are qualitatively different giving rise to different patterns of co-activation and inhibition. 

The existence of a contrastive deficit implies that readers attempt to access word forms directly and not through grapheme-phoneme conversion. This is because relying exclusively on a consecutive conversion of each grapheme into the associated phoneme will inevitably lead correct reading of the target word regardless of whether similar variants have been encountered previously or not. We had hypothesised that introduction of spelling training should attenuate the contrastive deficit if it facilitates such phonologically mediated decoding strategies [@Taylor2017]. However, we found that the contrastive deficit in reading emerged even when spelling training was introduced. Moreover, as the joint analyses of the first three experiments showed, introducing spelling training did not lead to a significant improvement in overall literacy nor in phonologically mediated decoding, in contrast to studies demonstrating that invented, i.e. non-normative spelling facilitates reading by boosting phonemic awareness and by promoting an analytical stance towards letter-sound correspondences [@Ehri2006; @Ouellette2008a; @Ouellette2008b; @Ouellette2017]. This finding leads us to conclude that adult learners, who already have mastered the alphabetic principle, do not experience an additional boost from spelling training but rather appear to maintain a separation of processing strategies for reading and for spelling, relying more on direct access of word forms during reading while converting phonemes into graphemes during spelling. One possible explanation for why spelling did not boost phonologically mediated decoding and, consequently, did not attenuate the contrastive deficit in reading is that if conversion of individual phonemes into graphemes and vice versa is perceived as effortful participants may rely on this strategy only when there is no alternative, as in spelling, but may prefer trying to access the phonological form -- either triggered by decoded initial graphemes or directly from the depicted word meaning or both -- whenever this alternative seems feasible, as it is in reading. The fact that spelling performance was consistently worse than reading performance confirms that systematic rule-based conversion is a more effortful and hence more error-prone process. Thus, for adult learners, acquiring literacy in a novel orthography appears to involve making strategy choices, e.g. between converting graphemes into phonemes vs. attempting to recall word forms from memory.

We had introduced testing of untrained words, which is the artificial-language analogy to non-word reading tests for beginning readers, to ascertain to what extent learners were able to generalise phonologically mediated decoding skills. For reading, inferior performance with untrained words could arise if participants had been relying on memory-based retrieval of phonological forms with trained words, a strategy that would then be unsuccessful for words they never encountered before. Such retrieval may be triggered either by word meaning as represented by the picture or by partial decoding that provides access to the beginnings of the word's phonology. Conversely, similar performance with trained and untrained words would indicate that participants applied phonological decoding via grapheme-phoneme conversion to a similar extent regardless of word familiarity.  For spelling, superior performance with trained words could only arise if participants were able to directly retrieve representations of the entire orthographic form, a possibility that is unlikely given the complexity of our invented script, or if participants had acquired knowledge of transitional probabilities between graphemes via statistical learning of serial information, e.g. about spatial locations of letters on the on-screen keyboard and the associated motor routines. We found that in those experiments where participants learned an opaque orthography (Experiments 1, 2b, and 3), a familiarity benefit emerged only for reading in the variety match condition when pictures were present. This suggests that when faced with inconsistent mappings between graphemes and phonemes, participants tried to capitalise as much as possible on direct word form retrieval, yet this strategy was attenuated when participants encountered competing variants, as the absence of a word familiarity effect in the opaque orthography variety mismatch conditions suggests. In contrast, in Experiment 2a, when participants learned to read and spell a transparent orthography, the familiarity effect was ubiquitous and emerged in almost all conditions except in two of the spelling conditions, and even there the mean edit distances pointed in the direction of better performance with familiar words. The observed benefit for processing familiar words points towards entrenchment of acquired decoding strategies when learning to read was made easier by the transparent orthography -- regardless of whether it relied on meaning-based access, partial phonological decoding, memory of orthographic representations or serial information of motor routines that underpinned keyboard-based spelling. 

The crucial question of the present study was whether exposure to different variants of some of the training words in the variety mismatch conditions impaired decoding skills. In Experiments 1 and 2, Bayesian analyses indicated that there was not sufficient evidence to decide, neither with respect to overall performance nor with untrained words. For the opaque orthography, this may simply have been due to the overall difficulty of the task so that the absence of a variety mismatch effect could be construed as due to a floor effect. But even for the transparent orthography, where learning was more successful, there was no evidence for a detrimental variety mismatch effect. However, when we increased our sample size (presumably resulting in greater statistical power) and extended the training phase in Experiment 3 we found a clear performance benefit in the variety mismatch condition. This benefit was significant for overall performance as well as for untrained words separately, and provides clear evidence that under conditions mimicking dialect exposure participants acquired superior decoding skills compared to conditions that mimicked the absence of dialect variation. 

What might account for such a dialect benefit in artificial literacy acquisition? When discussing the differential performance in reading and spelling we suggested that learners appear to make strategy choices based on perceived difficulty. Specifically, reading, especially in variety match conditions, may seem to be more easily achievable through direct retrieval of memorised word forms. However, noticing greater variety in the variety mismatch conditions may have diminished participants' reliance on such a memory-based retrieval strategy during reading and led them to favour a more rule-based, phonologically mediated strategy. This push towards phonologically mediated reading, triggered by greater variability in the input, seemed to have led to an overall improvement in decoding skills. Thus, counter to expectations formulated in the literature so far, our finding suggests that when all other conditions are controlled dialect exposure may actually be beneficial for literacy acquisition.

The finding of a dialect benefit comes with several caveats: First, learners in this study were adults who already had acquired literacy in one or more languages and were certainly familiar with the alphabetic principle. Their prior literacy competence may have endowed them with  knowledge -- implicit or explicit -- of a variety of strategies they could switch between depending on conditions. Such strategy choices may not be available to children who are just starting on the path to literacy using whatever strategies are emphasised in their educational setting. Future research will have to investigate whether dialect exposure may have similar benefits for children who are still just learning about the different routes to reading and spelling. 

Secondly, the artificial conditions of our study differ from naturalistic literacy acquisition in several fundamental ways. For one, the goal of learning in the conditions in which no pictures were present was different from the typical goal of reading and spelling, which is to access and to convey meaning. Here, all that participants were asked to learn was the connection between print and sound, a limitation that was motivated by our attempt to replicate the findings from the @Brown2015 connectionist simulations. Still, it may have shifted the emphasis on access to phonological and graphemic representations more than is appropriate in naturalistic literacy learning thereby affecting the repertoire of strategies learners may have used. We had tried to remedy this limitation by comparing these conditions with conditions in which pictorial information about the meaning was available at all times. However, unlike children, who typically know the meanings of the words they try to read and spell, in these conditions our participants learned the meaning of novel words at the same time as they learned to read and spell. This is more akin to acquisition of a second language in settings where learning is underpinned by print exposure, e.g. when adult learners learn a new language both from a teacher and a textbook -- a more complex and potentially more effortful learning task. We had tried to mitigate against this additional burden by providing pictorial information about the meaning at all times but it is still possible that this more difficult task may have proved taxing on attentional resources and thereby altered learning strategies. To be able to generalise from learning of artificial scripts to literacy acquisition by children, future research may seek to investigate literacy acquisition under more ecologically valid conditions, for example by using novel scripts with familiar words.

Thirdly, our experiments provided no cues, social or otherwise, for dialect use. All that participants encountered in the variety mismatch conditions was greater variability in terms of variants, whether associated with the same meaning or not. Yet dialect use is typically associated with specific regional, social and situational constraints. @Brown2015, in their second simulation, showed that when dialect variants were cued by context nodes coding for variety (AAE vs. MAE) the contrastive deficit was attenuated. This shows that additional differentiating contextual information, provided consistently alongside the phonologically similar variants, reduces competition in line with an interactive activation and competition account that explains competition from strong, and facilitation from weak(er), competitors through inhibitory connections with non-linear activation functions [@Chen2012]. Such an account may even allow for the possibility that dialect variants turn from competing high frequency neighbours into facilitating variety-tagged cognates as information about the context of use is acquired and stored. Despite the lack of contextual information, the present artificial language learning experiments are still of relevance as some evidence suggests that, unlike bilingual language acquisition, bi-dialectal sociolinguistic competence in contextualising variation may take considerable time to build, as indicated by the developmental trajectory for dialect recognition [@McCullough2019] and emergence of social attitudes towards dialects [@Kinzler2013]. One could construe the situation simulated in these experiments as one in which literacy acquisition precedes reliable acquisition of the sociolinguistic competence that governs dialect use. In future studies, we are planning to provide contextual information about each variety alongside each of the different variants, which we predict should reduce the difficulty with processing contrastive words. The intriguing question is in what ways such contextual information affects learners' strategy choices with respect to memorisation vs. phonological mediation. 

Finally, we observed considerable variability in performance in all reported experiments. A visual inspection of our figures suggests that the distributions of edit distances to the target were bimodal in many conditions. Even though the lack of a normal distribution of this dependent variable does not preclude fitting the statistical models described above, as the residuals were normally distributed in all instances, it still  point to qualitatively different strategies employed by subgroups of our participants. These strategy choices may have been influenced by individual differences in cognitive capacity and in perception of task difficulty but may also reflect trade-offs between expended effort and monetary gain unique to participants recruited on crowdsourcing platforms, especially those who use these platforms repeatedly as a source of income [@ElMaarry2018]. Specifically, the substantial duration of each experiment, in conjunction with the reward offered in compliance with minimum wage requirements, may have induced further effort-minimising strategies beyond what would be expected in more naturalistic and potentially better supervised literacy acquisition contexts. Although we tried to mitigate against outright cheating (e.g. note-taking) by placing time constraints on different tasks, we still have to accept that some participants may have expended too little effort for learning to occur. Online artificial language learning studies of the kind conducted here trade research convenience against greater variability in the interest of testing large numbers of participants. Thus, our findings need to be viewed in the context of the perhaps somewhat altered incentive structure that arises from reward-based online research. We hope that future research will be able to scrutinise whether our main finding of a dialect benefit also holds in educational intervention studies.

# Conclusions

In naturalistic contexts, it is difficult to disentangle dialect exposure from other confounding factors that may adversely affect literacy learning. The results from this artificial literacy learning study using an invented script showed that while words with dialect variants are more difficult to read, overall phonological decoding skills can be facilitated if the increased variation in the input leads learners to choose decoding strategies that involve grapheme-phoneme conversion as a means of minimising memory load. Because such a phonologically mediated pathway to literacy acquisition has been shown to be essential in the early stages of learning to read and spell [@Castles2018; @Taylor2017] our results -- if confirmed in further studies with children -- raise the intriguing possibility that dialect exposure may, in fact, yield tangible benefits for literacy acquisition.

\newpage

# References

```{r create-r-references, include = FALSE}
# make a new .bib file for citing r-packages
# Note: We manually changed the "irr" package since it incorrectly includes an
#       email as an author name when created programatically.
# r_refs(file = here("05_paper", "01_manuscript", "r-references.bib"))
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
